{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results  \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'pump'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine_status',\n",
       " 'sensor_01',\n",
       " 'sensor_02',\n",
       " 'sensor_03',\n",
       " 'sensor_04',\n",
       " 'sensor_05',\n",
       " 'sensor_10',\n",
       " 'sensor_11',\n",
       " 'sensor_12',\n",
       " 'sensor_13',\n",
       " 'sensor_14',\n",
       " 'sensor_16',\n",
       " 'sensor_17',\n",
       " 'sensor_18',\n",
       " 'sensor_19',\n",
       " 'sensor_20',\n",
       " 'sensor_21',\n",
       " 'sensor_22',\n",
       " 'sensor_23',\n",
       " 'sensor_24',\n",
       " 'sensor_25',\n",
       " 'sensor_26',\n",
       " 'sensor_27',\n",
       " 'sensor_28',\n",
       " 'sensor_29',\n",
       " 'sensor_30',\n",
       " 'sensor_31',\n",
       " 'sensor_32',\n",
       " 'sensor_33',\n",
       " 'sensor_34',\n",
       " 'sensor_35',\n",
       " 'sensor_36',\n",
       " 'sensor_37',\n",
       " 'sensor_38',\n",
       " 'sensor_39',\n",
       " 'sensor_40',\n",
       " 'sensor_41',\n",
       " 'sensor_42',\n",
       " 'sensor_43',\n",
       " 'sensor_44',\n",
       " 'sensor_45',\n",
       " 'sensor_46',\n",
       " 'sensor_47',\n",
       " 'sensor_48',\n",
       " 'sensor_49']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the pump dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use all tag names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'machine_status','sensor_01','sensor_02','sensor_03','sensor_04','sensor_05','sensor_10','sensor_11','sensor_12','sensor_13','sensor_14','sensor_16','sensor_17','sensor_18','sensor_19','sensor_20','sensor_21','sensor_22','sensor_23','sensor_24','sensor_25','sensor_26','sensor_27','sensor_28','sensor_29','sensor_30','sensor_31','sensor_32','sensor_33','sensor_34','sensor_35','sensor_36','sensor_37','sensor_38','sensor_39','sensor_40','sensor_41','sensor_42','sensor_43','sensor_44','sensor_45','sensor_46','sensor_47','sensor_48','sensor_49'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = name\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pump Sensor Dataset\n",
    "* Load the data using all tag name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "\n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "\n",
    "    # Set TIME column\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'])\n",
    "\n",
    "    # Set time index\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    \n",
    "    # Move the machine_status column to the end and rename it to label\n",
    "    df['machine_status'] = df.pop('machine_status')\n",
    "    df.rename(columns={'machine_status': 'label'}, inplace=True)\n",
    "    \n",
    "    # Convert label column data to integer type\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data time loading function\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'time'\n",
    "    \n",
    "    # Load the data  \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # Create a dummy value column for resampling\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # Perform resampling\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df = df.resample('1s').mean()\n",
    "    \n",
    "    # Remove missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove the dummy value column\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time update function\n",
    "# Update start and end times based on batch size\n",
    "def update_time(time_df, start_time, batch_size):\n",
    "    \n",
    "    # Calculate how many data points need to be loaded\n",
    "    time = batch_size - 1\n",
    "    \n",
    "    # Check the index number of the current time\n",
    "    # If not found, set to the first index as there is no data for the current time\n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # Set the end time for the batch data based on the current time \n",
    "    end_time_ = str(time_df.index[index_now + time] + timedelta(seconds=1))\n",
    "    \n",
    "    # Set the index number for the next start time\n",
    "    index_next = index_now + time + 1\n",
    "    \n",
    "    # Set the next start time\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL encoding\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the maximum and minimum values for selected tag names\n",
    "def set_minmax_value(table, name, start_time_train, end_time_train):\n",
    "    \n",
    "    # URL encoding\n",
    "    start = quote(start_time_train)\n",
    "    end = quote(end_time_train)\n",
    "    \n",
    "    # Load Min, Max data\n",
    "    df_ = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-scale.tql?table={table}&name={name}&start={start}&end={end}')\n",
    "    \n",
    "    # Set Min, Max values\n",
    "    Min = df_.iloc[:,1:-1].T\n",
    "    Max = df_.iloc[:,2:].T\n",
    "    \n",
    "    return Min, Max  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 MinMax Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Min-Max Scaling Setup\n",
    "* Set up a Min-Max Scaler that uses the maximum and minimum values, as the entire dataset is not loaded due to the process concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the MinMaxScaler class\n",
    "class MinMaxScaler_custom:\n",
    "    def __init__(self):\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "\n",
    "    # Set scale values based on the specified parameters\n",
    "    def transform(self, X, min_values, max_values):\n",
    "        X = np.array(X)\n",
    "        self.min_ = np.array(min_values)\n",
    "        self.max_ = np.array(max_values)\n",
    "        \n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        scale = (self.max_ - self.min_)\n",
    "        if np.any(scale == 0):\n",
    "            raise ValueError(\"Min and Max values are the same, resulting in a scale of 0.\")\n",
    "        \n",
    "        return (X - self.min_) / scale\n",
    "    \n",
    "    # Normalize data based on calculated scale values\n",
    "    def fit_transform(self, X, min_values, max_values):\n",
    "        \"\"\"Set parameters and then transform X\"\"\"\n",
    "        return self.transform(X, min_values, max_values)\n",
    "\n",
    "    # Inverse the normalized data back to original values\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"Inverse the transformation and return original values\"\"\"\n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        X_scaled = np.array(X_scaled)\n",
    "        scale = (self.max_ - self.min_)\n",
    "        \n",
    "        return X_scaled * scale + self.min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using Linear_classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Linear_classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear_classifier(\n",
      "  (fc1): Linear(in_features=44, out_features=20, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "input_dim = 44 \n",
    "hidden_dim = 20 \n",
    "output_dim = 2 \n",
    "\n",
    "# Model configuration\n",
    "model = Linear_classifier(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(table, name, timeformat, model, batch_size, epochs, scaler, Min, Max, time_df_train, time_df_valid):\n",
    "\n",
    "    # Initialize training loss\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Initialize best F1 Score value\n",
    "    best_f1= -np.inf\n",
    "\n",
    "    # Start model training\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        correct = 0\n",
    "        total=0\n",
    "        \n",
    "        # Set initial start time\n",
    "        start_time_ = str(time_df_train.index[0])\n",
    "\n",
    "        # Set end time\n",
    "        end_time_train = str(time_df_train.index[-1])  \n",
    "        \n",
    "        # Use a while loop to call data \n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_, end_time_, next_start_time_, index_next= update_time(time_df_train, start_time_, batch_size)\n",
    "            \n",
    "            # Load batch data \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # Apply Scaler\n",
    "            train = scaler.fit_transform(data.iloc[:,:-1].values, Min.iloc[:,1:], Max.iloc[:,1:])\n",
    "            \n",
    "            # Set each DataFrames\n",
    "            train = pd.DataFrame(train)  \n",
    "            train['label'] = data.iloc[:,-1:].values\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(train) == 0:\n",
    "                print(\"No data available.\")\n",
    "                \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(train) == batch_size:\n",
    "                \n",
    "                # Check total batch count  \n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data = np.array(train.iloc[:,:-1])\n",
    "                label = np.array(train.iloc[:,-1:])\n",
    "\n",
    "                # Convert data to Tensor\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "                label = torch.tensor(label).to(device).long().squeeze()\n",
    "\n",
    "                # Optimize the optimizer\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Input to the model\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.squeeze(1), label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Set label predictions \n",
    "                _,pred = torch.max(outputs.squeeze(1), dim=1)\n",
    "                correct += torch.sum(pred==label).item()\n",
    "                total += label.size(0)\n",
    "                \n",
    "                # Reset batch data\n",
    "                train = 0\n",
    "                    \n",
    "            # Set the next start time   \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "\n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next + batch_size - 1 >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "        train_acc.append(100 * correct / total)\n",
    "        train_loss.append(running_loss/total_step)\n",
    "        print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "\n",
    "        # Perform validation at the end of each epoch and save the model with the best performance\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            preds_ = []\n",
    "            targets_ = []\n",
    "                \n",
    "            # Set initial start time\n",
    "            start_time_v = str(time_df_valid.index[0])\n",
    "            \n",
    "            # Set end time\n",
    "            end_time_valid = str(time_df_valid.index[-1])\n",
    "            \n",
    "            # Use a while loop to call data \n",
    "            while start_time_v < end_time_valid:\n",
    "                \n",
    "                # Set the time for loading data based on the batch size\n",
    "                start_time_v, end_time_v, next_start_time_v, index_next_v = update_time(time_df_valid, start_time_v, batch_size)\n",
    "                \n",
    "                # Load batch data \n",
    "                data_v = data_load(table, name, start_time_v, end_time_v, timeformat)\n",
    "                \n",
    "                # Apply Scaler\n",
    "                test = scaler.fit_transform(data_v.iloc[:,:-1].values, Min.iloc[:,1:], Max.iloc[:,1:])\n",
    "                \n",
    "                # Set each DataFrames\n",
    "                test = pd.DataFrame(test)  \n",
    "                test['label'] = data_v.iloc[:,-1:].values\n",
    "                \n",
    "                # Print if the loaded data is empty \n",
    "                if len(test) == 0:\n",
    "                    print(\"No data available.\")\n",
    "                    \n",
    "                # Input the data into the model when it accumulates to the batch size\n",
    "                if len(test) == batch_size:\n",
    "                    \n",
    "                    # Convert data to numpy arrays\n",
    "                    input_data_v = np.array(test.iloc[:,:-1])\n",
    "                    label_v = np.array(test.iloc[:,-1:])\n",
    "\n",
    "                    # Convert data to Tensor\n",
    "                    input_data_v = torch.tensor(input_data_v, dtype=torch.float32).to(device).float()\n",
    "                    label_v = torch.tensor(label_v).to(device).long().squeeze()\n",
    "                    \n",
    "                    # Input to the model\n",
    "                    outputs_v = model(input_data_v)\n",
    "                    \n",
    "                    # Set label predictions \n",
    "                    _,pred_v = torch.max(outputs_v.squeeze(1), dim=1)\n",
    "                    target_v = label_v.view_as(pred_v)\n",
    "      \n",
    "                    preds_.append(pred_v)\n",
    "                    targets_.append(target_v)\n",
    "                    \n",
    "                    # Reset batch data\n",
    "                    test = 0\n",
    "                    \n",
    "                # Set the next start time    \n",
    "                start_time_v = unquote(next_start_time_v)\n",
    "                \n",
    "                # Prevent fetching beyond the last time\n",
    "                if index_next_v + batch_size >= len(time_df_valid):\n",
    "                    break\n",
    "                \n",
    "            # Combine predictions and labels collected from all batches\n",
    "            preds_v = torch.cat(preds_).detach().cpu().numpy()\n",
    "            targets_v = torch.cat(targets_).detach().cpu().numpy()\n",
    "            \n",
    "            f1score = f1_score(targets_v, preds_v,  average='macro')\n",
    "            if best_f1 < f1score:\n",
    "                best_f1 = f1score\n",
    "                # Save the best model \n",
    "                with open(\"./result/Pump_New_Batch.txt\", \"a\") as text_file:\n",
    "                    print('epoch=====',epoch, file=text_file)\n",
    "                    print(classification_report(targets_v, preds_v, digits=4), file=text_file)\n",
    "                torch.save(model, f'./result/Pump_New_Batch.pt') \n",
    "            epochs.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1da77155a64b19a84baecdb9c311bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.35056771747963467, train acc: 92.2485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.3487319800161779, train acc: 93.8110\n",
      "\n",
      "train loss: 0.24732729155411123, train acc: 98.0957\n",
      "\n",
      "train loss: 0.19180987661599613, train acc: 99.1898\n",
      "\n",
      "train loss: 0.1578121520184405, train acc: 99.2416\n",
      "\n",
      "train loss: 0.1349038767120431, train acc: 99.2798\n",
      "\n",
      "train loss: 0.11841244749880493, train acc: 99.2889\n",
      "\n",
      "train loss: 0.10595870213087244, train acc: 99.2966\n",
      "\n",
      "train loss: 0.0962023769350937, train acc: 99.3073\n",
      "\n",
      "train loss: 0.08834125952772398, train acc: 99.3118\n",
      "\n",
      "train loss: 0.08186073051937992, train acc: 99.3179\n",
      "\n",
      "train loss: 0.07641937611039078, train acc: 99.3195\n",
      "\n",
      "train loss: 0.07177855512392445, train acc: 99.3576\n",
      "\n",
      "train loss: 0.06776840811235761, train acc: 99.3805\n",
      "\n",
      "train loss: 0.0642656336406904, train acc: 99.4858\n",
      "\n",
      "train loss: 0.061176571196932816, train acc: 99.5102\n",
      "\n",
      "train loss: 0.05842968151304785, train acc: 99.5148\n",
      "\n",
      "train loss: 0.05596924837409651, train acc: 99.5193\n",
      "\n",
      "train loss: 0.0537508893426157, train acc: 99.5255\n",
      "\n",
      "train loss: 0.05173884694265962, train acc: 99.5300\n",
      "\n",
      "train loss: 0.049903957433300394, train acc: 99.5422\n",
      "\n",
      "train loss: 0.04822255560391813, train acc: 99.5514\n",
      "\n",
      "train loss: 0.046675699916878234, train acc: 99.5560\n",
      "\n",
      "train loss: 0.04524880205521368, train acc: 99.5605\n",
      "\n",
      "train loss: 0.043932967699772923, train acc: 99.5575\n",
      "\n",
      "train loss: 0.042729725313489016, train acc: 99.5529\n",
      "\n",
      "train loss: 0.041652905638288246, train acc: 99.5453\n",
      "\n",
      "train loss: 0.040686425880304786, train acc: 99.5392\n",
      "\n",
      "train loss: 0.03971033899785601, train acc: 99.5575\n",
      "\n",
      "train loss: 0.038887896002562865, train acc: 99.5422\n",
      "\n",
      "train loss: 0.038128399542572466, train acc: 99.4614\n",
      "\n",
      "train loss: 0.03729026363935971, train acc: 99.5941\n",
      "\n",
      "train loss: 0.03659453279922356, train acc: 99.3896\n",
      "\n",
      "train loss: 0.03606894963857584, train acc: 99.2981\n",
      "\n",
      "train loss: 0.03537850052252125, train acc: 99.5575\n",
      "\n",
      "train loss: 0.035041069962130494, train acc: 99.1516\n",
      "\n",
      "train loss: 0.03447073413267212, train acc: 99.5483\n",
      "\n",
      "train loss: 0.03392421328658951, train acc: 99.4705\n",
      "\n",
      "train loss: 0.033371543051642916, train acc: 99.5575\n",
      "\n",
      "train loss: 0.032793307144524275, train acc: 99.5956\n",
      "\n",
      "train loss: 0.032237301624970696, train acc: 99.6399\n",
      "\n",
      "train loss: 0.03170589644041393, train acc: 99.6597\n",
      "\n",
      "train loss: 0.031188617981240577, train acc: 99.6399\n",
      "\n",
      "train loss: 0.03069867802403031, train acc: 99.6124\n",
      "\n",
      "train loss: 0.030268550486199988, train acc: 99.5346\n",
      "\n",
      "train loss: 0.0299109527602287, train acc: 99.4965\n",
      "\n",
      "train loss: 0.029502005813714004, train acc: 99.5789\n",
      "\n",
      "train loss: 0.029163089649702236, train acc: 99.5255\n",
      "\n",
      "train loss: 0.02874328994954006, train acc: 99.6628\n",
      "\n",
      "train loss: 0.028365735157024802, train acc: 99.6704\n"
     ]
    }
   ],
   "source": [
    "########################################### Training Parameter Settings ################################################\n",
    "# Set the tag table name\n",
    "table = 'pump'\n",
    "# Set the tag names\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set the time format \n",
    "timeformat = 'default'\n",
    "# Set the data start time\n",
    "start_time_train = '2018-04-01 00:00:00'\n",
    "# Set the data end time\n",
    "end_time_train = '2018-05-16 21:35:00'\n",
    "# Set batch size\n",
    "batch_size = 1024\n",
    "# Set number of epochs\n",
    "epochs = trange(50, desc='training')\n",
    "# Set Min, Max value \n",
    "Min, Max = set_minmax_value(table, name, start_time_train, end_time_train)\n",
    "# Set scalers\n",
    "scaler = MinMaxScaler_custom()\n",
    "# Load training time list \n",
    "time_df_train = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "########################################### validation Parameter Settings ################################################\n",
    "# Set the start time for the validation data\n",
    "start_time_valid = '2018-05-16 21:36:00'\n",
    "# Set the end time for the validation data\n",
    "end_time_valid = '2018-06-28 17:44:00'\n",
    "# Load validation time list\n",
    "time_df_valid = time_data_load(table, name, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "########################################### Proceed with training ################################################\n",
    "model = train(table, name, timeformat, model, batch_size, epochs, scaler, Min, Max, time_df_train, time_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(table, name, timeformat, model, batch_size, scaler,Min, Max, time_df_test):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Initial settings \n",
    "    preds_t = []\n",
    "    targets_t = []\n",
    "\n",
    "    # Set the initial start time\n",
    "    start_time_t = str(time_df_test.index[0])\n",
    "\n",
    "    # Set the end time\n",
    "    end_time_test = str(time_df_test.index[-1])\n",
    "\n",
    "    # Use a while loop to call data   \n",
    "    while start_time_t < end_time_test:\n",
    "        \n",
    "        # Set the time for loading data based on the batch size\n",
    "        start_time_t, end_time_t, next_start_time_t, index_next_t = update_time(time_df_test, start_time_t, batch_size)\n",
    "        \n",
    "        # Load batch data \n",
    "        data = data_load(table, name, start_time_t, end_time_t, timeformat)\n",
    "        \n",
    "        # Apply Scaler\n",
    "        test = scaler.fit_transform(data.iloc[:,:-1].values, Min.iloc[:,1:], Max.iloc[:,1:])\n",
    "        \n",
    "        # Set each DataFrames\n",
    "        test = pd.DataFrame(test)  \n",
    "        test['label'] = data.iloc[:,-1:].values\n",
    "        \n",
    "        # Print if the loaded data is empty \n",
    "        if len(test) == 0:\n",
    "            print(\"No data available.\")\n",
    "            \n",
    "        # Input the data into the model when it accumulates to the batch size\n",
    "        if len(test) == batch_size:\n",
    "            \n",
    "            # Convert data to numpy arrays\n",
    "            input_data_test = np.array(test.iloc[:,:-1]).reshape(batch_size, 1 , -1)\n",
    "            input_data_label = np.array(test.iloc[:,-1:])\n",
    "            \n",
    "            # Convert data to Tensor\n",
    "            input_data_test = torch.tensor(input_data_test, dtype=torch.float32).to(device).float()\n",
    "            input_data_label = torch.tensor(input_data_label, dtype=torch.float32).to(device).long()\n",
    "                \n",
    "            # Input to the model\n",
    "            outputs_t = model(input_data_test)\n",
    "            \n",
    "            # Set label predictions\n",
    "            _,pred_t = torch.max(outputs_t.squeeze(1), dim=1)\n",
    "            target_t = input_data_label.view_as(pred_t)\n",
    "\n",
    "            preds_t.append(pred_t)\n",
    "            targets_t.append(target_t)\n",
    "            \n",
    "            # Reset batch data\n",
    "            test = 0\n",
    "            \n",
    "        # Set the next start time   \n",
    "        start_time_t = unquote(next_start_time_t)\n",
    "                    \n",
    "        # Prevent fetching beyond the last time\n",
    "        if index_next_t + batch_size  >= len(time_df_test):\n",
    "            break\n",
    "            \n",
    "    # Combine predictions and labels collected from all batches\n",
    "    preds_t = torch.cat(preds_t).detach().cpu().numpy()\n",
    "    targets_t = torch.cat(targets_t).detach().cpu().numpy()\n",
    "\n",
    "    return targets_t, preds_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Test Parameter Settings ################################################\n",
    "# Load the best model\n",
    "model_ = torch.load(f'./result/Pump_New_Batch.pt') \n",
    "# Set the start time for the test data\n",
    "start_time_test = '2018-06-28 17:45:00'\n",
    "# Set the end time for the test data\n",
    "end_time_test = '2018-08-31 23:59:00'\n",
    "# Set batch size\n",
    "batch_size = 1024\n",
    "# Load the test time list\n",
    "time_df_test = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "######################################## Proceed with testing #############################################\n",
    "targets_t, preds_t = test(table, name, timeformat, model_, batch_size, scaler, Min, Max, time_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     83651\n",
      "           1       0.94      0.95      0.94      8509\n",
      "\n",
      "    accuracy                           0.99     92160\n",
      "   macro avg       0.97      0.97      0.97     92160\n",
      "weighted avg       0.99      0.99      0.99     92160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print F1 Score based on testing data\n",
    "print(classification_report(targets_t, preds_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
