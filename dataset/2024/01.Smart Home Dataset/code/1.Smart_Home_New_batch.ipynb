{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "\n",
    "## Import libraries for the model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "import statistics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Set path for saving model training results  \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "# Set seed \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'home'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TAG-Barn [kW]',\n",
       " 'TAG-Dishwasher [kW]',\n",
       " 'TAG-Fridge [kW]',\n",
       " 'TAG-Furnace 1 [kW]',\n",
       " 'TAG-Furnace 2 [kW]',\n",
       " 'TAG-Garage door [kW]',\n",
       " 'TAG-Home office [kW]',\n",
       " 'TAG-House overall [kW]',\n",
       " 'TAG-Kitchen 12 [kW]',\n",
       " 'TAG-Kitchen 14 [kW]',\n",
       " 'TAG-Kitchen 38 [kW]',\n",
       " 'TAG-Living room [kW]',\n",
       " 'TAG-Microwave [kW]',\n",
       " 'TAG-Solar [kW]',\n",
       " 'TAG-Well [kW]',\n",
       " 'TAG-Wine cellar [kW]',\n",
       " 'TAG-apparentTemperature',\n",
       " 'TAG-dewPoint',\n",
       " 'TAG-gen [kW]',\n",
       " 'TAG-humidity',\n",
       " 'TAG-precipIntensity',\n",
       " 'TAG-precipProbability',\n",
       " 'TAG-pressure',\n",
       " 'TAG-temperature',\n",
       " 'TAG-use [kW]',\n",
       " 'TAG-visibility',\n",
       " 'TAG-windBearing',\n",
       " 'TAG-windSpeed']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the Smart home dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names TAG-windBearing, TAG-windSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TAG-windBearing','TAG-windSpeed'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = name[-2:]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Smart Home Dataset\n",
    "* Load the data using the Tag Names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load function\n",
    "# '1D': Daily interval (1 day)\n",
    "# '1H': Hourly interval (1 hour)\n",
    "# '1T' or 'min': Minute interval (1 minute)\n",
    "# '1S': Second interval (1 second)\n",
    "def data_load(table, name, start_time_, end_time_, timeformat, resample_time):\n",
    "    \n",
    "    # Load data \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time_}&end={end_time_}&timeformat={timeformat}')\n",
    "    \n",
    "    # Convert to data grouped by the same time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # Set time index\n",
    "    df = df.set_index(pd.to_datetime(df['TIME']))\n",
    "    df = df.drop(['TIME'], axis=1)\n",
    "    \n",
    "    # Resampling with 1-second intervals\n",
    "    # Can be modified to desired intervals such as day, hour, minute, etc.\n",
    "    df = df.resample(f'{resample_time}').mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data time loading function\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'time'\n",
    "    \n",
    "    # Load the data  \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # Create a dummy value column for resampling\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # Perform resampling\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df = df.resample('1s').mean()\n",
    "    \n",
    "    # Remove missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove the dummy value column\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time update function\n",
    "# Update start and end times based on batch size\n",
    "# window_size: The period over which data is collected in batches\n",
    "# step_size: The interval between the data points\n",
    "def update_time(time_df, start_time, batch_size, window_size, step_size):\n",
    "    \n",
    "    # Calculate how many data points need to be loaded\n",
    "    time = (batch_size * step_size)+ window_size - step_size - 1\n",
    "    \n",
    "    # Check the index number of the current time\n",
    "    # If not found, set to the first index as there is no data for the current time\n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # Set the end time for the batch data based on the current time \n",
    "    end_time_ = str(time_df.index[index_now + time])\n",
    "    \n",
    "    # Set the index number for the next start time\n",
    "    index_next = index_now + time - abs(window_size - step_size - 1)\n",
    "    \n",
    "    # Set the next start time\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL encoding\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the maximum and minimum values for selected tag names\n",
    "def set_minmax_value(table, name, start_time_train, end_time_train):\n",
    "    \n",
    "    # URL encoding\n",
    "    start = quote(start_time_train)\n",
    "    end = quote(end_time_train)\n",
    "    \n",
    "    # Load Min, Max data\n",
    "    df_ = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-scale.tql?table={table}&name={name}&start={start}&end={end}')\n",
    "    \n",
    "    # Set Min, Max values\n",
    "    Min = df_.iloc[:,1:-1].T\n",
    "    Max = df_.iloc[:,2:].T\n",
    "    \n",
    "    return Min, Max "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 MinMax Scaling\n",
    "* 2 Window sliding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Min-Max Scaling Setup\n",
    "* Set up a Min-Max Scaler that uses the maximum and minimum values, as the entire dataset is not loaded due to the process concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the MinMaxScaler class\n",
    "class MinMaxScaler_custom:\n",
    "    def __init__(self):\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "\n",
    "    # Set scale values based on the specified parameters\n",
    "    def transform(self, X, min_values, max_values):\n",
    "        X = np.array(X)\n",
    "        self.min_ = np.array(min_values)\n",
    "        self.max_ = np.array(max_values)\n",
    "        \n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        # Add 1e-6 to prevent the scale value from becoming 0\n",
    "        scale = (self.max_ - self.min_) + 1e-6\n",
    "        if np.any(scale == 0):\n",
    "            raise ValueError(\"Min and Max values are the same, resulting in a scale of 0.\")\n",
    "        \n",
    "        return (X - self.min_) / scale\n",
    "    \n",
    "    # Normalize data based on calculated scale values\n",
    "    def fit_transform(self, X, min_values, max_values):\n",
    "        \"\"\"Set parameters and then transform X\"\"\"\n",
    "        return self.transform(X, min_values, max_values)\n",
    "\n",
    "    # Inverse the normalized data back to original values\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"Inverse the transformation and return original values\"\"\"\n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        X_scaled = np.array(X_scaled)\n",
    "        scale = (self.max_ - self.min_) + 1e-6\n",
    "        \n",
    "        return X_scaled * scale + self.min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Window sliding setup\n",
    "* Window size: Determines how many time points to group together.\n",
    "* Step size: The time interval by which the window moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window(data, window_size, step_size):\n",
    "    \n",
    "    # List to store the results of sliding windows\n",
    "    windows = []\n",
    "\n",
    "    # Apply sliding window\n",
    "    for i in range(0, data.shape[0] - window_size + 1, step_size):\n",
    "        window = data[i:i + window_size, :]\n",
    "        windows.append(window)\n",
    "        \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using LSTM AE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder class definition\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.encoder_fc = nn.Linear(hidden_dim, 2*hidden_dim)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.decoder_fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder part\n",
    "        _, (h, _) = self.encoder_lstm(x)\n",
    "        latent = self.encoder_fc(h[-1])\n",
    "        \n",
    "        # Decoder part\n",
    "        hidden = self.decoder_fc(latent).unsqueeze(0).repeat(x.size(1), 1, 1).permute(1, 0, 2)\n",
    "        output, _ = self.decoder_lstm(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAutoencoder(\n",
      "  (encoder_lstm): LSTM(2, 4, num_layers=3, batch_first=True)\n",
      "  (encoder_fc): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (decoder_fc): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (decoder_lstm): LSTM(4, 2, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "\n",
    "# number of input data columns\n",
    "input_dim = len(tags)\n",
    "\n",
    "# LSMT hidden state size\n",
    "hidden_dim = 2*len(tags)\n",
    "\n",
    "# layer size\n",
    "num_layers = 3\n",
    "\n",
    "# Learning rate \n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = LSTMAutoencoder(input_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "* Save the model with the Best Loss based on the training data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train(table, name, timeformat, model, batch_size, window_size, step_size, epochs, Min, Max, scaler, time_df_train):\n",
    "    \n",
    "    # Initialize training loss\n",
    "    train_loss = []\n",
    "    \n",
    "    # Initialize Best Loss value \n",
    "    best_Loss=np.inf\n",
    "    \n",
    "    for epoch in epochs:\n",
    "         \n",
    "        model.train()\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        \n",
    "        # Set initial start time\n",
    "        start_time_ = str(time_df_train.index[0])\n",
    "        \n",
    "        # Set end time\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "\n",
    "        # Use a while loop to call data  \n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_, end_time_, next_start_time_, index_next= update_time(time_df_train, start_time_, batch_size, window_size, step_size)\n",
    "            \n",
    "            # Load batch data \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat, resample_time=\"1s\")\n",
    "            \n",
    "            # Apply MinMax scaler\n",
    "            scaled_data = scaler.fit_transform(data, Min, Max)\n",
    "            \n",
    "            # Set window \n",
    "            windows = make_window(scaled_data, window_size, step_size)\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(scaled_data) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(windows) == batch_size:\n",
    "                \n",
    "                # Check total batch count \n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data = np.array(windows)\n",
    "\n",
    "                # Convert data to Tensor\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    " \n",
    "                # Optimize the optimizer\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Input to the model\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, input_data)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Reset batch data\n",
    "                windows = []\n",
    "            \n",
    "            # Set the next start time   \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next + (batch_size * step_size)+ window_size - step_size - 1 >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "        if total_step > 0:\n",
    "            train_loss.append(running_loss / total_step)\n",
    "            print(f'\\ntrain loss: {np.mean(train_loss)}')\n",
    "        \n",
    "        # best model save     \n",
    "        if best_Loss > np.mean(train_loss):\n",
    "            best_Loss = np.mean(train_loss)\n",
    "            torch.save(model, f'./result/Smart_home_LSTM_AE_New_Batch.pt')\n",
    "            print('Save the best model.') \n",
    "            \n",
    "        epochs.set_postfix_str(f\"epoch = {epoch}, best_Loss = {best_Loss}\")\n",
    "               \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c270ed4d137f4efdbab38d4642a69a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.06853104370280302\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.07128580011807831\n",
      "\n",
      "train loss: 0.07216161777025609\n",
      "\n",
      "train loss: 0.07226099306886422\n",
      "\n",
      "train loss: 0.0719388827928924\n",
      "\n",
      "train loss: 0.0715274648341403\n",
      "\n",
      "train loss: 0.07115997141843355\n",
      "\n",
      "train loss: 0.07085420131319811\n",
      "\n",
      "train loss: 0.0706027934074449\n",
      "\n",
      "train loss: 0.0703952223592101\n",
      "\n",
      "train loss: 0.07022234958744125\n",
      "\n",
      "train loss: 0.07007698614385542\n",
      "\n",
      "train loss: 0.0699536116404528\n",
      "\n",
      "train loss: 0.069847991950118\n",
      "\n",
      "train loss: 0.06975685262919537\n",
      "\n",
      "train loss: 0.06967763928621806\n",
      "\n",
      "train loss: 0.06960833308333068\n",
      "\n",
      "train loss: 0.0695473242156743\n",
      "\n",
      "train loss: 0.06949331391264342\n",
      "\n",
      "train loss: 0.06944524595502508\n",
      "\n",
      "train loss: 0.0694022541122272\n",
      "\n",
      "train loss: 0.06936362115854312\n",
      "\n",
      "train loss: 0.06932875077861654\n",
      "\n",
      "train loss: 0.06929714246988235\n",
      "\n",
      "train loss: 0.06926837529159717\n",
      "\n",
      "train loss: 0.06924209155015258\n",
      "\n",
      "train loss: 0.06921798678394268\n",
      "\n",
      "train loss: 0.0691958007696392\n",
      "\n",
      "train loss: 0.06917530949614034\n",
      "\n",
      "train loss: 0.06915631982481588\n",
      "\n",
      "train loss: 0.06913866411766073\n",
      "\n",
      "train loss: 0.06912219681251892\n",
      "\n",
      "train loss: 0.06910679100393909\n",
      "\n",
      "train loss: 0.06909233478005003\n",
      "\n",
      "train loss: 0.06907873075244277\n",
      "\n",
      "train loss: 0.06906589223546604\n",
      "\n",
      "train loss: 0.06905374248705029\n",
      "\n",
      "train loss: 0.06904221311730423\n",
      "\n",
      "train loss: 0.06903124272665769\n",
      "\n",
      "train loss: 0.0690207756869183\n",
      "\n",
      "train loss: 0.06901076081281543\n",
      "\n",
      "train loss: 0.0690011499223363\n",
      "\n",
      "train loss: 0.06899189631680612\n",
      "\n",
      "train loss: 0.0689829501645552\n",
      "\n",
      "train loss: 0.06897422085758224\n",
      "\n",
      "train loss: 0.06896584068371628\n",
      "\n",
      "train loss: 0.06895732918661247\n",
      "\n",
      "train loss: 0.06894873147732705\n",
      "\n",
      "train loss: 0.06893961201891778\n",
      "\n",
      "train loss: 0.06892924905202694\n",
      "\n",
      "train loss: 0.06891677786831682\n",
      "\n",
      "train loss: 0.06890181496507197\n",
      "\n",
      "train loss: 0.06888419290245133\n",
      "\n",
      "train loss: 0.06886398574958245\n",
      "\n",
      "train loss: 0.06884209561525872\n",
      "\n",
      "train loss: 0.06881946630713229\n",
      "\n",
      "train loss: 0.0687966750629221\n",
      "\n",
      "train loss: 0.0687740320478723\n",
      "\n",
      "train loss: 0.06875170102917216\n",
      "\n",
      "train loss: 0.06872976865429553\n",
      "\n",
      "train loss: 0.068708280135532\n",
      "\n",
      "train loss: 0.06868725706908034\n",
      "\n",
      "train loss: 0.06866670708551692\n",
      "\n",
      "train loss: 0.06864662918572353\n",
      "\n",
      "train loss: 0.06862701719460797\n",
      "\n",
      "train loss: 0.06860786154216736\n",
      "\n",
      "train loss: 0.06858915046221786\n",
      "\n",
      "train loss: 0.06857087139280124\n",
      "\n",
      "train loss: 0.06855301081136823\n",
      "\n",
      "train loss: 0.06853555492080904\n",
      "\n",
      "train loss: 0.06851849017481178\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06850180280420434\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.0684854796197309\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06846950772486776\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06845387456320516\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06843856798379312\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06842357637967608\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06840888870811088\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06839449397391363\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06838038210598825\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06836654330666452\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06835296805047779\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.0683396472479292\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.0683265724937759\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06831373554900701\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.0683011284488997\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06828874365970994\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06827657418075256\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06826461304645697\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.068252853681435\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06824128994697588\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06822991578459132\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.0682187254239905\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06820771360346159\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06819687494999906\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06818620445324629\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06817569729695411\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06816534887638341\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.0681551547432321\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.06814511067219428\n",
      "Save the best model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMAutoencoder(\n",
       "  (encoder_lstm): LSTM(2, 4, num_layers=3, batch_first=True)\n",
       "  (encoder_fc): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (decoder_fc): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (decoder_lstm): LSTM(4, 2, num_layers=3, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### Training Parameter Settings ################################################\n",
    "# Set tag table name\n",
    "table = 'home'\n",
    "# Set tag name\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set the start time for the train data\n",
    "start_time_train = '2016-01-01 14:00:00'\n",
    "# Set the end time for the train data\n",
    "end_time_train = '2016-01-01 15:00:00'\n",
    "# Set time format \n",
    "timeformat = 'Default'\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "# Set window size\n",
    "window_size = 3\n",
    "# Set step size -> it must be less than or equal to the window size\n",
    "step_size = 1\n",
    "# Set number of epochs\n",
    "epochs = trange(100, desc='training')\n",
    "# Set Min, Max value \n",
    "Min, Max = set_minmax_value(table, name, start_time_train, end_time_train)\n",
    "# Set Min-Max scaler\n",
    "scaler = MinMaxScaler_custom()\n",
    "# Load training time list \n",
    "time_df_train = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "########################################### Proceed with training ################################################\n",
    "train(table, name, timeformat, model, batch_size, window_size, step_size, epochs, Min, Max, scaler, time_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Setting\n",
    "* Calculate the threshold using validation data:\n",
    "    * 1 Mean + Standard Deviation\n",
    "    * 2 Maximum Value\n",
    "    * 3 99% - Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation function\n",
    "def threshold_set(table, name, timeformat, model, batch_size, window_size, step_size, option, Min, Max, scaler, time_df_valid):\n",
    "    \n",
    "    # Initialize validation loss\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Set initial start time\n",
    "        start_time_ = str(time_df_valid.index[0])\n",
    "        \n",
    "        # Set end time\n",
    "        end_time_val = str(time_df_valid.index[-1])\n",
    "        \n",
    "        # Use a while loop to call data\n",
    "        while start_time_ < end_time_val:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_, end_time_, next_start_time_, index_next_ = update_time(time_df_valid, start_time_, batch_size, window_size, step_size)\n",
    "            \n",
    "            # Load batch data \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat, resample_time=\"1s\")\n",
    "            \n",
    "            # Apply Min Max Scaling\n",
    "            scaled_data = scaler.fit_transform(data, Min, Max)\n",
    "            \n",
    "            # Set window \n",
    "            windows = make_window(scaled_data, window_size, step_size)\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(scaled_data) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(windows) == batch_size:\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data_val = np.array(windows)\n",
    "                \n",
    "                # Convert data to Tensor\n",
    "                input_data_val = torch.tensor(input_data_val, dtype=torch.float32).to(device).float()\n",
    "\n",
    "                # Input to the model\n",
    "                outputs_val = model(input_data_val)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss_val = criterion(outputs_val, input_data_val)\n",
    "            \n",
    "                valid_loss.append(loss_val.item())\n",
    "                \n",
    "                # Reset batch data\n",
    "                windows = []\n",
    "                \n",
    "            # Set the next start time    \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next_ + (batch_size * step_size)+ window_size - step_size - 1 >= len(time_df_valid):\n",
    "                break\n",
    "            \n",
    "        # Calculate threshold\n",
    "        if option == 0:\n",
    "            # Mean + Standard Deviation\n",
    "            threshold =  statistics.mean(valid_loss) + statistics.stdev(valid_loss)\n",
    "\n",
    "        # Calculate threshold\n",
    "        if option == 1:\n",
    "            # Maximum Value\n",
    "            threshold =  max(valid_loss)\n",
    "\n",
    "        # Calculate threshold\n",
    "        if option == 2:\n",
    "            # 99th Percentile - Standard Deviation\n",
    "            threshold =  np.percentile(valid_loss, 99) - statistics.stdev(valid_loss)\n",
    "      \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.180762767791748\n"
     ]
    }
   ],
   "source": [
    "########################################### validation Parameter Settings ################################################\n",
    "# Load the best model\n",
    "model_ = torch.load(f'./result/Smart_home_LSTM_AE_New_Batch.pt')\n",
    "# Set the start time for the validation data\n",
    "start_time_val = '2016-01-01 15:00:00'\n",
    "# Set the end time for the validation data\n",
    "end_time_val = '2016-01-01 16:00:00'\n",
    "# Set the threshold Option\n",
    "option = 1\n",
    "# Load validation time list\n",
    "time_df_valid = time_data_load(table, name, quote(start_time_val), quote(end_time_val), timeformat)\n",
    "\n",
    "########################################### Proceed with validation ################################################\n",
    "threshold = threshold_set(table, name, timeformat, model_, batch_size, window_size, step_size, option, Min, Max, scaler, time_df_valid)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "* Proceed with model testing on the test data based on the threshold calculated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing function\n",
    "def test(table, name, timeformat, model, batch_size, window_size, step_size, threshold, Min, Max, scaler, time_df_test):\n",
    "    \n",
    "    # Initial settings \n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Set the initial start time\n",
    "        start_time_ = str(time_df_test.index[0])\n",
    "        \n",
    "        # Set the end time\n",
    "        end_time_test = str(time_df_test.index[-1])\n",
    "        \n",
    "        # Use a while loop to call data \n",
    "        while start_time_ < end_time_test:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_, end_time_, next_start_time_, index_next_ = update_time(time_df_test, start_time_, batch_size, window_size, step_size)\n",
    "\n",
    "            # Load batch data \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat, resample_time=\"1s\")\n",
    "            \n",
    "            # Apply MinMax scaler\n",
    "            scaled_data = scaler.fit_transform(data, Min, Max)\n",
    "            \n",
    "            # Set window \n",
    "            windows = make_window(scaled_data, window_size, step_size)\n",
    "            \n",
    "            # Print if the loaded data is empty   \n",
    "            if len(scaled_data) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(windows) == batch_size:\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data_test = np.array(windows)\n",
    "                \n",
    "                # Convert data to Tensor\n",
    "                input_data_test = torch.tensor(input_data_test, dtype=torch.float32).to(device).float()\n",
    "\n",
    "                # Input to the model\n",
    "                outputs_test = model(input_data_test)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss_test = criterion(outputs_test, input_data_test)\n",
    "            \n",
    "                test_loss.append(loss_test.item())\n",
    "                \n",
    "                # Reset batch data\n",
    "                windows = []\n",
    "                \n",
    "            # Set the next start time   \n",
    "            start_time_ = unquote(next_start_time_) \n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next_ + (batch_size * step_size)+ window_size - step_size - 1 >= len(time_df_test):\n",
    "                break\n",
    "            \n",
    "    # Generate final results \n",
    "    final_df = pd.DataFrame(test_loss, columns=['reconst_score'])\n",
    "    final_df['label'] = 0\n",
    "\n",
    "    # Set labels based on each threshold\n",
    "    final_df['pred_label'] = np.where(final_df['reconst_score']>threshold,1,0)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Test Parameter Settings ################################################\n",
    "# Set the start time for the test data\n",
    "start_time_test = '2016-01-01 16:00:00'\n",
    "# Set the end time for the test data\n",
    "end_time_test = '2016-01-01 17:00:00'\n",
    "# Load the test time list\n",
    "time_df_test = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "######################################## Proceed with testing #############################################\n",
    "result_df = test(table, name, timeformat, model_, batch_size, window_size, step_size, threshold, Min, Max, scaler, time_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "* Evaluate performance based on the F1 Score.\n",
    "* After evaluating performance across different thresholds, fix the threshold that shows the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.49      0.66       112\n",
      "\n",
      "   micro avg       1.00      0.49      0.66       112\n",
      "   macro avg       1.00      0.49      0.66       112\n",
      "weighted avg       1.00      0.49      0.66       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0. Threshold Setting using Mean + Standard Deviation\n",
    "print(classification_report(result_df['label'], result_df['pred_label'],labels=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       112\n",
      "\n",
      "    accuracy                           1.00       112\n",
      "   macro avg       1.00      1.00      1.00       112\n",
      "weighted avg       1.00      1.00      1.00       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Threshold Setting using Maximum Value\n",
    "print(classification_report(result_df['label'], result_df['pred_label'],labels=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.72      0.84       112\n",
      "\n",
      "   micro avg       1.00      0.72      0.84       112\n",
      "   macro avg       1.00      0.72      0.84       112\n",
      "weighted avg       1.00      0.72      0.84       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Threshold Setting using 99% - Standard Deviation\n",
    "print(classification_report(result_df['label'], result_df['pred_label'],labels=[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
