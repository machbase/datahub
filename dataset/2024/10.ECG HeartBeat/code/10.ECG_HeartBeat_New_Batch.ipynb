{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Heartbeat Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "import joblib\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results  \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'ecg'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mit_bih_test_0',\n",
       " 'mit_bih_test_1',\n",
       " 'mit_bih_test_10',\n",
       " 'mit_bih_test_100',\n",
       " 'mit_bih_test_101',\n",
       " 'mit_bih_test_102',\n",
       " 'mit_bih_test_103',\n",
       " 'mit_bih_test_104',\n",
       " 'mit_bih_test_105',\n",
       " 'mit_bih_test_106',\n",
       " 'mit_bih_test_107',\n",
       " 'mit_bih_test_108',\n",
       " 'mit_bih_test_109',\n",
       " 'mit_bih_test_11',\n",
       " 'mit_bih_test_110',\n",
       " 'mit_bih_test_111',\n",
       " 'mit_bih_test_112',\n",
       " 'mit_bih_test_113',\n",
       " 'mit_bih_test_114',\n",
       " 'mit_bih_test_115',\n",
       " 'mit_bih_test_116',\n",
       " 'mit_bih_test_117',\n",
       " 'mit_bih_test_118',\n",
       " 'mit_bih_test_119',\n",
       " 'mit_bih_test_12',\n",
       " 'mit_bih_test_120',\n",
       " 'mit_bih_test_121',\n",
       " 'mit_bih_test_122',\n",
       " 'mit_bih_test_123',\n",
       " 'mit_bih_test_124',\n",
       " 'mit_bih_test_125',\n",
       " 'mit_bih_test_126',\n",
       " 'mit_bih_test_127',\n",
       " 'mit_bih_test_128',\n",
       " 'mit_bih_test_129',\n",
       " 'mit_bih_test_13',\n",
       " 'mit_bih_test_130',\n",
       " 'mit_bih_test_131',\n",
       " 'mit_bih_test_132',\n",
       " 'mit_bih_test_133',\n",
       " 'mit_bih_test_134',\n",
       " 'mit_bih_test_135',\n",
       " 'mit_bih_test_136',\n",
       " 'mit_bih_test_137',\n",
       " 'mit_bih_test_138',\n",
       " 'mit_bih_test_139',\n",
       " 'mit_bih_test_14',\n",
       " 'mit_bih_test_140',\n",
       " 'mit_bih_test_141',\n",
       " 'mit_bih_test_142',\n",
       " 'mit_bih_test_143',\n",
       " 'mit_bih_test_144',\n",
       " 'mit_bih_test_145',\n",
       " 'mit_bih_test_146',\n",
       " 'mit_bih_test_147',\n",
       " 'mit_bih_test_148',\n",
       " 'mit_bih_test_149',\n",
       " 'mit_bih_test_15',\n",
       " 'mit_bih_test_150',\n",
       " 'mit_bih_test_151',\n",
       " 'mit_bih_test_152',\n",
       " 'mit_bih_test_153',\n",
       " 'mit_bih_test_154',\n",
       " 'mit_bih_test_155',\n",
       " 'mit_bih_test_156',\n",
       " 'mit_bih_test_157',\n",
       " 'mit_bih_test_158',\n",
       " 'mit_bih_test_159',\n",
       " 'mit_bih_test_16',\n",
       " 'mit_bih_test_160',\n",
       " 'mit_bih_test_161',\n",
       " 'mit_bih_test_162',\n",
       " 'mit_bih_test_163',\n",
       " 'mit_bih_test_164',\n",
       " 'mit_bih_test_165',\n",
       " 'mit_bih_test_166',\n",
       " 'mit_bih_test_167',\n",
       " 'mit_bih_test_168',\n",
       " 'mit_bih_test_169',\n",
       " 'mit_bih_test_17',\n",
       " 'mit_bih_test_170',\n",
       " 'mit_bih_test_171',\n",
       " 'mit_bih_test_172',\n",
       " 'mit_bih_test_173',\n",
       " 'mit_bih_test_174',\n",
       " 'mit_bih_test_175',\n",
       " 'mit_bih_test_176',\n",
       " 'mit_bih_test_177',\n",
       " 'mit_bih_test_178',\n",
       " 'mit_bih_test_179',\n",
       " 'mit_bih_test_18',\n",
       " 'mit_bih_test_180',\n",
       " 'mit_bih_test_181',\n",
       " 'mit_bih_test_182',\n",
       " 'mit_bih_test_183',\n",
       " 'mit_bih_test_184',\n",
       " 'mit_bih_test_185',\n",
       " 'mit_bih_test_186',\n",
       " 'mit_bih_test_19',\n",
       " 'mit_bih_test_2',\n",
       " 'mit_bih_test_20',\n",
       " 'mit_bih_test_21',\n",
       " 'mit_bih_test_22',\n",
       " 'mit_bih_test_23',\n",
       " 'mit_bih_test_24',\n",
       " 'mit_bih_test_25',\n",
       " 'mit_bih_test_26',\n",
       " 'mit_bih_test_27',\n",
       " 'mit_bih_test_28',\n",
       " 'mit_bih_test_29',\n",
       " 'mit_bih_test_3',\n",
       " 'mit_bih_test_30',\n",
       " 'mit_bih_test_31',\n",
       " 'mit_bih_test_32',\n",
       " 'mit_bih_test_33',\n",
       " 'mit_bih_test_34',\n",
       " 'mit_bih_test_35',\n",
       " 'mit_bih_test_36',\n",
       " 'mit_bih_test_37',\n",
       " 'mit_bih_test_38',\n",
       " 'mit_bih_test_39',\n",
       " 'mit_bih_test_4',\n",
       " 'mit_bih_test_40',\n",
       " 'mit_bih_test_41',\n",
       " 'mit_bih_test_42',\n",
       " 'mit_bih_test_43',\n",
       " 'mit_bih_test_44',\n",
       " 'mit_bih_test_45',\n",
       " 'mit_bih_test_46',\n",
       " 'mit_bih_test_47',\n",
       " 'mit_bih_test_48',\n",
       " 'mit_bih_test_49',\n",
       " 'mit_bih_test_5',\n",
       " 'mit_bih_test_50',\n",
       " 'mit_bih_test_51',\n",
       " 'mit_bih_test_52',\n",
       " 'mit_bih_test_53',\n",
       " 'mit_bih_test_54',\n",
       " 'mit_bih_test_55',\n",
       " 'mit_bih_test_56',\n",
       " 'mit_bih_test_57',\n",
       " 'mit_bih_test_58',\n",
       " 'mit_bih_test_59',\n",
       " 'mit_bih_test_6',\n",
       " 'mit_bih_test_60',\n",
       " 'mit_bih_test_61',\n",
       " 'mit_bih_test_62',\n",
       " 'mit_bih_test_63',\n",
       " 'mit_bih_test_64',\n",
       " 'mit_bih_test_65',\n",
       " 'mit_bih_test_66',\n",
       " 'mit_bih_test_67',\n",
       " 'mit_bih_test_68',\n",
       " 'mit_bih_test_69',\n",
       " 'mit_bih_test_7',\n",
       " 'mit_bih_test_70',\n",
       " 'mit_bih_test_71',\n",
       " 'mit_bih_test_72',\n",
       " 'mit_bih_test_73',\n",
       " 'mit_bih_test_74',\n",
       " 'mit_bih_test_75',\n",
       " 'mit_bih_test_76',\n",
       " 'mit_bih_test_77',\n",
       " 'mit_bih_test_78',\n",
       " 'mit_bih_test_79',\n",
       " 'mit_bih_test_8',\n",
       " 'mit_bih_test_80',\n",
       " 'mit_bih_test_81',\n",
       " 'mit_bih_test_82',\n",
       " 'mit_bih_test_83',\n",
       " 'mit_bih_test_84',\n",
       " 'mit_bih_test_85',\n",
       " 'mit_bih_test_86',\n",
       " 'mit_bih_test_87',\n",
       " 'mit_bih_test_88',\n",
       " 'mit_bih_test_89',\n",
       " 'mit_bih_test_9',\n",
       " 'mit_bih_test_90',\n",
       " 'mit_bih_test_91',\n",
       " 'mit_bih_test_92',\n",
       " 'mit_bih_test_93',\n",
       " 'mit_bih_test_94',\n",
       " 'mit_bih_test_95',\n",
       " 'mit_bih_test_96',\n",
       " 'mit_bih_test_97',\n",
       " 'mit_bih_test_98',\n",
       " 'mit_bih_test_99',\n",
       " 'mit_bih_test_label',\n",
       " 'mit_bih_train_0',\n",
       " 'mit_bih_train_1',\n",
       " 'mit_bih_train_10',\n",
       " 'mit_bih_train_100',\n",
       " 'mit_bih_train_101',\n",
       " 'mit_bih_train_102',\n",
       " 'mit_bih_train_103',\n",
       " 'mit_bih_train_104',\n",
       " 'mit_bih_train_105',\n",
       " 'mit_bih_train_106',\n",
       " 'mit_bih_train_107',\n",
       " 'mit_bih_train_108',\n",
       " 'mit_bih_train_109',\n",
       " 'mit_bih_train_11',\n",
       " 'mit_bih_train_110',\n",
       " 'mit_bih_train_111',\n",
       " 'mit_bih_train_112',\n",
       " 'mit_bih_train_113',\n",
       " 'mit_bih_train_114',\n",
       " 'mit_bih_train_115',\n",
       " 'mit_bih_train_116',\n",
       " 'mit_bih_train_117',\n",
       " 'mit_bih_train_118',\n",
       " 'mit_bih_train_119',\n",
       " 'mit_bih_train_12',\n",
       " 'mit_bih_train_120',\n",
       " 'mit_bih_train_121',\n",
       " 'mit_bih_train_122',\n",
       " 'mit_bih_train_123',\n",
       " 'mit_bih_train_124',\n",
       " 'mit_bih_train_125',\n",
       " 'mit_bih_train_126',\n",
       " 'mit_bih_train_127',\n",
       " 'mit_bih_train_128',\n",
       " 'mit_bih_train_129',\n",
       " 'mit_bih_train_13',\n",
       " 'mit_bih_train_130',\n",
       " 'mit_bih_train_131',\n",
       " 'mit_bih_train_132',\n",
       " 'mit_bih_train_133',\n",
       " 'mit_bih_train_134',\n",
       " 'mit_bih_train_135',\n",
       " 'mit_bih_train_136',\n",
       " 'mit_bih_train_137',\n",
       " 'mit_bih_train_138',\n",
       " 'mit_bih_train_139',\n",
       " 'mit_bih_train_14',\n",
       " 'mit_bih_train_140',\n",
       " 'mit_bih_train_141',\n",
       " 'mit_bih_train_142',\n",
       " 'mit_bih_train_143',\n",
       " 'mit_bih_train_144',\n",
       " 'mit_bih_train_145',\n",
       " 'mit_bih_train_146',\n",
       " 'mit_bih_train_147',\n",
       " 'mit_bih_train_148',\n",
       " 'mit_bih_train_149',\n",
       " 'mit_bih_train_15',\n",
       " 'mit_bih_train_150',\n",
       " 'mit_bih_train_151',\n",
       " 'mit_bih_train_152',\n",
       " 'mit_bih_train_153',\n",
       " 'mit_bih_train_154',\n",
       " 'mit_bih_train_155',\n",
       " 'mit_bih_train_156',\n",
       " 'mit_bih_train_157',\n",
       " 'mit_bih_train_158',\n",
       " 'mit_bih_train_159',\n",
       " 'mit_bih_train_16',\n",
       " 'mit_bih_train_160',\n",
       " 'mit_bih_train_161',\n",
       " 'mit_bih_train_162',\n",
       " 'mit_bih_train_163',\n",
       " 'mit_bih_train_164',\n",
       " 'mit_bih_train_165',\n",
       " 'mit_bih_train_166',\n",
       " 'mit_bih_train_167',\n",
       " 'mit_bih_train_168',\n",
       " 'mit_bih_train_169',\n",
       " 'mit_bih_train_17',\n",
       " 'mit_bih_train_170',\n",
       " 'mit_bih_train_171',\n",
       " 'mit_bih_train_172',\n",
       " 'mit_bih_train_173',\n",
       " 'mit_bih_train_174',\n",
       " 'mit_bih_train_175',\n",
       " 'mit_bih_train_176',\n",
       " 'mit_bih_train_177',\n",
       " 'mit_bih_train_178',\n",
       " 'mit_bih_train_179',\n",
       " 'mit_bih_train_18',\n",
       " 'mit_bih_train_180',\n",
       " 'mit_bih_train_181',\n",
       " 'mit_bih_train_182',\n",
       " 'mit_bih_train_183',\n",
       " 'mit_bih_train_184',\n",
       " 'mit_bih_train_185',\n",
       " 'mit_bih_train_186',\n",
       " 'mit_bih_train_19',\n",
       " 'mit_bih_train_2',\n",
       " 'mit_bih_train_20',\n",
       " 'mit_bih_train_21',\n",
       " 'mit_bih_train_22',\n",
       " 'mit_bih_train_23',\n",
       " 'mit_bih_train_24',\n",
       " 'mit_bih_train_25',\n",
       " 'mit_bih_train_26',\n",
       " 'mit_bih_train_27',\n",
       " 'mit_bih_train_28',\n",
       " 'mit_bih_train_29',\n",
       " 'mit_bih_train_3',\n",
       " 'mit_bih_train_30',\n",
       " 'mit_bih_train_31',\n",
       " 'mit_bih_train_32',\n",
       " 'mit_bih_train_33',\n",
       " 'mit_bih_train_34',\n",
       " 'mit_bih_train_35',\n",
       " 'mit_bih_train_36',\n",
       " 'mit_bih_train_37',\n",
       " 'mit_bih_train_38',\n",
       " 'mit_bih_train_39',\n",
       " 'mit_bih_train_4',\n",
       " 'mit_bih_train_40',\n",
       " 'mit_bih_train_41',\n",
       " 'mit_bih_train_42',\n",
       " 'mit_bih_train_43',\n",
       " 'mit_bih_train_44',\n",
       " 'mit_bih_train_45',\n",
       " 'mit_bih_train_46',\n",
       " 'mit_bih_train_47',\n",
       " 'mit_bih_train_48',\n",
       " 'mit_bih_train_49',\n",
       " 'mit_bih_train_5',\n",
       " 'mit_bih_train_50',\n",
       " 'mit_bih_train_51',\n",
       " 'mit_bih_train_52',\n",
       " 'mit_bih_train_53',\n",
       " 'mit_bih_train_54',\n",
       " 'mit_bih_train_55',\n",
       " 'mit_bih_train_56',\n",
       " 'mit_bih_train_57',\n",
       " 'mit_bih_train_58',\n",
       " 'mit_bih_train_59',\n",
       " 'mit_bih_train_6',\n",
       " 'mit_bih_train_60',\n",
       " 'mit_bih_train_61',\n",
       " 'mit_bih_train_62',\n",
       " 'mit_bih_train_63',\n",
       " 'mit_bih_train_64',\n",
       " 'mit_bih_train_65',\n",
       " 'mit_bih_train_66',\n",
       " 'mit_bih_train_67',\n",
       " 'mit_bih_train_68',\n",
       " 'mit_bih_train_69',\n",
       " 'mit_bih_train_7',\n",
       " 'mit_bih_train_70',\n",
       " 'mit_bih_train_71',\n",
       " 'mit_bih_train_72',\n",
       " 'mit_bih_train_73',\n",
       " 'mit_bih_train_74',\n",
       " 'mit_bih_train_75',\n",
       " 'mit_bih_train_76',\n",
       " 'mit_bih_train_77',\n",
       " 'mit_bih_train_78',\n",
       " 'mit_bih_train_79',\n",
       " 'mit_bih_train_8',\n",
       " 'mit_bih_train_80',\n",
       " 'mit_bih_train_81',\n",
       " 'mit_bih_train_82',\n",
       " 'mit_bih_train_83',\n",
       " 'mit_bih_train_84',\n",
       " 'mit_bih_train_85',\n",
       " 'mit_bih_train_86',\n",
       " 'mit_bih_train_87',\n",
       " 'mit_bih_train_88',\n",
       " 'mit_bih_train_89',\n",
       " 'mit_bih_train_9',\n",
       " 'mit_bih_train_90',\n",
       " 'mit_bih_train_91',\n",
       " 'mit_bih_train_92',\n",
       " 'mit_bih_train_93',\n",
       " 'mit_bih_train_94',\n",
       " 'mit_bih_train_95',\n",
       " 'mit_bih_train_96',\n",
       " 'mit_bih_train_97',\n",
       " 'mit_bih_train_98',\n",
       " 'mit_bih_train_99',\n",
       " 'mit_bih_train_label',\n",
       " 'ptb_0',\n",
       " 'ptb_1',\n",
       " 'ptb_10',\n",
       " 'ptb_100',\n",
       " 'ptb_101',\n",
       " 'ptb_102',\n",
       " 'ptb_103',\n",
       " 'ptb_104',\n",
       " 'ptb_105',\n",
       " 'ptb_106',\n",
       " 'ptb_107',\n",
       " 'ptb_108',\n",
       " 'ptb_109',\n",
       " 'ptb_11',\n",
       " 'ptb_110',\n",
       " 'ptb_111',\n",
       " 'ptb_112',\n",
       " 'ptb_113',\n",
       " 'ptb_114',\n",
       " 'ptb_115',\n",
       " 'ptb_116',\n",
       " 'ptb_117',\n",
       " 'ptb_118',\n",
       " 'ptb_119',\n",
       " 'ptb_12',\n",
       " 'ptb_120',\n",
       " 'ptb_121',\n",
       " 'ptb_122',\n",
       " 'ptb_123',\n",
       " 'ptb_124',\n",
       " 'ptb_125',\n",
       " 'ptb_126',\n",
       " 'ptb_127',\n",
       " 'ptb_128',\n",
       " 'ptb_129',\n",
       " 'ptb_13',\n",
       " 'ptb_130',\n",
       " 'ptb_131',\n",
       " 'ptb_132',\n",
       " 'ptb_133',\n",
       " 'ptb_134',\n",
       " 'ptb_135',\n",
       " 'ptb_136',\n",
       " 'ptb_137',\n",
       " 'ptb_138',\n",
       " 'ptb_139',\n",
       " 'ptb_14',\n",
       " 'ptb_140',\n",
       " 'ptb_141',\n",
       " 'ptb_142',\n",
       " 'ptb_143',\n",
       " 'ptb_144',\n",
       " 'ptb_145',\n",
       " 'ptb_146',\n",
       " 'ptb_147',\n",
       " 'ptb_148',\n",
       " 'ptb_149',\n",
       " 'ptb_15',\n",
       " 'ptb_150',\n",
       " 'ptb_151',\n",
       " 'ptb_152',\n",
       " 'ptb_153',\n",
       " 'ptb_154',\n",
       " 'ptb_155',\n",
       " 'ptb_156',\n",
       " 'ptb_157',\n",
       " 'ptb_158',\n",
       " 'ptb_159',\n",
       " 'ptb_16',\n",
       " 'ptb_160',\n",
       " 'ptb_161',\n",
       " 'ptb_162',\n",
       " 'ptb_163',\n",
       " 'ptb_164',\n",
       " 'ptb_165',\n",
       " 'ptb_166',\n",
       " 'ptb_167',\n",
       " 'ptb_168',\n",
       " 'ptb_169',\n",
       " 'ptb_17',\n",
       " 'ptb_170',\n",
       " 'ptb_171',\n",
       " 'ptb_172',\n",
       " 'ptb_173',\n",
       " 'ptb_174',\n",
       " 'ptb_175',\n",
       " 'ptb_176',\n",
       " 'ptb_177',\n",
       " 'ptb_178',\n",
       " 'ptb_179',\n",
       " 'ptb_18',\n",
       " 'ptb_180',\n",
       " 'ptb_181',\n",
       " 'ptb_182',\n",
       " 'ptb_183',\n",
       " 'ptb_184',\n",
       " 'ptb_185',\n",
       " 'ptb_186',\n",
       " 'ptb_19',\n",
       " 'ptb_2',\n",
       " 'ptb_20',\n",
       " 'ptb_21',\n",
       " 'ptb_22',\n",
       " 'ptb_23',\n",
       " 'ptb_24',\n",
       " 'ptb_25',\n",
       " 'ptb_26',\n",
       " 'ptb_27',\n",
       " 'ptb_28',\n",
       " 'ptb_29',\n",
       " 'ptb_3',\n",
       " 'ptb_30',\n",
       " 'ptb_31',\n",
       " 'ptb_32',\n",
       " 'ptb_33',\n",
       " 'ptb_34',\n",
       " 'ptb_35',\n",
       " 'ptb_36',\n",
       " 'ptb_37',\n",
       " 'ptb_38',\n",
       " 'ptb_39',\n",
       " 'ptb_4',\n",
       " 'ptb_40',\n",
       " 'ptb_41',\n",
       " 'ptb_42',\n",
       " 'ptb_43',\n",
       " 'ptb_44',\n",
       " 'ptb_45',\n",
       " 'ptb_46',\n",
       " 'ptb_47',\n",
       " 'ptb_48',\n",
       " 'ptb_49',\n",
       " 'ptb_5',\n",
       " 'ptb_50',\n",
       " 'ptb_51',\n",
       " 'ptb_52',\n",
       " 'ptb_53',\n",
       " 'ptb_54',\n",
       " 'ptb_55',\n",
       " 'ptb_56',\n",
       " 'ptb_57',\n",
       " 'ptb_58',\n",
       " 'ptb_59',\n",
       " 'ptb_6',\n",
       " 'ptb_60',\n",
       " 'ptb_61',\n",
       " 'ptb_62',\n",
       " 'ptb_63',\n",
       " 'ptb_64',\n",
       " 'ptb_65',\n",
       " 'ptb_66',\n",
       " 'ptb_67',\n",
       " 'ptb_68',\n",
       " 'ptb_69',\n",
       " 'ptb_7',\n",
       " 'ptb_70',\n",
       " 'ptb_71',\n",
       " 'ptb_72',\n",
       " 'ptb_73',\n",
       " 'ptb_74',\n",
       " 'ptb_75',\n",
       " 'ptb_76',\n",
       " 'ptb_77',\n",
       " 'ptb_78',\n",
       " 'ptb_79',\n",
       " 'ptb_8',\n",
       " 'ptb_80',\n",
       " 'ptb_81',\n",
       " 'ptb_82',\n",
       " 'ptb_83',\n",
       " 'ptb_84',\n",
       " 'ptb_85',\n",
       " 'ptb_86',\n",
       " 'ptb_87',\n",
       " 'ptb_88',\n",
       " 'ptb_89',\n",
       " 'ptb_9',\n",
       " 'ptb_90',\n",
       " 'ptb_91',\n",
       " 'ptb_92',\n",
       " 'ptb_93',\n",
       " 'ptb_94',\n",
       " 'ptb_95',\n",
       " 'ptb_96',\n",
       " 'ptb_97',\n",
       " 'ptb_98',\n",
       " 'ptb_99',\n",
       " 'ptb_label']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the ecg dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names related to the mit_bih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mit_bih_train_0','mit_bih_train_1','mit_bih_train_10','mit_bih_train_100','mit_bih_train_101','mit_bih_train_102','mit_bih_train_103','mit_bih_train_104','mit_bih_train_105','mit_bih_train_106','mit_bih_train_107','mit_bih_train_108','mit_bih_train_109','mit_bih_train_11','mit_bih_train_110','mit_bih_train_111','mit_bih_train_112','mit_bih_train_113','mit_bih_train_114','mit_bih_train_115','mit_bih_train_116','mit_bih_train_117','mit_bih_train_118','mit_bih_train_119','mit_bih_train_12','mit_bih_train_120','mit_bih_train_121','mit_bih_train_122','mit_bih_train_123','mit_bih_train_124','mit_bih_train_125','mit_bih_train_126','mit_bih_train_127','mit_bih_train_128','mit_bih_train_129','mit_bih_train_13','mit_bih_train_130','mit_bih_train_131','mit_bih_train_132','mit_bih_train_133','mit_bih_train_134','mit_bih_train_135','mit_bih_train_136','mit_bih_train_137','mit_bih_train_138','mit_bih_train_139','mit_bih_train_14','mit_bih_train_140','mit_bih_train_141','mit_bih_train_142','mit_bih_train_143','mit_bih_train_144','mit_bih_train_145','mit_bih_train_146','mit_bih_train_147','mit_bih_train_148','mit_bih_train_149','mit_bih_train_15','mit_bih_train_150','mit_bih_train_151','mit_bih_train_152','mit_bih_train_153','mit_bih_train_154','mit_bih_train_155','mit_bih_train_156','mit_bih_train_157','mit_bih_train_158','mit_bih_train_159','mit_bih_train_16','mit_bih_train_160','mit_bih_train_161','mit_bih_train_162','mit_bih_train_163','mit_bih_train_164','mit_bih_train_165','mit_bih_train_166','mit_bih_train_167','mit_bih_train_168','mit_bih_train_169','mit_bih_train_17','mit_bih_train_170','mit_bih_train_171','mit_bih_train_172','mit_bih_train_173','mit_bih_train_174','mit_bih_train_175','mit_bih_train_176','mit_bih_train_177','mit_bih_train_178','mit_bih_train_179','mit_bih_train_18','mit_bih_train_180','mit_bih_train_181','mit_bih_train_182','mit_bih_train_183','mit_bih_train_184','mit_bih_train_185','mit_bih_train_186','mit_bih_train_19','mit_bih_train_2','mit_bih_train_20','mit_bih_train_21','mit_bih_train_22','mit_bih_train_23','mit_bih_train_24','mit_bih_train_25','mit_bih_train_26','mit_bih_train_27','mit_bih_train_28','mit_bih_train_29','mit_bih_train_3','mit_bih_train_30','mit_bih_train_31','mit_bih_train_32','mit_bih_train_33','mit_bih_train_34','mit_bih_train_35','mit_bih_train_36','mit_bih_train_37','mit_bih_train_38','mit_bih_train_39','mit_bih_train_4','mit_bih_train_40','mit_bih_train_41','mit_bih_train_42','mit_bih_train_43','mit_bih_train_44','mit_bih_train_45','mit_bih_train_46','mit_bih_train_47','mit_bih_train_48','mit_bih_train_49','mit_bih_train_5','mit_bih_train_50','mit_bih_train_51','mit_bih_train_52','mit_bih_train_53','mit_bih_train_54','mit_bih_train_55','mit_bih_train_56','mit_bih_train_57','mit_bih_train_58','mit_bih_train_59','mit_bih_train_6','mit_bih_train_60','mit_bih_train_61','mit_bih_train_62','mit_bih_train_63','mit_bih_train_64','mit_bih_train_65','mit_bih_train_66','mit_bih_train_67','mit_bih_train_68','mit_bih_train_69','mit_bih_train_7','mit_bih_train_70','mit_bih_train_71','mit_bih_train_72','mit_bih_train_73','mit_bih_train_74','mit_bih_train_75','mit_bih_train_76','mit_bih_train_77','mit_bih_train_78','mit_bih_train_79','mit_bih_train_8','mit_bih_train_80','mit_bih_train_81','mit_bih_train_82','mit_bih_train_83','mit_bih_train_84','mit_bih_train_85','mit_bih_train_86','mit_bih_train_87','mit_bih_train_88','mit_bih_train_89','mit_bih_train_9','mit_bih_train_90','mit_bih_train_91','mit_bih_train_92','mit_bih_train_93','mit_bih_train_94','mit_bih_train_95','mit_bih_train_96','mit_bih_train_97','mit_bih_train_98','mit_bih_train_99','mit_bih_train_label'\n",
      "'mit_bih_test_0','mit_bih_test_1','mit_bih_test_10','mit_bih_test_100','mit_bih_test_101','mit_bih_test_102','mit_bih_test_103','mit_bih_test_104','mit_bih_test_105','mit_bih_test_106','mit_bih_test_107','mit_bih_test_108','mit_bih_test_109','mit_bih_test_11','mit_bih_test_110','mit_bih_test_111','mit_bih_test_112','mit_bih_test_113','mit_bih_test_114','mit_bih_test_115','mit_bih_test_116','mit_bih_test_117','mit_bih_test_118','mit_bih_test_119','mit_bih_test_12','mit_bih_test_120','mit_bih_test_121','mit_bih_test_122','mit_bih_test_123','mit_bih_test_124','mit_bih_test_125','mit_bih_test_126','mit_bih_test_127','mit_bih_test_128','mit_bih_test_129','mit_bih_test_13','mit_bih_test_130','mit_bih_test_131','mit_bih_test_132','mit_bih_test_133','mit_bih_test_134','mit_bih_test_135','mit_bih_test_136','mit_bih_test_137','mit_bih_test_138','mit_bih_test_139','mit_bih_test_14','mit_bih_test_140','mit_bih_test_141','mit_bih_test_142','mit_bih_test_143','mit_bih_test_144','mit_bih_test_145','mit_bih_test_146','mit_bih_test_147','mit_bih_test_148','mit_bih_test_149','mit_bih_test_15','mit_bih_test_150','mit_bih_test_151','mit_bih_test_152','mit_bih_test_153','mit_bih_test_154','mit_bih_test_155','mit_bih_test_156','mit_bih_test_157','mit_bih_test_158','mit_bih_test_159','mit_bih_test_16','mit_bih_test_160','mit_bih_test_161','mit_bih_test_162','mit_bih_test_163','mit_bih_test_164','mit_bih_test_165','mit_bih_test_166','mit_bih_test_167','mit_bih_test_168','mit_bih_test_169','mit_bih_test_17','mit_bih_test_170','mit_bih_test_171','mit_bih_test_172','mit_bih_test_173','mit_bih_test_174','mit_bih_test_175','mit_bih_test_176','mit_bih_test_177','mit_bih_test_178','mit_bih_test_179','mit_bih_test_18','mit_bih_test_180','mit_bih_test_181','mit_bih_test_182','mit_bih_test_183','mit_bih_test_184','mit_bih_test_185','mit_bih_test_186','mit_bih_test_19','mit_bih_test_2','mit_bih_test_20','mit_bih_test_21','mit_bih_test_22','mit_bih_test_23','mit_bih_test_24','mit_bih_test_25','mit_bih_test_26','mit_bih_test_27','mit_bih_test_28','mit_bih_test_29','mit_bih_test_3','mit_bih_test_30','mit_bih_test_31','mit_bih_test_32','mit_bih_test_33','mit_bih_test_34','mit_bih_test_35','mit_bih_test_36','mit_bih_test_37','mit_bih_test_38','mit_bih_test_39','mit_bih_test_4','mit_bih_test_40','mit_bih_test_41','mit_bih_test_42','mit_bih_test_43','mit_bih_test_44','mit_bih_test_45','mit_bih_test_46','mit_bih_test_47','mit_bih_test_48','mit_bih_test_49','mit_bih_test_5','mit_bih_test_50','mit_bih_test_51','mit_bih_test_52','mit_bih_test_53','mit_bih_test_54','mit_bih_test_55','mit_bih_test_56','mit_bih_test_57','mit_bih_test_58','mit_bih_test_59','mit_bih_test_6','mit_bih_test_60','mit_bih_test_61','mit_bih_test_62','mit_bih_test_63','mit_bih_test_64','mit_bih_test_65','mit_bih_test_66','mit_bih_test_67','mit_bih_test_68','mit_bih_test_69','mit_bih_test_7','mit_bih_test_70','mit_bih_test_71','mit_bih_test_72','mit_bih_test_73','mit_bih_test_74','mit_bih_test_75','mit_bih_test_76','mit_bih_test_77','mit_bih_test_78','mit_bih_test_79','mit_bih_test_8','mit_bih_test_80','mit_bih_test_81','mit_bih_test_82','mit_bih_test_83','mit_bih_test_84','mit_bih_test_85','mit_bih_test_86','mit_bih_test_87','mit_bih_test_88','mit_bih_test_89','mit_bih_test_9','mit_bih_test_90','mit_bih_test_91','mit_bih_test_92','mit_bih_test_93','mit_bih_test_94','mit_bih_test_95','mit_bih_test_96','mit_bih_test_97','mit_bih_test_98','mit_bih_test_99','mit_bih_test_label'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired train, test tag names\n",
    "tags_train = name[188:376]\n",
    "tags_test = name[:188]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_train = \",\".join(f\"'{tag}'\" for tag in tags_train)\n",
    "tags_test = \",\".join(f\"'{tag}'\" for tag in tags_test)\n",
    "\n",
    "# Check the selected train, test tag names\n",
    "print(tags_train)\n",
    "print(tags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ECG Dataset\n",
    "* Load the entire dataset upon data loading.\n",
    "\n",
    "    * Label description:\n",
    "\n",
    "        * N (Normal): 0\n",
    "            * Normal heartbeat\n",
    "            * Indicates a normal heart rhythm, reflecting regular electrical activity in the ECG.\n",
    "        * S (Supraventricular ectopic beat): 1\n",
    "            * Supraventricular ectopic beat\n",
    "            * Abnormal heartbeats originating in the atria or atrioventricular node, representing abnormal beats that start from the upper chambers of the heart.\n",
    "        * V (Ventricular ectopic beat): 2\n",
    "            * Ventricular ectopic beat\n",
    "            * Abnormal heartbeats originating from the ventricles, representing fast or abnormal electrical activity in the ventricles.\n",
    "        * F (Fusion of ventricular and normal beat): 3\n",
    "            * Fusion of ventricular and normal beat\n",
    "            * Occurs when a normal heartbeat and a ventricular ectopic beat happen simultaneously, leading to a fused heartbeat appearance.\n",
    "        * Q (Unknown beat): 4\n",
    "            * Unknown beat\n",
    "            * Represents beats that cannot be classified, typically due to insufficient information or difficulty in classifying the specific beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data  \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # Set TIME column\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Determine the label column dynamically (for train and test sets)\n",
    "    label_col = [col for col in df.columns if col.endswith('_label')][0]\n",
    "\n",
    "    # Sort column names in numerical order, excluding the 'TIME' and the dynamic label column\n",
    "    df = df.reindex(['TIME'] + sorted([col for col in df.columns if col not in ['TIME', label_col] and col.split('_')[-1].isdigit()], key=lambda x: int(x.split('_')[-1])) + [label_col], axis=1)\n",
    "    \n",
    "    # Convert label column data to integer type\n",
    "    df[label_col] = df[label_col].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data time loading function\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'TIME'\n",
    "    \n",
    "    # Load the data \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # Create a dummy value column for resampling\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # Perform resampling\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'])\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    # Determine resampling units based on the data and perform resampling\n",
    "    df = df.resample('1T').mean()\n",
    "    \n",
    "    # Remove missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove the dummy value column\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time update function\n",
    "# Update start and end times based on batch size\n",
    "def update_time(time_df, start_time, batch_size):\n",
    "    \n",
    "    # Calculate how many data points need to be loaded\n",
    "    time = batch_size -1 \n",
    "    \n",
    "    # Check the index number of the current time\n",
    "    # If not found, set to the first index as there is no data for the current time\n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # Set the end time for the batch data based on the current time \n",
    "    end_time_ = str(time_df.index[index_now + time] + timedelta(seconds=1))\n",
    "    \n",
    "    # Set the index number for the next start time\n",
    "    index_next = index_now + time + 1\n",
    "    \n",
    "    # Set the next start time\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL encoding\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the maximum and minimum values for selected tag names\n",
    "def set_minmax_value(table, name, start_time_train, end_time_train):\n",
    "    \n",
    "    # URL encoding\n",
    "    start = quote(start_time_train)\n",
    "    end = quote(end_time_train)\n",
    "    \n",
    "    # Load Min, Max data\n",
    "    df_ = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-scale.tql?table={table}&name={name}&start={start}&end={end}')\n",
    "    \n",
    "    # Set Min, Max values\n",
    "    Min = df_.iloc[:,1:-1].T\n",
    "    Max = df_.iloc[:,2:].T\n",
    "    \n",
    "    return Min, Max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Mean values for selected tag names\n",
    "def set_mean_value(table, name, start_time_train, end_time_train, len):\n",
    "    \n",
    "    # URL encoding\n",
    "    start = quote(start_time_train)\n",
    "    end = quote(end_time_train)\n",
    "\n",
    "    # Parameter settings\n",
    "    timeunit = 'min'\n",
    "    func = 'Sum' \n",
    "    timesize = 10\n",
    "\n",
    "    # Load Sum data\n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rollup.tql?timeunit={timeunit}&timesize={timesize}&func={func}&table={table}&name={name}&start={start}&end={end}')\n",
    "\n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='mtime', columns='name', values='Sum(value)', aggfunc='first').reset_index()\n",
    "\n",
    "    # Set TIME column\n",
    "    df['TIME'] = pd.to_datetime(df['mtime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(df.iloc[:,1:-2].sum(), columns=['sum']).reset_index()\n",
    "\n",
    "    # Calculate Global Mean\n",
    "    df['mean'] = df['sum'] / len\n",
    "\n",
    "    # Remove the Sum(value) column\n",
    "    df = df.drop('sum', axis=1)\n",
    "    df = df.iloc[:, -1:].values.reshape(-1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 MinMax Scaling\n",
    "* 2 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Min-Max Scaling Setup\n",
    "* Set up a Min-Max Scaler that uses the maximum and minimum values, as the entire dataset is not loaded due to the process concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the MinMaxScaler class\n",
    "class MinMaxScaler_custom:\n",
    "    def __init__(self):\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "\n",
    "    # Set scale values based on the specified parameters\n",
    "    def transform(self, X, min_values, max_values):\n",
    "        X = np.array(X)\n",
    "        self.min_ = np.array(min_values)\n",
    "        self.max_ = np.array(max_values)\n",
    "        \n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        scale = (self.max_ - self.min_)\n",
    "        if np.any(scale == 0):\n",
    "            raise ValueError(\"Min and Max values are the same, resulting in a scale of 0.\")\n",
    "        \n",
    "        return (X - self.min_) / scale\n",
    "    \n",
    "    # Normalize data based on calculated scale values\n",
    "    def fit_transform(self, X, min_values, max_values):\n",
    "        \"\"\"Set parameters and then transform X\"\"\"\n",
    "        return self.transform(X, min_values, max_values)\n",
    "\n",
    "    # Inverse the normalized data back to original values\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"Inverse the transformation and return original values\"\"\"\n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        X_scaled = np.array(X_scaled)\n",
    "        scale = (self.max_ - self.min_)\n",
    "        \n",
    "        return X_scaled * scale + self.min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PCA(Principal Component Analysis) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_custom:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.mean_ = None\n",
    "        self.components_ = None\n",
    "\n",
    "    def fit(self, X, mean=None):\n",
    "        # Calculate the mean value of the data or use the provided mean value\n",
    "        if mean is None:\n",
    "            self.mean_ = np.mean(X, axis=0)\n",
    "        else:\n",
    "            self.mean_ = mean\n",
    "        # Center the data based on the mean\n",
    "        X_centered = X - self.mean_\n",
    "        # Calculate the covariance matrix\n",
    "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "        # Calculate eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "        # Sort the eigenvalues in descending order\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        self.components_ = eigenvectors[:, sorted_indices][:, :self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform the data using the principal components\n",
    "        X_centered = X - self.mean_\n",
    "        return np.dot(X_centered, self.components_)\n",
    "\n",
    "    def fit_transform(self, X, mean=None):\n",
    "        self.fit(X, mean)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using ResNet1d model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet 1D Model Setup\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=4):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet1D(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = ResNet1D(ResidualBlock, [2, 2, 2, 2], num_classes=5).to(device)\n",
    "\n",
    "# Adjust weights for each class\n",
    "class_weights = torch.tensor([1.0, 5.0, 1.0, 5.0, 1.0]).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "* Proceed by loading only the necessary batch size of data for training.\n",
    "* Save the model with the highest F1 score based on the validation data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train(table, name_train, timeformat, model, batch_size, epochs, scaler, Min, Max, Mean, time_df_train, time_df_valid):\n",
    "    \n",
    "    # Initialize training loss & accuracy\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Initialize best F1 Score value\n",
    "    best_f1= 0\n",
    "\n",
    "    # Start model training\n",
    "    for epoch in epochs:\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        correct = 0\n",
    "        total=0\n",
    "        \n",
    "        # Set initial start time\n",
    "        start_time_ = str(time_df_train.index[0])\n",
    "        \n",
    "        # Set end time\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "        \n",
    "        # Use a while loop to call data\n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size \n",
    "            start_time_, end_time_, next_start_time_, index_next= update_time(time_df_train, start_time_, batch_size)\n",
    "        \n",
    "            # Load batch data \n",
    "            data = data_load(table, name_train, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # Apply MinMax scaler\n",
    "            data_scaled = scaler.fit_transform(data.iloc[:, 1:-1].values, Min.iloc[:,:-1], Max.iloc[:,:-1])\n",
    "            \n",
    "            # Check if PCA model already exists (For the first iteration)\n",
    "            if start_time_ == quote(str(time_df_train.index[0])):\n",
    "                \n",
    "                # Set the desired number of principal components\n",
    "                pca = PCA_custom(n_components=35)\n",
    "                \n",
    "                # Fit the PCA model\n",
    "                pca.fit(data_scaled, Mean)\n",
    "                joblib.dump(pca, f'./result/pca.pkl')\n",
    "            \n",
    "            # For subsequent iterations, load the saved PCA model    \n",
    "            else:  \n",
    "                \n",
    "                pca = joblib.load(f'./result/pca.pkl')\n",
    "                \n",
    "            # Apply PCA\n",
    "            data_scaled = pca.transform(data_scaled)\n",
    "            \n",
    "            # Set DataFrames\n",
    "            data_ = pd.DataFrame(data_scaled)  \n",
    "            data_['label'] = data.iloc[:,-1:].values\n",
    "            \n",
    "            # Print if the loaded data is empty\n",
    "            if len(data_) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(data_) == batch_size:\n",
    "            \n",
    "                # Check total batch count\n",
    "                total_step += 1\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data = np.array(data_.iloc[:, :-1])\n",
    "                input_target = np.array(data_.iloc[:, -1:])\n",
    "                \n",
    "                # Convert data to Tensor\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "                input_target = torch.tensor(input_target, dtype=torch.float32).to(device).long().squeeze()\n",
    "                \n",
    "                # Optimize the optimizer\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Input to the model\n",
    "                outputs = model(input_data.unsqueeze(1))\n",
    "        \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, input_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Set label predictions\n",
    "                _,pred = torch.max(outputs, dim=1)\n",
    "                correct += torch.sum(pred==input_target).item()\n",
    "                total += input_target.size(0)   \n",
    "                \n",
    "                # Reset batch data\n",
    "                data_ = 0\n",
    "\n",
    "            # Set the next start time   \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next + batch_size >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "        train_acc.append(100 * correct / total)\n",
    "        train_loss.append(running_loss/total_step)\n",
    "        print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "        \n",
    "        # Perform validation at the end of each epoch and save the model with the best performance\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            preds_v = []\n",
    "            targets_v = []\n",
    "                \n",
    "            # Set initial start time\n",
    "            start_time_v = str(time_df_valid.index[0])\n",
    "            \n",
    "            # Set end time\n",
    "            end_time_valid = str(time_df_valid.index[-1])\n",
    "            \n",
    "            # Use a while loop to call data \n",
    "            while start_time_v < end_time_valid:\n",
    "                \n",
    "                # Set the time for loading data based on the batch size\n",
    "                start_time_v, end_time_v, next_start_time_v, index_next_v = update_time(time_df_valid, start_time_v, batch_size)\n",
    "                \n",
    "                # Load batch data \n",
    "                data_v = data_load(table, name_train, start_time_v, end_time_v, timeformat)\n",
    "                \n",
    "                # Apply MinMax scaler\n",
    "                data_scaled_v = scaler.fit_transform(data_v.iloc[:, 1:-1].values, Min.iloc[:,:-1], Max.iloc[:,:-1])\n",
    "                \n",
    "                # Apply PCA\n",
    "                data_scaled_v = pca.transform(data_scaled_v)\n",
    "                \n",
    "                # Set DataFrames\n",
    "                data_ = pd.DataFrame(data_scaled_v)  \n",
    "                data_['label'] = data_v.iloc[:,-1:].values\n",
    "                \n",
    "                # Print if the loaded data is empty\n",
    "                if len(data_) == 0:\n",
    "                    print(\"No data available.\")\n",
    "                \n",
    "                # Input the data into the model when it accumulates to the batch size\n",
    "                if len(data_) == batch_size:\n",
    "                    \n",
    "                    # Convert data to numpy arrays\n",
    "                    input_data_v = np.array(data_.iloc[:,:-1])\n",
    "                    input_target_v = np.array(data_.iloc[:, -1:])\n",
    "\n",
    "                    # Convert data to Tensor\n",
    "                    input_data_v = torch.tensor(input_data_v, dtype=torch.float32).to(device).float()\n",
    "                    input_target_v = torch.tensor(input_target_v, dtype=torch.float32).to(device).long().squeeze()\n",
    "                    \n",
    "                    # Input to the model\n",
    "                    outputs_v = model(input_data_v.unsqueeze(1))\n",
    "                    \n",
    "                    # Set label predictions \n",
    "                    _,pred_v = torch.max(outputs_v, dim=1)\n",
    "                    target_v = input_target_v.view_as(pred_v)\n",
    "\n",
    "                    preds_v.append(pred_v)\n",
    "                    targets_v.append(target_v)\n",
    "                    \n",
    "                    # Reset batch data\n",
    "                    data_ = 0\n",
    "                \n",
    "                # Set the next start time    \n",
    "                start_time_v = unquote(next_start_time_v)\n",
    "                \n",
    "                # Prevent fetching beyond the last time\n",
    "                if index_next_v + batch_size >= len(time_df_valid):\n",
    "                    break\n",
    "            \n",
    "            # Combine predictions and labels collected from all batches\n",
    "            preds_v = torch.cat(preds_v).detach().cpu().numpy()\n",
    "            targets_v = torch.cat(targets_v).detach().cpu().numpy()\n",
    "            \n",
    "            f1score = f1_score(targets_v, preds_v,  average='macro')\n",
    "            if best_f1 < f1score:\n",
    "                best_f1 = f1score\n",
    "                # Save the best model \n",
    "                with open(\"./result/ECG_HeartBeat_New_Batch.txt\", \"a\") as text_file:\n",
    "                    print('epoch=====',epoch, file=text_file)\n",
    "                    print(classification_report(targets_v, preds_v, digits=4), file=text_file)\n",
    "                torch.save(model, f'./result/ECG_HeartBeat_New_Batch.pt') \n",
    "            epochs.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")\n",
    "            \n",
    "    return model, scaler, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfe7d86e5be46f1922a360d7ad19f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.5912496722058246, train acc: 88.8646\n",
      "\n",
      "train loss: 0.4353831749605505, train acc: 94.4028\n",
      "\n",
      "train loss: 0.3640984951665527, train acc: 95.6993\n",
      "\n",
      "train loss: 0.3214868417401847, train acc: 96.1708\n",
      "\n",
      "train loss: 0.29232934476121475, train acc: 96.4420\n",
      "\n",
      "train loss: 0.27079266579331535, train acc: 96.4767\n",
      "\n",
      "train loss: 0.252842793829347, train acc: 96.8159\n",
      "\n",
      "train loss: 0.2379818110187587, train acc: 96.9752\n",
      "\n",
      "train loss: 0.225365149401869, train acc: 97.1577\n",
      "\n",
      "train loss: 0.21424199073624456, train acc: 97.2682\n",
      "\n",
      "train loss: 0.20469525799101762, train acc: 97.3723\n",
      "\n",
      "train loss: 0.19621081922709813, train acc: 97.4147\n",
      "\n",
      "train loss: 0.18859040881186603, train acc: 97.6383\n",
      "\n",
      "train loss: 0.18149913185367123, train acc: 97.6742\n",
      "\n",
      "train loss: 0.174977035758396, train acc: 97.8117\n",
      "\n",
      "train loss: 0.1690489673178251, train acc: 97.7950\n",
      "\n",
      "train loss: 0.16351410534936392, train acc: 97.9556\n",
      "\n",
      "train loss: 0.15833528624161294, train acc: 98.0636\n",
      "\n",
      "train loss: 0.1538212116049226, train acc: 97.9801\n",
      "\n",
      "train loss: 0.1493047371543454, train acc: 98.2075\n",
      "\n",
      "train loss: 0.1451279507812234, train acc: 98.2255\n",
      "\n",
      "train loss: 0.1412296362263574, train acc: 98.3026\n",
      "\n",
      "train loss: 0.13964647225272636, train acc: 97.3286\n",
      "\n",
      "train loss: 0.13652380307821818, train acc: 98.2383\n",
      "\n",
      "train loss: 0.13322870287063876, train acc: 98.4298\n",
      "\n",
      "train loss: 0.13013671656980025, train acc: 98.4259\n",
      "\n",
      "train loss: 0.12685462419800533, train acc: 98.7575\n",
      "\n",
      "train loss: 0.12399332030742828, train acc: 98.6983\n",
      "\n",
      "train loss: 0.12125570647597665, train acc: 98.7279\n",
      "\n",
      "train loss: 0.1184536122672031, train acc: 98.8757\n",
      "\n",
      "train loss: 0.11591254590128205, train acc: 98.8089\n",
      "\n",
      "train loss: 0.11346035722090164, train acc: 98.9258\n",
      "\n",
      "train loss: 0.11118140884681027, train acc: 98.8435\n",
      "\n",
      "train loss: 0.1090089366947886, train acc: 98.8949\n",
      "\n",
      "train loss: 0.1069850230442458, train acc: 98.9052\n",
      "\n",
      "train loss: 0.1050550645851797, train acc: 98.9155\n",
      "\n",
      "train loss: 0.1030649416463961, train acc: 99.0980\n",
      "\n",
      "train loss: 0.10129994576683375, train acc: 98.9708\n",
      "\n",
      "train loss: 0.09979795805347, train acc: 98.8204\n",
      "\n",
      "train loss: 0.0980019119136207, train acc: 99.1751\n",
      "\n",
      "train loss: 0.09646849634020623, train acc: 99.0376\n",
      "\n",
      "train loss: 0.09496292821948597, train acc: 98.9990\n",
      "\n",
      "train loss: 0.09337853271123184, train acc: 99.1699\n",
      "\n",
      "train loss: 0.09194294627589893, train acc: 99.1455\n",
      "\n",
      "train loss: 0.09045852750665045, train acc: 99.2663\n",
      "\n",
      "train loss: 0.08902505727245598, train acc: 99.2804\n",
      "\n",
      "train loss: 0.0876051770486167, train acc: 99.3126\n",
      "\n",
      "train loss: 0.08638047023442486, train acc: 99.1763\n",
      "\n",
      "train loss: 0.08512386273421152, train acc: 99.2444\n",
      "\n",
      "train loss: 0.0839467670128828, train acc: 99.2085\n",
      "\n",
      "train loss: 0.08276778052587896, train acc: 99.2779\n",
      "\n",
      "train loss: 0.08165620943401311, train acc: 99.2817\n",
      "\n",
      "train loss: 0.0806179756582821, train acc: 99.1931\n",
      "\n",
      "train loss: 0.0796017758456353, train acc: 99.2200\n",
      "\n",
      "train loss: 0.07854775384427205, train acc: 99.3639\n",
      "\n",
      "train loss: 0.07744614523955835, train acc: 99.4886\n",
      "\n",
      "train loss: 0.07646272967460747, train acc: 99.3729\n",
      "\n",
      "train loss: 0.07560258211549828, train acc: 99.2457\n",
      "\n",
      "train loss: 0.07482186410138691, train acc: 99.1417\n",
      "\n",
      "train loss: 0.07401253419525906, train acc: 99.2046\n",
      "\n",
      "train loss: 0.073268607603824, train acc: 99.1635\n",
      "\n",
      "train loss: 0.07238276307370298, train acc: 99.4578\n",
      "\n",
      "train loss: 0.071514270689596, train acc: 99.4899\n",
      "\n",
      "train loss: 0.07071166049205968, train acc: 99.4025\n",
      "\n",
      "train loss: 0.06990400802354452, train acc: 99.4899\n",
      "\n",
      "train loss: 0.06911832696864599, train acc: 99.4745\n",
      "\n",
      "train loss: 0.06834177934481979, train acc: 99.5066\n",
      "\n",
      "train loss: 0.06758178772614552, train acc: 99.5066\n",
      "\n",
      "train loss: 0.06698592481958877, train acc: 99.2701\n",
      "\n",
      "train loss: 0.06640356876475988, train acc: 99.2599\n",
      "\n",
      "train loss: 0.06575905212991318, train acc: 99.3896\n",
      "\n",
      "train loss: 0.06507775448388638, train acc: 99.5438\n",
      "\n",
      "train loss: 0.06437472016242567, train acc: 99.5978\n",
      "\n",
      "train loss: 0.06367578520754201, train acc: 99.6248\n",
      "\n",
      "train loss: 0.06310289092672368, train acc: 99.4076\n",
      "\n",
      "train loss: 0.06255936023129094, train acc: 99.4012\n",
      "\n",
      "train loss: 0.062046981122063354, train acc: 99.3203\n",
      "\n",
      "train loss: 0.061452582854005865, train acc: 99.5361\n",
      "\n",
      "train loss: 0.06086262810552153, train acc: 99.5824\n",
      "\n",
      "train loss: 0.06030757580971878, train acc: 99.5605\n",
      "\n",
      "train loss: 0.0597605633461192, train acc: 99.5670\n",
      "\n",
      "train loss: 0.059219311528299096, train acc: 99.5734\n",
      "\n",
      "train loss: 0.05871780722834775, train acc: 99.5271\n",
      "\n",
      "train loss: 0.05826266165164079, train acc: 99.4513\n",
      "\n",
      "train loss: 0.057739532550641175, train acc: 99.6030\n",
      "\n",
      "train loss: 0.05735116352819048, train acc: 99.3383\n",
      "\n",
      "train loss: 0.056900872843505125, train acc: 99.4873\n",
      "\n",
      "train loss: 0.05645618278994982, train acc: 99.4680\n",
      "\n",
      "train loss: 0.05603720710828591, train acc: 99.4526\n",
      "\n",
      "train loss: 0.05553940373594732, train acc: 99.6646\n",
      "\n",
      "train loss: 0.05506618259695953, train acc: 99.6698\n",
      "\n",
      "train loss: 0.05459600534027076, train acc: 99.6685\n",
      "\n",
      "train loss: 0.05412289687773393, train acc: 99.7096\n",
      "\n",
      "train loss: 0.053704809212806154, train acc: 99.6119\n",
      "\n",
      "train loss: 0.05333762027116375, train acc: 99.4321\n",
      "\n",
      "train loss: 0.05296835474435987, train acc: 99.5271\n",
      "\n",
      "train loss: 0.052571495937767984, train acc: 99.5773\n",
      "\n",
      "train loss: 0.05212852998293451, train acc: 99.7302\n",
      "\n",
      "train loss: 0.051688340233729806, train acc: 99.7764\n",
      "\n",
      "train loss: 0.05128332611217048, train acc: 99.6775\n"
     ]
    }
   ],
   "source": [
    "########################################### Training Parameter Settings ################################################\n",
    "# Set tag table name\n",
    "table = 'ecg'\n",
    "# Set tag name\n",
    "name_train = quote(tags_train, safe=\":/\")\n",
    "# Set time format\n",
    "timeformat = 'default'\n",
    "# Set the start time for the train data\n",
    "start_time_train = '2024-10-14 00:00:00'\n",
    "# Set the end time for the train data\n",
    "end_time_train = '2024-12-07 17:16:00'\n",
    "# Set train batch size\n",
    "batch_size = 1024\n",
    "# Set number of epochs\n",
    "epochs = trange(100, desc='training')\n",
    "# Set Min, Max value \n",
    "Min, Max = set_minmax_value(table, name_train, start_time_train, end_time_train)\n",
    "# Set scalers\n",
    "scaler = MinMaxScaler_custom()\n",
    "# Load training time list \n",
    "time_df_train = time_data_load(table, name_train, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "# Set Mean\n",
    "Mean = set_mean_value(table, name_train, start_time_train, end_time_train, len(time_df_train))\n",
    "########################################### validation Parameter Settings ################################################\n",
    "# Set the start time for the validation data\n",
    "start_time_valid = '2024-12-07 17:17:00'\n",
    "# Set the end time for the validation data\n",
    "end_time_valid = '2024-12-13 19:12:00'\n",
    "# Load validation time list\n",
    "time_df_valid = time_data_load(table, name_train, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "########################################### Proceed with training ################################################\n",
    "model, scaler, pca = train(table, name_train, timeformat, model, batch_size, epochs, scaler, Min, Max, Mean, time_df_train, time_df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing function  \n",
    "def test(table, name_test, timeformat, model, batch_size, scaler, Min, Max, pca, time_df_test):\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                model.eval()\n",
    "                \n",
    "                # Initial settings \n",
    "                preds_t = []\n",
    "                targets_t = []\n",
    "                    \n",
    "                # Set the initial start time\n",
    "                start_time_t = str(time_df_test.index[0])\n",
    "                \n",
    "                # Set the end time\n",
    "                end_time_test = str(time_df_test.index[-1])\n",
    "                \n",
    "                # Use a while loop to call data  \n",
    "                while start_time_t < end_time_test:\n",
    "                    \n",
    "                    # Set the time for loading data based on the batch size\n",
    "                    start_time_t, end_time_t, next_start_time_t, index_next_t = update_time(time_df_test, start_time_t, batch_size)\n",
    "                    \n",
    "                    # Load batch data\n",
    "                    data_t = data_load(table, name_test, start_time_t, end_time_t, timeformat)\n",
    "                    \n",
    "                    # Apply MinMax scaler\n",
    "                    data_scaled_t = scaler.fit_transform(data_t.iloc[:, 1:-1].values, Min.iloc[:,:-1], Max.iloc[:,:-1])\n",
    "\n",
    "                    # Apply PCA\n",
    "                    data_scaled_t = pca.transform(data_scaled_t)\n",
    "                    \n",
    "                    # Set DataFrames\n",
    "                    data_ = pd.DataFrame(data_scaled_t)  \n",
    "                    data_['label'] = data_t.iloc[:,-1:].values\n",
    "\n",
    "                    # Print if the loaded data is empty\n",
    "                    if len(data_) == 0:\n",
    "                        print(\"No data available.\")\n",
    "                    \n",
    "                    # Input the data into the model when it accumulates to the batch size\n",
    "                    if len(data_) == batch_size:\n",
    "                        \n",
    "                        # Convert data to numpy arrays\n",
    "                        input_data_t = np.array(data_.iloc[:,:-1])\n",
    "                        input_target_t = np.array(data_.iloc[:, -1:])\n",
    "\n",
    "                        # Convert data to Tensor\n",
    "                        input_data_t = torch.tensor(input_data_t, dtype=torch.float32).to(device).float()\n",
    "                        input_target_t = torch.tensor(input_target_t, dtype=torch.float32).to(device).long().squeeze()\n",
    "                        \n",
    "                        # Input to the model\n",
    "                        outputs_t = model(input_data_t.unsqueeze(1))\n",
    "                        \n",
    "                        # Set label predictions\n",
    "                        _,pred_t = torch.max(outputs_t, dim=1)\n",
    "                        target_t = input_target_t.view_as(pred_t)\n",
    "        \n",
    "                        preds_t.append(pred_t)\n",
    "                        \n",
    "                        targets_t.append(target_t)\n",
    "                        \n",
    "                        # Reset batch data\n",
    "                        data_ = []\n",
    "                        \n",
    "                    # Set the next start time   \n",
    "                    start_time_t = unquote(next_start_time_t)\n",
    "                    \n",
    "                    # Prevent fetching beyond the last time\n",
    "                    if index_next_t + batch_size >= len(time_df_test):\n",
    "                        break\n",
    "                        \n",
    "                # Combine predictions and labels collected from all batches\n",
    "                preds_t = torch.cat(preds_t).detach().cpu().numpy()\n",
    "                targets_t = torch.cat(targets_t).detach().cpu().numpy()\n",
    "                    \n",
    "            return targets_t, preds_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Test Parameter Settings ################################################\n",
    "# Load the best model\n",
    "model_ = torch.load(f'./result/ECG_HeartBeat_New_Batch.pt') \n",
    "# Set the start time for the test data\n",
    "start_time_test = '2024-12-13 19:13:00'\n",
    "# Set the end time for the test data\n",
    "end_time_test = '2024-12-29 00:03:00'\n",
    "# Set the test batch size\n",
    "batch_size_test = 1024\n",
    "# Set tag name\n",
    "name_test = quote(tags_test, safe=\":/\")\n",
    "# Load the test time list\n",
    "time_df_test = time_data_load(table, name_test, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "pca = joblib.load(f'./result/pca.pkl')\n",
    "######################################## Proceed with testing #############################################\n",
    "targets_t, preds_t = test(table, name_test, timeformat, model_, batch_size, scaler, Min, Max, pca, time_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     17805\n",
      "           1       0.79      0.77      0.78       549\n",
      "           2       0.95      0.91      0.93      1421\n",
      "           3       0.77      0.75      0.76       159\n",
      "           4       0.99      0.97      0.98      1570\n",
      "\n",
      "    accuracy                           0.98     21504\n",
      "   macro avg       0.90      0.88      0.89     21504\n",
      "weighted avg       0.98      0.98      0.98     21504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets_t, preds_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
