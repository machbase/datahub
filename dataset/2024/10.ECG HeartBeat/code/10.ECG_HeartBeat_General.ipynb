{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results  \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'ecg'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mit_bih_test_0',\n",
       " 'mit_bih_test_1',\n",
       " 'mit_bih_test_10',\n",
       " 'mit_bih_test_100',\n",
       " 'mit_bih_test_101',\n",
       " 'mit_bih_test_102',\n",
       " 'mit_bih_test_103',\n",
       " 'mit_bih_test_104',\n",
       " 'mit_bih_test_105',\n",
       " 'mit_bih_test_106',\n",
       " 'mit_bih_test_107',\n",
       " 'mit_bih_test_108',\n",
       " 'mit_bih_test_109',\n",
       " 'mit_bih_test_11',\n",
       " 'mit_bih_test_110',\n",
       " 'mit_bih_test_111',\n",
       " 'mit_bih_test_112',\n",
       " 'mit_bih_test_113',\n",
       " 'mit_bih_test_114',\n",
       " 'mit_bih_test_115',\n",
       " 'mit_bih_test_116',\n",
       " 'mit_bih_test_117',\n",
       " 'mit_bih_test_118',\n",
       " 'mit_bih_test_119',\n",
       " 'mit_bih_test_12',\n",
       " 'mit_bih_test_120',\n",
       " 'mit_bih_test_121',\n",
       " 'mit_bih_test_122',\n",
       " 'mit_bih_test_123',\n",
       " 'mit_bih_test_124',\n",
       " 'mit_bih_test_125',\n",
       " 'mit_bih_test_126',\n",
       " 'mit_bih_test_127',\n",
       " 'mit_bih_test_128',\n",
       " 'mit_bih_test_129',\n",
       " 'mit_bih_test_13',\n",
       " 'mit_bih_test_130',\n",
       " 'mit_bih_test_131',\n",
       " 'mit_bih_test_132',\n",
       " 'mit_bih_test_133',\n",
       " 'mit_bih_test_134',\n",
       " 'mit_bih_test_135',\n",
       " 'mit_bih_test_136',\n",
       " 'mit_bih_test_137',\n",
       " 'mit_bih_test_138',\n",
       " 'mit_bih_test_139',\n",
       " 'mit_bih_test_14',\n",
       " 'mit_bih_test_140',\n",
       " 'mit_bih_test_141',\n",
       " 'mit_bih_test_142',\n",
       " 'mit_bih_test_143',\n",
       " 'mit_bih_test_144',\n",
       " 'mit_bih_test_145',\n",
       " 'mit_bih_test_146',\n",
       " 'mit_bih_test_147',\n",
       " 'mit_bih_test_148',\n",
       " 'mit_bih_test_149',\n",
       " 'mit_bih_test_15',\n",
       " 'mit_bih_test_150',\n",
       " 'mit_bih_test_151',\n",
       " 'mit_bih_test_152',\n",
       " 'mit_bih_test_153',\n",
       " 'mit_bih_test_154',\n",
       " 'mit_bih_test_155',\n",
       " 'mit_bih_test_156',\n",
       " 'mit_bih_test_157',\n",
       " 'mit_bih_test_158',\n",
       " 'mit_bih_test_159',\n",
       " 'mit_bih_test_16',\n",
       " 'mit_bih_test_160',\n",
       " 'mit_bih_test_161',\n",
       " 'mit_bih_test_162',\n",
       " 'mit_bih_test_163',\n",
       " 'mit_bih_test_164',\n",
       " 'mit_bih_test_165',\n",
       " 'mit_bih_test_166',\n",
       " 'mit_bih_test_167',\n",
       " 'mit_bih_test_168',\n",
       " 'mit_bih_test_169',\n",
       " 'mit_bih_test_17',\n",
       " 'mit_bih_test_170',\n",
       " 'mit_bih_test_171',\n",
       " 'mit_bih_test_172',\n",
       " 'mit_bih_test_173',\n",
       " 'mit_bih_test_174',\n",
       " 'mit_bih_test_175',\n",
       " 'mit_bih_test_176',\n",
       " 'mit_bih_test_177',\n",
       " 'mit_bih_test_178',\n",
       " 'mit_bih_test_179',\n",
       " 'mit_bih_test_18',\n",
       " 'mit_bih_test_180',\n",
       " 'mit_bih_test_181',\n",
       " 'mit_bih_test_182',\n",
       " 'mit_bih_test_183',\n",
       " 'mit_bih_test_184',\n",
       " 'mit_bih_test_185',\n",
       " 'mit_bih_test_186',\n",
       " 'mit_bih_test_19',\n",
       " 'mit_bih_test_2',\n",
       " 'mit_bih_test_20',\n",
       " 'mit_bih_test_21',\n",
       " 'mit_bih_test_22',\n",
       " 'mit_bih_test_23',\n",
       " 'mit_bih_test_24',\n",
       " 'mit_bih_test_25',\n",
       " 'mit_bih_test_26',\n",
       " 'mit_bih_test_27',\n",
       " 'mit_bih_test_28',\n",
       " 'mit_bih_test_29',\n",
       " 'mit_bih_test_3',\n",
       " 'mit_bih_test_30',\n",
       " 'mit_bih_test_31',\n",
       " 'mit_bih_test_32',\n",
       " 'mit_bih_test_33',\n",
       " 'mit_bih_test_34',\n",
       " 'mit_bih_test_35',\n",
       " 'mit_bih_test_36',\n",
       " 'mit_bih_test_37',\n",
       " 'mit_bih_test_38',\n",
       " 'mit_bih_test_39',\n",
       " 'mit_bih_test_4',\n",
       " 'mit_bih_test_40',\n",
       " 'mit_bih_test_41',\n",
       " 'mit_bih_test_42',\n",
       " 'mit_bih_test_43',\n",
       " 'mit_bih_test_44',\n",
       " 'mit_bih_test_45',\n",
       " 'mit_bih_test_46',\n",
       " 'mit_bih_test_47',\n",
       " 'mit_bih_test_48',\n",
       " 'mit_bih_test_49',\n",
       " 'mit_bih_test_5',\n",
       " 'mit_bih_test_50',\n",
       " 'mit_bih_test_51',\n",
       " 'mit_bih_test_52',\n",
       " 'mit_bih_test_53',\n",
       " 'mit_bih_test_54',\n",
       " 'mit_bih_test_55',\n",
       " 'mit_bih_test_56',\n",
       " 'mit_bih_test_57',\n",
       " 'mit_bih_test_58',\n",
       " 'mit_bih_test_59',\n",
       " 'mit_bih_test_6',\n",
       " 'mit_bih_test_60',\n",
       " 'mit_bih_test_61',\n",
       " 'mit_bih_test_62',\n",
       " 'mit_bih_test_63',\n",
       " 'mit_bih_test_64',\n",
       " 'mit_bih_test_65',\n",
       " 'mit_bih_test_66',\n",
       " 'mit_bih_test_67',\n",
       " 'mit_bih_test_68',\n",
       " 'mit_bih_test_69',\n",
       " 'mit_bih_test_7',\n",
       " 'mit_bih_test_70',\n",
       " 'mit_bih_test_71',\n",
       " 'mit_bih_test_72',\n",
       " 'mit_bih_test_73',\n",
       " 'mit_bih_test_74',\n",
       " 'mit_bih_test_75',\n",
       " 'mit_bih_test_76',\n",
       " 'mit_bih_test_77',\n",
       " 'mit_bih_test_78',\n",
       " 'mit_bih_test_79',\n",
       " 'mit_bih_test_8',\n",
       " 'mit_bih_test_80',\n",
       " 'mit_bih_test_81',\n",
       " 'mit_bih_test_82',\n",
       " 'mit_bih_test_83',\n",
       " 'mit_bih_test_84',\n",
       " 'mit_bih_test_85',\n",
       " 'mit_bih_test_86',\n",
       " 'mit_bih_test_87',\n",
       " 'mit_bih_test_88',\n",
       " 'mit_bih_test_89',\n",
       " 'mit_bih_test_9',\n",
       " 'mit_bih_test_90',\n",
       " 'mit_bih_test_91',\n",
       " 'mit_bih_test_92',\n",
       " 'mit_bih_test_93',\n",
       " 'mit_bih_test_94',\n",
       " 'mit_bih_test_95',\n",
       " 'mit_bih_test_96',\n",
       " 'mit_bih_test_97',\n",
       " 'mit_bih_test_98',\n",
       " 'mit_bih_test_99',\n",
       " 'mit_bih_test_label',\n",
       " 'mit_bih_train_0',\n",
       " 'mit_bih_train_1',\n",
       " 'mit_bih_train_10',\n",
       " 'mit_bih_train_100',\n",
       " 'mit_bih_train_101',\n",
       " 'mit_bih_train_102',\n",
       " 'mit_bih_train_103',\n",
       " 'mit_bih_train_104',\n",
       " 'mit_bih_train_105',\n",
       " 'mit_bih_train_106',\n",
       " 'mit_bih_train_107',\n",
       " 'mit_bih_train_108',\n",
       " 'mit_bih_train_109',\n",
       " 'mit_bih_train_11',\n",
       " 'mit_bih_train_110',\n",
       " 'mit_bih_train_111',\n",
       " 'mit_bih_train_112',\n",
       " 'mit_bih_train_113',\n",
       " 'mit_bih_train_114',\n",
       " 'mit_bih_train_115',\n",
       " 'mit_bih_train_116',\n",
       " 'mit_bih_train_117',\n",
       " 'mit_bih_train_118',\n",
       " 'mit_bih_train_119',\n",
       " 'mit_bih_train_12',\n",
       " 'mit_bih_train_120',\n",
       " 'mit_bih_train_121',\n",
       " 'mit_bih_train_122',\n",
       " 'mit_bih_train_123',\n",
       " 'mit_bih_train_124',\n",
       " 'mit_bih_train_125',\n",
       " 'mit_bih_train_126',\n",
       " 'mit_bih_train_127',\n",
       " 'mit_bih_train_128',\n",
       " 'mit_bih_train_129',\n",
       " 'mit_bih_train_13',\n",
       " 'mit_bih_train_130',\n",
       " 'mit_bih_train_131',\n",
       " 'mit_bih_train_132',\n",
       " 'mit_bih_train_133',\n",
       " 'mit_bih_train_134',\n",
       " 'mit_bih_train_135',\n",
       " 'mit_bih_train_136',\n",
       " 'mit_bih_train_137',\n",
       " 'mit_bih_train_138',\n",
       " 'mit_bih_train_139',\n",
       " 'mit_bih_train_14',\n",
       " 'mit_bih_train_140',\n",
       " 'mit_bih_train_141',\n",
       " 'mit_bih_train_142',\n",
       " 'mit_bih_train_143',\n",
       " 'mit_bih_train_144',\n",
       " 'mit_bih_train_145',\n",
       " 'mit_bih_train_146',\n",
       " 'mit_bih_train_147',\n",
       " 'mit_bih_train_148',\n",
       " 'mit_bih_train_149',\n",
       " 'mit_bih_train_15',\n",
       " 'mit_bih_train_150',\n",
       " 'mit_bih_train_151',\n",
       " 'mit_bih_train_152',\n",
       " 'mit_bih_train_153',\n",
       " 'mit_bih_train_154',\n",
       " 'mit_bih_train_155',\n",
       " 'mit_bih_train_156',\n",
       " 'mit_bih_train_157',\n",
       " 'mit_bih_train_158',\n",
       " 'mit_bih_train_159',\n",
       " 'mit_bih_train_16',\n",
       " 'mit_bih_train_160',\n",
       " 'mit_bih_train_161',\n",
       " 'mit_bih_train_162',\n",
       " 'mit_bih_train_163',\n",
       " 'mit_bih_train_164',\n",
       " 'mit_bih_train_165',\n",
       " 'mit_bih_train_166',\n",
       " 'mit_bih_train_167',\n",
       " 'mit_bih_train_168',\n",
       " 'mit_bih_train_169',\n",
       " 'mit_bih_train_17',\n",
       " 'mit_bih_train_170',\n",
       " 'mit_bih_train_171',\n",
       " 'mit_bih_train_172',\n",
       " 'mit_bih_train_173',\n",
       " 'mit_bih_train_174',\n",
       " 'mit_bih_train_175',\n",
       " 'mit_bih_train_176',\n",
       " 'mit_bih_train_177',\n",
       " 'mit_bih_train_178',\n",
       " 'mit_bih_train_179',\n",
       " 'mit_bih_train_18',\n",
       " 'mit_bih_train_180',\n",
       " 'mit_bih_train_181',\n",
       " 'mit_bih_train_182',\n",
       " 'mit_bih_train_183',\n",
       " 'mit_bih_train_184',\n",
       " 'mit_bih_train_185',\n",
       " 'mit_bih_train_186',\n",
       " 'mit_bih_train_19',\n",
       " 'mit_bih_train_2',\n",
       " 'mit_bih_train_20',\n",
       " 'mit_bih_train_21',\n",
       " 'mit_bih_train_22',\n",
       " 'mit_bih_train_23',\n",
       " 'mit_bih_train_24',\n",
       " 'mit_bih_train_25',\n",
       " 'mit_bih_train_26',\n",
       " 'mit_bih_train_27',\n",
       " 'mit_bih_train_28',\n",
       " 'mit_bih_train_29',\n",
       " 'mit_bih_train_3',\n",
       " 'mit_bih_train_30',\n",
       " 'mit_bih_train_31',\n",
       " 'mit_bih_train_32',\n",
       " 'mit_bih_train_33',\n",
       " 'mit_bih_train_34',\n",
       " 'mit_bih_train_35',\n",
       " 'mit_bih_train_36',\n",
       " 'mit_bih_train_37',\n",
       " 'mit_bih_train_38',\n",
       " 'mit_bih_train_39',\n",
       " 'mit_bih_train_4',\n",
       " 'mit_bih_train_40',\n",
       " 'mit_bih_train_41',\n",
       " 'mit_bih_train_42',\n",
       " 'mit_bih_train_43',\n",
       " 'mit_bih_train_44',\n",
       " 'mit_bih_train_45',\n",
       " 'mit_bih_train_46',\n",
       " 'mit_bih_train_47',\n",
       " 'mit_bih_train_48',\n",
       " 'mit_bih_train_49',\n",
       " 'mit_bih_train_5',\n",
       " 'mit_bih_train_50',\n",
       " 'mit_bih_train_51',\n",
       " 'mit_bih_train_52',\n",
       " 'mit_bih_train_53',\n",
       " 'mit_bih_train_54',\n",
       " 'mit_bih_train_55',\n",
       " 'mit_bih_train_56',\n",
       " 'mit_bih_train_57',\n",
       " 'mit_bih_train_58',\n",
       " 'mit_bih_train_59',\n",
       " 'mit_bih_train_6',\n",
       " 'mit_bih_train_60',\n",
       " 'mit_bih_train_61',\n",
       " 'mit_bih_train_62',\n",
       " 'mit_bih_train_63',\n",
       " 'mit_bih_train_64',\n",
       " 'mit_bih_train_65',\n",
       " 'mit_bih_train_66',\n",
       " 'mit_bih_train_67',\n",
       " 'mit_bih_train_68',\n",
       " 'mit_bih_train_69',\n",
       " 'mit_bih_train_7',\n",
       " 'mit_bih_train_70',\n",
       " 'mit_bih_train_71',\n",
       " 'mit_bih_train_72',\n",
       " 'mit_bih_train_73',\n",
       " 'mit_bih_train_74',\n",
       " 'mit_bih_train_75',\n",
       " 'mit_bih_train_76',\n",
       " 'mit_bih_train_77',\n",
       " 'mit_bih_train_78',\n",
       " 'mit_bih_train_79',\n",
       " 'mit_bih_train_8',\n",
       " 'mit_bih_train_80',\n",
       " 'mit_bih_train_81',\n",
       " 'mit_bih_train_82',\n",
       " 'mit_bih_train_83',\n",
       " 'mit_bih_train_84',\n",
       " 'mit_bih_train_85',\n",
       " 'mit_bih_train_86',\n",
       " 'mit_bih_train_87',\n",
       " 'mit_bih_train_88',\n",
       " 'mit_bih_train_89',\n",
       " 'mit_bih_train_9',\n",
       " 'mit_bih_train_90',\n",
       " 'mit_bih_train_91',\n",
       " 'mit_bih_train_92',\n",
       " 'mit_bih_train_93',\n",
       " 'mit_bih_train_94',\n",
       " 'mit_bih_train_95',\n",
       " 'mit_bih_train_96',\n",
       " 'mit_bih_train_97',\n",
       " 'mit_bih_train_98',\n",
       " 'mit_bih_train_99',\n",
       " 'mit_bih_train_label',\n",
       " 'ptb_0',\n",
       " 'ptb_1',\n",
       " 'ptb_10',\n",
       " 'ptb_100',\n",
       " 'ptb_101',\n",
       " 'ptb_102',\n",
       " 'ptb_103',\n",
       " 'ptb_104',\n",
       " 'ptb_105',\n",
       " 'ptb_106',\n",
       " 'ptb_107',\n",
       " 'ptb_108',\n",
       " 'ptb_109',\n",
       " 'ptb_11',\n",
       " 'ptb_110',\n",
       " 'ptb_111',\n",
       " 'ptb_112',\n",
       " 'ptb_113',\n",
       " 'ptb_114',\n",
       " 'ptb_115',\n",
       " 'ptb_116',\n",
       " 'ptb_117',\n",
       " 'ptb_118',\n",
       " 'ptb_119',\n",
       " 'ptb_12',\n",
       " 'ptb_120',\n",
       " 'ptb_121',\n",
       " 'ptb_122',\n",
       " 'ptb_123',\n",
       " 'ptb_124',\n",
       " 'ptb_125',\n",
       " 'ptb_126',\n",
       " 'ptb_127',\n",
       " 'ptb_128',\n",
       " 'ptb_129',\n",
       " 'ptb_13',\n",
       " 'ptb_130',\n",
       " 'ptb_131',\n",
       " 'ptb_132',\n",
       " 'ptb_133',\n",
       " 'ptb_134',\n",
       " 'ptb_135',\n",
       " 'ptb_136',\n",
       " 'ptb_137',\n",
       " 'ptb_138',\n",
       " 'ptb_139',\n",
       " 'ptb_14',\n",
       " 'ptb_140',\n",
       " 'ptb_141',\n",
       " 'ptb_142',\n",
       " 'ptb_143',\n",
       " 'ptb_144',\n",
       " 'ptb_145',\n",
       " 'ptb_146',\n",
       " 'ptb_147',\n",
       " 'ptb_148',\n",
       " 'ptb_149',\n",
       " 'ptb_15',\n",
       " 'ptb_150',\n",
       " 'ptb_151',\n",
       " 'ptb_152',\n",
       " 'ptb_153',\n",
       " 'ptb_154',\n",
       " 'ptb_155',\n",
       " 'ptb_156',\n",
       " 'ptb_157',\n",
       " 'ptb_158',\n",
       " 'ptb_159',\n",
       " 'ptb_16',\n",
       " 'ptb_160',\n",
       " 'ptb_161',\n",
       " 'ptb_162',\n",
       " 'ptb_163',\n",
       " 'ptb_164',\n",
       " 'ptb_165',\n",
       " 'ptb_166',\n",
       " 'ptb_167',\n",
       " 'ptb_168',\n",
       " 'ptb_169',\n",
       " 'ptb_17',\n",
       " 'ptb_170',\n",
       " 'ptb_171',\n",
       " 'ptb_172',\n",
       " 'ptb_173',\n",
       " 'ptb_174',\n",
       " 'ptb_175',\n",
       " 'ptb_176',\n",
       " 'ptb_177',\n",
       " 'ptb_178',\n",
       " 'ptb_179',\n",
       " 'ptb_18',\n",
       " 'ptb_180',\n",
       " 'ptb_181',\n",
       " 'ptb_182',\n",
       " 'ptb_183',\n",
       " 'ptb_184',\n",
       " 'ptb_185',\n",
       " 'ptb_186',\n",
       " 'ptb_19',\n",
       " 'ptb_2',\n",
       " 'ptb_20',\n",
       " 'ptb_21',\n",
       " 'ptb_22',\n",
       " 'ptb_23',\n",
       " 'ptb_24',\n",
       " 'ptb_25',\n",
       " 'ptb_26',\n",
       " 'ptb_27',\n",
       " 'ptb_28',\n",
       " 'ptb_29',\n",
       " 'ptb_3',\n",
       " 'ptb_30',\n",
       " 'ptb_31',\n",
       " 'ptb_32',\n",
       " 'ptb_33',\n",
       " 'ptb_34',\n",
       " 'ptb_35',\n",
       " 'ptb_36',\n",
       " 'ptb_37',\n",
       " 'ptb_38',\n",
       " 'ptb_39',\n",
       " 'ptb_4',\n",
       " 'ptb_40',\n",
       " 'ptb_41',\n",
       " 'ptb_42',\n",
       " 'ptb_43',\n",
       " 'ptb_44',\n",
       " 'ptb_45',\n",
       " 'ptb_46',\n",
       " 'ptb_47',\n",
       " 'ptb_48',\n",
       " 'ptb_49',\n",
       " 'ptb_5',\n",
       " 'ptb_50',\n",
       " 'ptb_51',\n",
       " 'ptb_52',\n",
       " 'ptb_53',\n",
       " 'ptb_54',\n",
       " 'ptb_55',\n",
       " 'ptb_56',\n",
       " 'ptb_57',\n",
       " 'ptb_58',\n",
       " 'ptb_59',\n",
       " 'ptb_6',\n",
       " 'ptb_60',\n",
       " 'ptb_61',\n",
       " 'ptb_62',\n",
       " 'ptb_63',\n",
       " 'ptb_64',\n",
       " 'ptb_65',\n",
       " 'ptb_66',\n",
       " 'ptb_67',\n",
       " 'ptb_68',\n",
       " 'ptb_69',\n",
       " 'ptb_7',\n",
       " 'ptb_70',\n",
       " 'ptb_71',\n",
       " 'ptb_72',\n",
       " 'ptb_73',\n",
       " 'ptb_74',\n",
       " 'ptb_75',\n",
       " 'ptb_76',\n",
       " 'ptb_77',\n",
       " 'ptb_78',\n",
       " 'ptb_79',\n",
       " 'ptb_8',\n",
       " 'ptb_80',\n",
       " 'ptb_81',\n",
       " 'ptb_82',\n",
       " 'ptb_83',\n",
       " 'ptb_84',\n",
       " 'ptb_85',\n",
       " 'ptb_86',\n",
       " 'ptb_87',\n",
       " 'ptb_88',\n",
       " 'ptb_89',\n",
       " 'ptb_9',\n",
       " 'ptb_90',\n",
       " 'ptb_91',\n",
       " 'ptb_92',\n",
       " 'ptb_93',\n",
       " 'ptb_94',\n",
       " 'ptb_95',\n",
       " 'ptb_96',\n",
       " 'ptb_97',\n",
       " 'ptb_98',\n",
       " 'ptb_99',\n",
       " 'ptb_label']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the ecg dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names related to the mit_bih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mit_bih_train_0','mit_bih_train_1','mit_bih_train_10','mit_bih_train_100','mit_bih_train_101','mit_bih_train_102','mit_bih_train_103','mit_bih_train_104','mit_bih_train_105','mit_bih_train_106','mit_bih_train_107','mit_bih_train_108','mit_bih_train_109','mit_bih_train_11','mit_bih_train_110','mit_bih_train_111','mit_bih_train_112','mit_bih_train_113','mit_bih_train_114','mit_bih_train_115','mit_bih_train_116','mit_bih_train_117','mit_bih_train_118','mit_bih_train_119','mit_bih_train_12','mit_bih_train_120','mit_bih_train_121','mit_bih_train_122','mit_bih_train_123','mit_bih_train_124','mit_bih_train_125','mit_bih_train_126','mit_bih_train_127','mit_bih_train_128','mit_bih_train_129','mit_bih_train_13','mit_bih_train_130','mit_bih_train_131','mit_bih_train_132','mit_bih_train_133','mit_bih_train_134','mit_bih_train_135','mit_bih_train_136','mit_bih_train_137','mit_bih_train_138','mit_bih_train_139','mit_bih_train_14','mit_bih_train_140','mit_bih_train_141','mit_bih_train_142','mit_bih_train_143','mit_bih_train_144','mit_bih_train_145','mit_bih_train_146','mit_bih_train_147','mit_bih_train_148','mit_bih_train_149','mit_bih_train_15','mit_bih_train_150','mit_bih_train_151','mit_bih_train_152','mit_bih_train_153','mit_bih_train_154','mit_bih_train_155','mit_bih_train_156','mit_bih_train_157','mit_bih_train_158','mit_bih_train_159','mit_bih_train_16','mit_bih_train_160','mit_bih_train_161','mit_bih_train_162','mit_bih_train_163','mit_bih_train_164','mit_bih_train_165','mit_bih_train_166','mit_bih_train_167','mit_bih_train_168','mit_bih_train_169','mit_bih_train_17','mit_bih_train_170','mit_bih_train_171','mit_bih_train_172','mit_bih_train_173','mit_bih_train_174','mit_bih_train_175','mit_bih_train_176','mit_bih_train_177','mit_bih_train_178','mit_bih_train_179','mit_bih_train_18','mit_bih_train_180','mit_bih_train_181','mit_bih_train_182','mit_bih_train_183','mit_bih_train_184','mit_bih_train_185','mit_bih_train_186','mit_bih_train_19','mit_bih_train_2','mit_bih_train_20','mit_bih_train_21','mit_bih_train_22','mit_bih_train_23','mit_bih_train_24','mit_bih_train_25','mit_bih_train_26','mit_bih_train_27','mit_bih_train_28','mit_bih_train_29','mit_bih_train_3','mit_bih_train_30','mit_bih_train_31','mit_bih_train_32','mit_bih_train_33','mit_bih_train_34','mit_bih_train_35','mit_bih_train_36','mit_bih_train_37','mit_bih_train_38','mit_bih_train_39','mit_bih_train_4','mit_bih_train_40','mit_bih_train_41','mit_bih_train_42','mit_bih_train_43','mit_bih_train_44','mit_bih_train_45','mit_bih_train_46','mit_bih_train_47','mit_bih_train_48','mit_bih_train_49','mit_bih_train_5','mit_bih_train_50','mit_bih_train_51','mit_bih_train_52','mit_bih_train_53','mit_bih_train_54','mit_bih_train_55','mit_bih_train_56','mit_bih_train_57','mit_bih_train_58','mit_bih_train_59','mit_bih_train_6','mit_bih_train_60','mit_bih_train_61','mit_bih_train_62','mit_bih_train_63','mit_bih_train_64','mit_bih_train_65','mit_bih_train_66','mit_bih_train_67','mit_bih_train_68','mit_bih_train_69','mit_bih_train_7','mit_bih_train_70','mit_bih_train_71','mit_bih_train_72','mit_bih_train_73','mit_bih_train_74','mit_bih_train_75','mit_bih_train_76','mit_bih_train_77','mit_bih_train_78','mit_bih_train_79','mit_bih_train_8','mit_bih_train_80','mit_bih_train_81','mit_bih_train_82','mit_bih_train_83','mit_bih_train_84','mit_bih_train_85','mit_bih_train_86','mit_bih_train_87','mit_bih_train_88','mit_bih_train_89','mit_bih_train_9','mit_bih_train_90','mit_bih_train_91','mit_bih_train_92','mit_bih_train_93','mit_bih_train_94','mit_bih_train_95','mit_bih_train_96','mit_bih_train_97','mit_bih_train_98','mit_bih_train_99','mit_bih_train_label'\n",
      "'mit_bih_test_0','mit_bih_test_1','mit_bih_test_10','mit_bih_test_100','mit_bih_test_101','mit_bih_test_102','mit_bih_test_103','mit_bih_test_104','mit_bih_test_105','mit_bih_test_106','mit_bih_test_107','mit_bih_test_108','mit_bih_test_109','mit_bih_test_11','mit_bih_test_110','mit_bih_test_111','mit_bih_test_112','mit_bih_test_113','mit_bih_test_114','mit_bih_test_115','mit_bih_test_116','mit_bih_test_117','mit_bih_test_118','mit_bih_test_119','mit_bih_test_12','mit_bih_test_120','mit_bih_test_121','mit_bih_test_122','mit_bih_test_123','mit_bih_test_124','mit_bih_test_125','mit_bih_test_126','mit_bih_test_127','mit_bih_test_128','mit_bih_test_129','mit_bih_test_13','mit_bih_test_130','mit_bih_test_131','mit_bih_test_132','mit_bih_test_133','mit_bih_test_134','mit_bih_test_135','mit_bih_test_136','mit_bih_test_137','mit_bih_test_138','mit_bih_test_139','mit_bih_test_14','mit_bih_test_140','mit_bih_test_141','mit_bih_test_142','mit_bih_test_143','mit_bih_test_144','mit_bih_test_145','mit_bih_test_146','mit_bih_test_147','mit_bih_test_148','mit_bih_test_149','mit_bih_test_15','mit_bih_test_150','mit_bih_test_151','mit_bih_test_152','mit_bih_test_153','mit_bih_test_154','mit_bih_test_155','mit_bih_test_156','mit_bih_test_157','mit_bih_test_158','mit_bih_test_159','mit_bih_test_16','mit_bih_test_160','mit_bih_test_161','mit_bih_test_162','mit_bih_test_163','mit_bih_test_164','mit_bih_test_165','mit_bih_test_166','mit_bih_test_167','mit_bih_test_168','mit_bih_test_169','mit_bih_test_17','mit_bih_test_170','mit_bih_test_171','mit_bih_test_172','mit_bih_test_173','mit_bih_test_174','mit_bih_test_175','mit_bih_test_176','mit_bih_test_177','mit_bih_test_178','mit_bih_test_179','mit_bih_test_18','mit_bih_test_180','mit_bih_test_181','mit_bih_test_182','mit_bih_test_183','mit_bih_test_184','mit_bih_test_185','mit_bih_test_186','mit_bih_test_19','mit_bih_test_2','mit_bih_test_20','mit_bih_test_21','mit_bih_test_22','mit_bih_test_23','mit_bih_test_24','mit_bih_test_25','mit_bih_test_26','mit_bih_test_27','mit_bih_test_28','mit_bih_test_29','mit_bih_test_3','mit_bih_test_30','mit_bih_test_31','mit_bih_test_32','mit_bih_test_33','mit_bih_test_34','mit_bih_test_35','mit_bih_test_36','mit_bih_test_37','mit_bih_test_38','mit_bih_test_39','mit_bih_test_4','mit_bih_test_40','mit_bih_test_41','mit_bih_test_42','mit_bih_test_43','mit_bih_test_44','mit_bih_test_45','mit_bih_test_46','mit_bih_test_47','mit_bih_test_48','mit_bih_test_49','mit_bih_test_5','mit_bih_test_50','mit_bih_test_51','mit_bih_test_52','mit_bih_test_53','mit_bih_test_54','mit_bih_test_55','mit_bih_test_56','mit_bih_test_57','mit_bih_test_58','mit_bih_test_59','mit_bih_test_6','mit_bih_test_60','mit_bih_test_61','mit_bih_test_62','mit_bih_test_63','mit_bih_test_64','mit_bih_test_65','mit_bih_test_66','mit_bih_test_67','mit_bih_test_68','mit_bih_test_69','mit_bih_test_7','mit_bih_test_70','mit_bih_test_71','mit_bih_test_72','mit_bih_test_73','mit_bih_test_74','mit_bih_test_75','mit_bih_test_76','mit_bih_test_77','mit_bih_test_78','mit_bih_test_79','mit_bih_test_8','mit_bih_test_80','mit_bih_test_81','mit_bih_test_82','mit_bih_test_83','mit_bih_test_84','mit_bih_test_85','mit_bih_test_86','mit_bih_test_87','mit_bih_test_88','mit_bih_test_89','mit_bih_test_9','mit_bih_test_90','mit_bih_test_91','mit_bih_test_92','mit_bih_test_93','mit_bih_test_94','mit_bih_test_95','mit_bih_test_96','mit_bih_test_97','mit_bih_test_98','mit_bih_test_99','mit_bih_test_label'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired train, test tag names\n",
    "tags_train = name[188:376]\n",
    "tags_test = name[:188]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_train = \",\".join(f\"'{tag}'\" for tag in tags_train)\n",
    "tags_test = \",\".join(f\"'{tag}'\" for tag in tags_test)\n",
    "\n",
    "# Check the selected train, test tag names\n",
    "print(tags_train)\n",
    "print(tags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ECG Dataset\n",
    "* Load the entire dataset upon data loading.\n",
    "\n",
    "    * Label description:\n",
    "\n",
    "        * N (Normal): 0\n",
    "            * Normal heartbeat\n",
    "            * Indicates a normal heart rhythm, reflecting regular electrical activity in the ECG.\n",
    "        * S (Supraventricular ectopic beat): 1\n",
    "            * Supraventricular ectopic beat\n",
    "            * Abnormal heartbeats originating in the atria or atrioventricular node, representing abnormal beats that start from the upper chambers of the heart.\n",
    "        * V (Ventricular ectopic beat): 2\n",
    "            * Ventricular ectopic beat\n",
    "            * Abnormal heartbeats originating from the ventricles, representing fast or abnormal electrical activity in the ventricles.\n",
    "        * F (Fusion of ventricular and normal beat): 3\n",
    "            * Fusion of ventricular and normal beat\n",
    "            * Occurs when a normal heartbeat and a ventricular ectopic beat happen simultaneously, leading to a fused heartbeat appearance.\n",
    "        * Q (Unknown beat): 4\n",
    "            * Unknown beat\n",
    "            * Represents beats that cannot be classified, typically due to insufficient information or difficulty in classifying the specific beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading parameter settings\n",
    "\n",
    "# Set the tag table name\n",
    "table = 'ecg'\n",
    "# Set the train, test tag names\n",
    "name_train = quote(tags_train, safe=\":/\")\n",
    "name_test = quote(tags_test, safe=\":/\")\n",
    "# Set the time format  \n",
    "timeformat = 'default'\n",
    "# Set the data start time\n",
    "start_time = quote('2024-10-14 00:00:00')\n",
    "# Set the data end time\n",
    "end_time = quote('2024-12-29 00:03:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data  \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # Set TIME column\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Determine the label column dynamically (for train and test sets)\n",
    "    label_col = [col for col in df.columns if col.endswith('_label')][0]\n",
    "\n",
    "    # Sort column names in numerical order, excluding the 'TIME' and the dynamic label column\n",
    "    df = df.reindex(['TIME'] + sorted([col for col in df.columns if col not in ['TIME', label_col] and col.split('_')[-1].isdigit()], key=lambda x: int(x.split('_')[-1])) + [label_col], axis=1)\n",
    "    \n",
    "    # Convert label column data to integer type\n",
    "    df[label_col] = df[label_col].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = data_load(table, name_train, start_time, end_time, timeformat)\n",
    "test = data_load(table, name_test, start_time, end_time, timeformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train, valid = train_test_split(train, test_size=0.1, shuffle=False)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 MinMax Scaling\n",
    "* 2 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler Setup\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Scaler\n",
    "train_ = scaler.fit_transform(train.iloc[:,1:-1].values)\n",
    "valid_ = scaler.transform(valid.iloc[:,1:-1].values)\n",
    "test_ = scaler.transform(test.iloc[:,1:-1].values)\n",
    "\n",
    "# Set DataFrames\n",
    "train_scaled = pd.DataFrame(train_)\n",
    "valid_scaled = pd.DataFrame(valid_)\n",
    "test_scaled = pd.DataFrame(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    65228\n",
      "4     5805\n",
      "2     5184\n",
      "1     1998\n",
      "3      582\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    7242\n",
      "4     626\n",
      "2     604\n",
      "1     225\n",
      "3      59\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    18117\n",
      "4     1608\n",
      "2     1448\n",
      "1      556\n",
      "3      162\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Applying PCA\n",
    "# Select principal components explaining 95% of the variance\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Apply PCA\n",
    "train_scaled_ = pca.fit_transform(train_scaled)\n",
    "valid_scaled_ = pca.transform(valid_scaled)\n",
    "test_scaled_ = pca.transform(test_scaled)\n",
    "\n",
    "# Set DataFrames\n",
    "train_scaled_ = pd.DataFrame(train_scaled_)\n",
    "valid_scaled_ = pd.DataFrame(valid_scaled_)\n",
    "test_scaled_ = pd.DataFrame(test_scaled_)\n",
    "\n",
    "# Add labels\n",
    "train_scaled_['label'] = train['mit_bih_train_label'].values\n",
    "valid_scaled_['label'] = valid['mit_bih_train_label'].values\n",
    "test_scaled_['label'] = test['mit_bih_test_label'].values\n",
    "\n",
    "print(train_scaled_['label'].value_counts())\n",
    "print(valid_scaled_['label'].value_counts())\n",
    "print(test_scaled_['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Loader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.freq_data = df.iloc[:,:-1]\n",
    "        self.label = df.iloc[:,-1:].squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.freq_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        input_time_data = self.freq_data.iloc[index,:]\n",
    "        input_time_data = torch.Tensor(input_time_data).expand(1, input_time_data.shape[0])\n",
    "        label = self.label[index]\n",
    "\n",
    "        return input_time_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up datasets  \n",
    "train_ = ECG_Dataset(train_scaled_)\n",
    "valid_ = ECG_Dataset(valid_scaled_)\n",
    "test_ = ECG_Dataset(test_scaled_)\n",
    "\n",
    "# Set up data loaders\n",
    "train_dataloader = DataLoader(train_, batch_size=512, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_, batch_size=512, shuffle=True)\n",
    "test_dataloader = DataLoader(test_, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 35])\n"
     ]
    }
   ],
   "source": [
    "# Verify DataLoader application and check the shape of the input data\n",
    "print(list(train_dataloader)[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using ResNet1d model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet 1D Model Setup\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=4):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet1D(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = ResNet1D(ResidualBlock, [2, 2, 2, 2], num_classes=5).to(device)\n",
    "\n",
    "# Adjust weights for each class\n",
    "class_weights = torch.tensor([1.0, 5.0, 1.0, 5.0, 1.0]).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "* Save the model with the Best F1 Score based on the validation data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860929d491b24a07a895d1adb51ce8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.5517776709098321, train acc: 88.9666\n",
      "\n",
      "train loss: 0.41100094370640716, train acc: 94.6356\n",
      "\n",
      "train loss: 0.3474797205233471, train acc: 95.5442\n",
      "\n",
      "train loss: 0.3073169313274421, train acc: 96.1610\n",
      "\n",
      "train loss: 0.28032552522498294, train acc: 96.3920\n",
      "\n",
      "train loss: 0.25926292981565385, train acc: 96.6839\n",
      "\n",
      "train loss: 0.2431209825967648, train acc: 96.8704\n",
      "\n",
      "train loss: 0.22951460080281771, train acc: 97.0545\n",
      "\n",
      "train loss: 0.21780475341643693, train acc: 97.1661\n",
      "\n",
      "train loss: 0.20768087327286797, train acc: 97.3413\n",
      "\n",
      "train loss: 0.19900040455831058, train acc: 97.4098\n",
      "\n",
      "train loss: 0.19123293478364425, train acc: 97.4263\n",
      "\n",
      "train loss: 0.1840641477380644, train acc: 97.6154\n",
      "\n",
      "train loss: 0.1775759693852009, train acc: 97.7030\n",
      "\n",
      "train loss: 0.17158821093329749, train acc: 97.7448\n",
      "\n",
      "train loss: 0.16599316247239976, train acc: 97.8692\n",
      "\n",
      "train loss: 0.16087136379421055, train acc: 97.9822\n",
      "\n",
      "train loss: 0.15633973515892208, train acc: 98.0063\n",
      "\n",
      "train loss: 0.15231585096802133, train acc: 97.9403\n",
      "\n",
      "train loss: 0.14828181367597312, train acc: 98.0418\n",
      "\n",
      "train loss: 0.1442347033238459, train acc: 98.2956\n",
      "\n",
      "train loss: 0.1402879770292099, train acc: 98.3806\n",
      "\n",
      "train loss: 0.13673533610200256, train acc: 98.4454\n",
      "\n",
      "train loss: 0.1333874292798679, train acc: 98.5076\n",
      "\n",
      "train loss: 0.13016174730542418, train acc: 98.4822\n",
      "\n",
      "train loss: 0.12711863553539463, train acc: 98.5862\n",
      "\n",
      "train loss: 0.12423510072724166, train acc: 98.6319\n",
      "\n",
      "train loss: 0.12140934223266829, train acc: 98.7398\n",
      "\n",
      "train loss: 0.1188177510917077, train acc: 98.7563\n",
      "\n",
      "train loss: 0.11644589491601778, train acc: 98.6713\n",
      "\n",
      "train loss: 0.11406869851675076, train acc: 98.8324\n",
      "\n",
      "train loss: 0.11181604909828136, train acc: 98.8159\n",
      "\n",
      "train loss: 0.10970393961047353, train acc: 98.8248\n",
      "\n",
      "train loss: 0.10754949444020204, train acc: 98.9708\n",
      "\n",
      "train loss: 0.10560281211847236, train acc: 98.8857\n",
      "\n",
      "train loss: 0.10379944699435074, train acc: 98.8654\n",
      "\n",
      "train loss: 0.10201390582106082, train acc: 98.9441\n",
      "\n",
      "train loss: 0.10018997303516078, train acc: 99.0558\n",
      "\n",
      "train loss: 0.0984780484298911, train acc: 99.0292\n",
      "\n",
      "train loss: 0.09683901722204277, train acc: 99.1319\n",
      "\n",
      "train loss: 0.0953738116663649, train acc: 98.9606\n",
      "\n",
      "train loss: 0.09403064053856416, train acc: 98.9010\n",
      "\n",
      "train loss: 0.092755399578555, train acc: 98.8807\n",
      "\n",
      "train loss: 0.0912528259441484, train acc: 99.2639\n",
      "\n",
      "train loss: 0.08989697226536866, train acc: 99.1599\n",
      "\n",
      "train loss: 0.08855604172254612, train acc: 99.2017\n",
      "\n",
      "train loss: 0.0872432319260503, train acc: 99.2703\n",
      "\n",
      "train loss: 0.08603431467262063, train acc: 99.2195\n",
      "\n",
      "train loss: 0.08479316992160789, train acc: 99.2779\n",
      "\n",
      "train loss: 0.08371958198101416, train acc: 99.1459\n",
      "\n",
      "train loss: 0.08262385439860147, train acc: 99.2157\n",
      "\n",
      "train loss: 0.08162596614967511, train acc: 99.1878\n",
      "\n",
      "train loss: 0.08061160693098364, train acc: 99.2068\n",
      "\n",
      "train loss: 0.0796097892165424, train acc: 99.2474\n",
      "\n",
      "train loss: 0.07867215820010905, train acc: 99.2208\n",
      "\n",
      "train loss: 0.07761147538979075, train acc: 99.4619\n",
      "\n",
      "train loss: 0.07667103510660431, train acc: 99.3426\n",
      "\n",
      "train loss: 0.07570499505645831, train acc: 99.4086\n",
      "\n",
      "train loss: 0.07479055338856322, train acc: 99.3553\n",
      "\n",
      "train loss: 0.07392508432246923, train acc: 99.3452\n",
      "\n",
      "train loss: 0.07311368406179612, train acc: 99.2639\n",
      "\n",
      "train loss: 0.07231376788572198, train acc: 99.3578\n",
      "\n",
      "train loss: 0.07146956857962532, train acc: 99.4784\n",
      "\n",
      "train loss: 0.07073800838514382, train acc: 99.2995\n",
      "\n",
      "train loss: 0.06997878950413226, train acc: 99.3946\n",
      "\n",
      "train loss: 0.06938111329736767, train acc: 99.1446\n",
      "\n",
      "train loss: 0.06868641364375666, train acc: 99.3604\n",
      "\n",
      "train loss: 0.06793406138356137, train acc: 99.5165\n",
      "\n",
      "train loss: 0.06726820572449996, train acc: 99.3972\n",
      "\n",
      "train loss: 0.06656597877626051, train acc: 99.4619\n",
      "\n",
      "train loss: 0.06589215623495455, train acc: 99.4670\n",
      "\n",
      "train loss: 0.0652012356555713, train acc: 99.5507\n",
      "\n",
      "train loss: 0.0645083480087395, train acc: 99.6091\n",
      "\n",
      "train loss: 0.06390743672172772, train acc: 99.4708\n",
      "\n",
      "train loss: 0.06331329130395759, train acc: 99.4708\n",
      "\n",
      "train loss: 0.06277688338919471, train acc: 99.3667\n",
      "\n",
      "train loss: 0.06221734628811915, train acc: 99.4200\n",
      "\n",
      "train loss: 0.06156359998534059, train acc: 99.6942\n",
      "\n",
      "train loss: 0.061017683952438284, train acc: 99.4835\n",
      "\n",
      "train loss: 0.06043944584830156, train acc: 99.5799\n",
      "\n",
      "train loss: 0.05988784600013794, train acc: 99.5457\n",
      "\n",
      "train loss: 0.05939762848361347, train acc: 99.4657\n",
      "\n",
      "train loss: 0.05894275820106619, train acc: 99.3908\n",
      "\n",
      "train loss: 0.05852081394376486, train acc: 99.3807\n",
      "\n",
      "train loss: 0.05801972683433393, train acc: 99.5431\n",
      "\n",
      "train loss: 0.05751319024324558, train acc: 99.5875\n",
      "\n",
      "train loss: 0.05698058455355547, train acc: 99.6878\n",
      "\n",
      "train loss: 0.05650251073599524, train acc: 99.5710\n",
      "\n",
      "train loss: 0.0560221452676039, train acc: 99.6104\n",
      "\n",
      "train loss: 0.055529806205920844, train acc: 99.6853\n",
      "\n",
      "train loss: 0.05501412655568863, train acc: 99.7779\n",
      "\n",
      "train loss: 0.05460670129348405, train acc: 99.5469\n",
      "\n",
      "train loss: 0.054208436432770166, train acc: 99.5025\n",
      "\n",
      "train loss: 0.0539092867985502, train acc: 99.3591\n",
      "\n",
      "train loss: 0.05350674739935565, train acc: 99.5457\n",
      "\n",
      "train loss: 0.05307272800204869, train acc: 99.6523\n",
      "\n",
      "train loss: 0.052618406863322816, train acc: 99.7335\n",
      "\n",
      "train loss: 0.05223599396332584, train acc: 99.6015\n",
      "\n",
      "train loss: 0.051807655413512683, train acc: 99.7221\n",
      "\n",
      "train loss: 0.051442989878815314, train acc: 99.6066\n"
     ]
    }
   ],
   "source": [
    "# Initialize training loss\n",
    "train_loss = []\n",
    "# Initialize training accuracy\n",
    "train_acc = []\n",
    "# Initialize total step\n",
    "total_step = len(train_dataloader)\n",
    "# Set number of epochs\n",
    "epoch_in = trange(100, desc='training')\n",
    "# Initialize best F1 Score value\n",
    "best_f1= 0\n",
    "\n",
    "# Start model training\n",
    "for epoch in epoch_in:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "\n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = train_data[0].to(device).float()\n",
    "        labels = train_data[1].to(device).long().squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Input to the model\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Set label predictions \n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==labels).item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "    \n",
    "    # Perform validation at the end of each epoch and save the model with the best performance\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "            inputs_v = valid_data[0].to(device).float()\n",
    "            labels_v = valid_data[1].to(device).long().squeeze() \n",
    "            \n",
    "            outputs_v = model(inputs_v)\n",
    "            \n",
    "            # Set label predictions\n",
    "            _,pred_v = torch.max(outputs_v, dim=1)\n",
    "            target_v = labels_v.view_as(pred_v)\n",
    "            \n",
    "            preds_.append(pred_v)\n",
    "            targets_.append(target_v)\n",
    "            \n",
    "        # Combine predictions and labels collected from all batches\n",
    "        preds_ = torch.cat(preds_).detach().cpu().numpy()\n",
    "        targets_ = torch.cat(targets_).detach().cpu().numpy()\n",
    "        \n",
    "        f1score = f1_score(targets_, preds_,  average='macro')\n",
    "        if best_f1 < f1score:\n",
    "            best_f1 = f1score\n",
    "            # Save the best model \n",
    "            with open(\"./result/ECG_HeartBeat_General.txt\", \"a\") as text_file:\n",
    "                print('epoch=====',epoch, file=text_file)\n",
    "                print(classification_report(targets_, preds_, digits=4), file=text_file)\n",
    "            torch.save(model, f'./result/ECG_HeartBeat_General.pt') \n",
    "        epoch_in.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_ = torch.load(f'./result/ECG_HeartBeat_General.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "# Model testing\n",
    "preds_test = []\n",
    "target_test = []\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    for batch_idx, test_data in enumerate(test_dataloader):\n",
    "        inputs_t = test_data[0].to(device).float()\n",
    "        labels_t =  test_data[1].to(device).long().squeeze() \n",
    "        \n",
    "        outputs_t = model_(inputs_t)\n",
    "        \n",
    "        _,pred_t = torch.max(outputs_t, dim=1)\n",
    "        targets_t = labels_t.view_as(pred_t).to(device)\n",
    "\n",
    "        preds_test.append(pred_t)\n",
    "        target_test.append(targets_t)\n",
    "        \n",
    "    # Combine predictions and labels collected from all batches\n",
    "    preds_test = torch.cat(preds_test).detach().cpu().numpy()\n",
    "    target_test = torch.cat(target_test).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     18117\n",
      "           1       0.80      0.77      0.79       556\n",
      "           2       0.95      0.92      0.94      1448\n",
      "           3       0.72      0.76      0.74       162\n",
      "           4       0.99      0.97      0.98      1608\n",
      "\n",
      "    accuracy                           0.98     21891\n",
      "   macro avg       0.89      0.88      0.89     21891\n",
      "weighted avg       0.98      0.98      0.98     21891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
