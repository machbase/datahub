{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## 사용 라이브러리 호출\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "\n",
    "## 모델 사용 라이브러리 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "## 모델 학습 결과 경로 설정 \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Cuda 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## 랜덤 시드 설정\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 시드 설정 \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 데이터 컬럼 선택\n",
    "\n",
    "* tag name 이 많은 경우 tag name을 지정하는 것에 있어서 변수 설정이 다소 유연해짐\n",
    "* tag name 은 순서대로 불러와짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag name 출력 함수 \n",
    "def show_column(URL):\n",
    "    \n",
    "    # Tag name 데이터 로드\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # List 형식으로 변환\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tag name 출력 파라미터 설정\n",
    "table = 'wind_elec_gen'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## tag name list 생성 \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h_6hrain',\n",
       " 'h_6hsnow',\n",
       " 'h_humidity',\n",
       " 'h_rainprobability',\n",
       " 'h_raintype',\n",
       " 'h_seawave',\n",
       " 'h_skystatus',\n",
       " 'h_temperature',\n",
       " 'h_winddirection',\n",
       " 'h_windspeed',\n",
       " 'hk1_1',\n",
       " 'hk1_2',\n",
       " 'hk1_3',\n",
       " 'hk2_1',\n",
       " 'hk2_2',\n",
       " 'hk2_3',\n",
       " 'hk2_4',\n",
       " 'hk2_5',\n",
       " 's_6hrain',\n",
       " 's_6hsnow',\n",
       " 's_humidity',\n",
       " 's_rainprobability',\n",
       " 's_raintype',\n",
       " 's_seawave',\n",
       " 's_skystatus',\n",
       " 's_temperature',\n",
       " 's_winddirection',\n",
       " 's_windspeed',\n",
       " 'ss_1',\n",
       " 'ss_10',\n",
       " 'ss_2',\n",
       " 'ss_3',\n",
       " 'ss_4',\n",
       " 'ss_5',\n",
       " 'ss_6',\n",
       " 'ss_7',\n",
       " 'ss_8',\n",
       " 'ss_9']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAG Name format 변환 \n",
    "\n",
    "* 위의 과정에서 Wind Elec Gen dataset의 모든 Tag Name 을 확인후 사용할 컬럼만 뽑아서 입력할 파라미터 형태로 변환\n",
    "* 한경 제 1 발전소 관련 Tag Name 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'h_6hrain','h_6hsnow','h_humidity','h_rainprobability','h_raintype','h_seawave','h_skystatus','h_temperature','h_winddirection','h_windspeed','hk1_1','hk1_2','hk1_3'\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# 원하는 tag name 설정\n",
    "# 여기서 tag name 은 컬럼을 의미\n",
    "tags = name[:13]\n",
    "\n",
    "# 리스트의 각 항목을 작은따옴표로 감싸고, 쉼표로 구분\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# 사용 tag name 확인\n",
    "print(tags_)\n",
    "\n",
    "# 해당 값을 모델의 input shape로 설정 \n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind Elec Gen Dataset 로드\n",
    "\n",
    "* 한경 제 1 발전소 관련 Tag Name 들을 사용하여 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 함수\n",
    "# 시간대를 맞추기 위해 3시간으로 resampling\n",
    "# target 값으로 사용할 통합 발전량 계산후 추가 \n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "\n",
    "    # 같은 시간대 별 데이터로 전환\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "\n",
    "    # 'TIME' 열을 datetime 형식으로 변환\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'])\n",
    "\n",
    "    # time index 설정\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    # 3시간 간격으로 reampling \n",
    "    df = df.resample('3H').mean()\n",
    "\n",
    "    # 결측값 제거\n",
    "    df = df.dropna()\n",
    "\n",
    "    # 발전량 병합\n",
    "    # 한경 발전소의 1, 2, 3 발전량을 합침\n",
    "    df.loc[:, 'hk1_total'] = df.iloc[:, 10:].sum(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시간 로드 함수\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = data_load(table, name, start_time, end_time, timeformat)\n",
    "    \n",
    "    # total 최대, 최소 값 계산\n",
    "    max = df.iloc[:,-1:].max()\n",
    "    min = df.iloc[:,-1:].min()\n",
    "    \n",
    "    # 타임 인덱스만 남김\n",
    "    df = df.iloc[:,:0]\n",
    "    \n",
    "    return df, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 변환 함수\n",
    "# 시간 추가하려면 해당 과정 필요\n",
    "# window_size : 데이터를 묶어서 수집되는 주기 \n",
    "# step_size : 데이터의 간격 \n",
    "def add_time(time_df, start_time, batch_size, window_size, step_size):\n",
    "    \n",
    "    # 몇개의 데이터를 로드해야 되는지 계산\n",
    "    time = (batch_size * step_size)+ window_size - step_size - 1\n",
    "    \n",
    "    # 현재 시간의 인덱스 번호를 확인\n",
    "    # 없는 경우는 맨처음 시간이 없는 경우이기 때문에 맨처음 인덱스로 지정함 \n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # 현재 시간 기준 배치 데이터의 마지막 시간 설정 \n",
    "    end_time_ = str(time_df.index[index_now + time] + timedelta(hours=1))\n",
    "    \n",
    "    # 다음 시작 시간의 인덱스 번호 설정\n",
    "    index_next = index_now + time - abs(window_size - step_size - 1)\n",
    "    \n",
    "    # 다음 시작 시간 설정\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL 인코딩\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택한 tag name 별 최대, 최소값 계산 함수\n",
    "def set_minmax_value(table, name, start_time_train, end_time_train):\n",
    "    \n",
    "    # URL 인코딩\n",
    "    start = quote(start_time_train)\n",
    "    end = quote(end_time_train)\n",
    "    \n",
    "    # min max 데이터 로드\n",
    "    df_ = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-scale.tql?table={table}&name={name}&start={start}&end={end}')\n",
    "    \n",
    "    ## Min , Max values 설정 \n",
    "    Min = df_.iloc[:,1:-1].T\n",
    "    Max = df_.iloc[:,2:].T\n",
    "    \n",
    "    return Min, Max "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "   * MinMaxScaler, window sliding 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window(data, window_size, step_size):\n",
    "    \n",
    "    # 윈도우 슬라이딩 결과를 저장할 리스트\n",
    "    windows = []\n",
    "    targets = []\n",
    "    \n",
    "    # 슬라이딩 윈도우 적용\n",
    "    for i in range(0, data.shape[0] - window_size + 1, step_size):\n",
    "        \n",
    "        window = data.iloc[i:i + window_size, :-1].values  # 마지막 컬럼 제외\n",
    "        target_array = data.iloc[i:i + window_size, -1].values  # 마지막 컬럼이 target\n",
    "            \n",
    "        windows.append(window)\n",
    "        targets.append(target_array)\n",
    "\n",
    "    return windows, targets    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling 설정\n",
    "\n",
    "* Process 컨셉상 전체 데이터를 불러오지 않기 때문에 최대 최소값을 사용하는 방식의 Min-Max Scaler를 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler 클래스 정의\n",
    "class MinMaxScaler_custom:\n",
    "    def __init__(self):\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "\n",
    "    # 설정 파라미터 기반 scale 값 설정 \n",
    "    def transform(self, X, min_values, max_values):\n",
    "        X = np.array(X)\n",
    "        self.min_ = np.array(min_values)\n",
    "        self.max_ = np.array(max_values)\n",
    "        \n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        # scale 값이 0이되는 것을 막기 위해 1e-6 을 더해줌 \n",
    "        scale = (self.max_ - self.min_) + 1e-6\n",
    "        if np.any(scale == 0):\n",
    "            raise ValueError(\"Min and Max values are the same, resulting in a scale of 0.\")\n",
    "        \n",
    "        return (X - self.min_) / scale\n",
    "    \n",
    "    # 계산한 scale 값 기준 데이터 정규화 \n",
    "    def fit_transform(self, X, min_values, max_values):\n",
    "        \"\"\"Set parameters and then transform X\"\"\"\n",
    "        return self.transform(X, min_values, max_values)\n",
    "\n",
    "    # 정규화된 데이터를 원래 값으로 되돌림\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"Inverse the transformation and return original values\"\"\"\n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        X_scaled = np.array(X_scaled)\n",
    "        scale = (self.max_ - self.min_) + 1e-6\n",
    "        \n",
    "        return X_scaled * scale + self.min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 모델 설정 \n",
    "\n",
    "* LSTM AE 기본 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder 클래스 정의\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        # 인코더 LSTM\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.encoder_fc = nn.Linear(hidden_dim, int(hidden_dim/2))\n",
    "        \n",
    "        # 디코더 LSTM\n",
    "        self.decoder_fc = nn.Linear(int(hidden_dim/2), hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # FC Layer 설정\n",
    "        self.fc1 = nn.Linear(input_dim,6)\n",
    "        self.fc2 = nn.Linear(6,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 인코더 부분\n",
    "        _, (h, _) = self.encoder_lstm(x)\n",
    "        latent = self.encoder_fc(h[-1])\n",
    "        \n",
    "        # 디코더 부분\n",
    "        hidden = self.decoder_fc(latent).unsqueeze(0).repeat(x.size(1), 1, 1).permute(1, 0, 2)\n",
    "        output, _ = self.decoder_lstm(hidden)\n",
    "        \n",
    "        # FC Layer\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAutoencoder(\n",
      "  (encoder_lstm): LSTM(13, 6, num_layers=2, batch_first=True)\n",
      "  (encoder_fc): Linear(in_features=6, out_features=3, bias=True)\n",
      "  (decoder_fc): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (decoder_lstm): LSTM(6, 13, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=13, out_features=6, bias=True)\n",
      "  (fc2): Linear(in_features=6, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정 파라미터\n",
    "\n",
    "# 입력 데이터 컬럼 수\n",
    "# PCA 한값 \n",
    "# print(list(train_dataloader)[0][0].shape) 에서 마지막 숫자 \n",
    "input_dim = 13\n",
    "\n",
    "# LSMT hidden state 크기\n",
    "hidden_dim = 6\n",
    "\n",
    "# layer 수\n",
    "num_layers = 2\n",
    "\n",
    "# 학습률 \n",
    "learning_rate = 0.001\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMAutoencoder(input_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 구조 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 설정\n",
    "\n",
    "* 학습 중 validation 데이터 기준 MSE 값이 제일 낮은 모델을 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수 설정\n",
    "def train(table, name, timeformat, model, start_time_train, end_time_train, start_time_valid, end_time_valid, batch_size, window_size, step_size, epochs, Min, Max, min, max, scaler_data, scaler_target, time_df_train, time_df_valid):\n",
    "    # 초기 train loss 설정\n",
    "    train_loss = []\n",
    "\n",
    "    # 베스트 loss 초기화 \n",
    "    best_Loss=np.inf\n",
    "\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        # 학습 모드 설정 \n",
    "        model.train()\n",
    "        \n",
    "        # 초기 loss 초기화\n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        \n",
    "        # 초기 시작 시간 설정\n",
    "        start_time_ = start_time_train\n",
    "        \n",
    "        # 끝 시간 설정\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "\n",
    "        # while 문을 통해 데이터 호출 \n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # 배치 크기에 따라 데이터 로드 \n",
    "            start_time_, end_time_, next_start_time_, index_next= add_time(time_df_train, start_time_, batch_size, window_size, step_size)\n",
    "        \n",
    "            # 데이터 로드 \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat)\n",
    "\n",
    "            # MinMax scaler 적용 \n",
    "            data_scaled = scaler_data.fit_transform(data.iloc[:,:-1], Min, Max)\n",
    "            target_scaled = scaler_target.fit_transform(data.iloc[:,-1:], min, max)\n",
    "            \n",
    "            # 데이터 프레임 + target 설정\n",
    "            data_scaled = pd.DataFrame(data_scaled)\n",
    "            data_scaled['target'] = target_scaled\n",
    "\n",
    "            # window 설정 \n",
    "            windows, targets = make_window(data_scaled, window_size, step_size)\n",
    "            \n",
    "            # 로드한 데이터가 비어 있을 경우 출력 \n",
    "            if len(data) == 0:\n",
    "                print(\"데이터가 없습니다.\")\n",
    "            \n",
    "            # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "            if len(windows) == batch_size:\n",
    "                \n",
    "                # 총 배치수 체크용  \n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # 데이터를 numpy 배열로 변환\n",
    "                input_data = np.array(windows)\n",
    "                input_target = np.array(targets)\n",
    "                \n",
    "                # 데이터를 Tensor로 변환\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "                input_target = torch.tensor(input_target, dtype=torch.float32).to(device).float()\n",
    "                \n",
    "                # 옵티마이저 최적화 \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 모델 입력\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # loss 계산\n",
    "                loss = criterion(outputs.squeeze(), input_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # 배치 리셋\n",
    "                windows = []\n",
    "                \n",
    "            # 다음 시작 시간 설정    \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "            \n",
    "            # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "            if index_next + (batch_size * step_size)+ window_size - step_size - 1 >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "        train_loss.append(running_loss/total_step)\n",
    "        print(f'\\ntrain loss: {np.mean(train_loss)}')\n",
    "\n",
    "        # Epoch 마다 validation을 진행해서 가장 좋은 성능을 보이는 모델을 저장 \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            # 초기 Loss 설정 \n",
    "            valid_loss = []\n",
    "            \n",
    "            # 초기 loss 초기화\n",
    "            running_loss_v = 0.0\n",
    "            total_step_v = 0\n",
    "                \n",
    "            # 초기 시작 시간 설정\n",
    "            start_time_v = start_time_valid\n",
    "            \n",
    "            # 끝 시간 설정\n",
    "            end_time_valid = str(time_df_valid.index[-1])\n",
    "            \n",
    "            # while 문을 통해 데이터 호출 \n",
    "            while start_time_v < end_time_valid:\n",
    "                \n",
    "                # 배치 크기에 따라 데이터 로드 \n",
    "                start_time_v, end_time_v, next_start_time_v, index_next_v = add_time(time_df_valid, start_time_v, batch_size, window_size, step_size)\n",
    "                \n",
    "                # 데이터 로드 \n",
    "                data_v = data_load(table, name, start_time_v, end_time_v, timeformat)\n",
    "\n",
    "                # MinMax scaler 적용 \n",
    "                data_scaled_v = scaler_data.fit_transform(data_v.iloc[:,:-1], Min, Max)\n",
    "                target_scaled_v = scaler_target.fit_transform(data_v.iloc[:,-1:], min, max)\n",
    "                \n",
    "                # 데이터 프레임 + target 설정\n",
    "                data_scaled_v = pd.DataFrame(data_scaled_v)\n",
    "                data_scaled_v['target'] = target_scaled_v\n",
    "\n",
    "                # window 설정 \n",
    "                windows_v, targets_v = make_window(data_scaled_v , window_size, step_size)\n",
    "                \n",
    "                # 로드한 데이터가 비어 있을 경우 출력 \n",
    "                if len(data_v) == 0:\n",
    "                    print(\"데이터가 없습니다.\")\n",
    "                \n",
    "                # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "                if len(windows_v) == batch_size:\n",
    "                    \n",
    "                    # 총 배치수 체크용  \n",
    "                    total_step_v = total_step_v + 1\n",
    "                    \n",
    "                    # 데이터를 numpy 배열로 변환\n",
    "                    input_data_v = np.array(windows_v)\n",
    "                    input_target_v = np.array(targets_v)\n",
    "\n",
    "                    # 데이터를 Tensor로 변환\n",
    "                    input_data_v = torch.tensor(input_data_v, dtype=torch.float32).to(device).float()\n",
    "                    input_target_v = torch.tensor(input_target_v, dtype=torch.float32).to(device).float()\n",
    "                    \n",
    "                    # 모델 입력\n",
    "                    outputs_v = model(input_data_v)\n",
    "                    \n",
    "                    # loss 계산\n",
    "                    loss_v = criterion(outputs_v.squeeze(), input_target_v)\n",
    "                    running_loss_v += loss_v.item()\n",
    "                    \n",
    "                    # 배치 리셋\n",
    "                    windows_v = []\n",
    "                \n",
    "                valid_loss.append(running_loss_v/total_step_v)\n",
    "                    \n",
    "                # 다음 시작 시간 설정    \n",
    "                start_time_v = unquote(next_start_time_v)\n",
    "                \n",
    "                # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "                if index_next_v + (batch_size * step_size)+ window_size - step_size - 1 >= len(time_df_valid):\n",
    "                    break\n",
    "                \n",
    "            if best_Loss > np.mean(valid_loss):\n",
    "                best_Loss = np.mean(valid_loss)\n",
    "                torch.save(model, f'./result/Wind_Elec_Gen_New_Batch.pt')\n",
    "                print('모델 저장')\n",
    "            epochs.set_postfix_str(f\"epoch = {epoch}, best_Loss = {best_Loss}\")  \n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12c0a08da8d46af945a20ad215896ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.11514101648686077\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.07896050558449769\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.06301201689964091\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.05452204142976409\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.04923053199026813\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.045606008223237716\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.04290481614524182\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.0397763626229354\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.03685090704934926\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.03438261714820183\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.032317210486170346\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.030570395706466486\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.029075276641404628\n",
      "\n",
      "train loss: 0.027780504439183087\n",
      "\n",
      "train loss: 0.026647957872711493\n",
      "\n",
      "train loss: 0.025649029059562397\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.0247614738432819\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.023967733518541482\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.023253237776531793\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.022606169916609943\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.022017034603089944\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.021478027314224046\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.020982747543652404\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.020525906877986805\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.020103048387544578\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.01971036598660557\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.0193445932956557\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.019002922228368514\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.018682929941540333\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.018382514440376873\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.018099865372848717\n",
      "\n",
      "train loss: 0.017833531037213888\n",
      "\n",
      "train loss: 0.017582588793111264\n",
      "\n",
      "train loss: 0.017346105543262103\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.017121551379865065\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.01690778129066241\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.016703729094994114\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.01650805981986486\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.016319197274713027\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.01613473006088827\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.015950716878439873\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.015763259016105938\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.015571932229446386\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.015378276857936331\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.015184146160463473\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.014990423185194333\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.01479797059561059\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.014608128013396652\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.014422668949135506\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.014242791688000335\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.014068748241974047\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.013900435279670413\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.013737699146988627\n",
      "\n",
      "train loss: 0.013580363887617986\n",
      "\n",
      "train loss: 0.013428237465357743\n",
      "\n",
      "train loss: 0.013281106084732935\n",
      "\n",
      "train loss: 0.01313872847709021\n",
      "\n",
      "train loss: 0.01300086249074861\n",
      "\n",
      "train loss: 0.01286728887090214\n",
      "\n",
      "train loss: 0.012737809124244411\n",
      "\n",
      "train loss: 0.012612237611348543\n",
      "\n",
      "train loss: 0.012490397533322732\n",
      "\n",
      "train loss: 0.012372122043471137\n",
      "\n",
      "train loss: 0.012257256880298079\n",
      "\n",
      "train loss: 0.012145660470596873\n",
      "\n",
      "train loss: 0.012037200423816369\n",
      "\n",
      "train loss: 0.0119317499671712\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.011829186651349985\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.011729392935859277\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.011632256917853277\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.011537672863794646\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.01144554140889242\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.011355769238857111\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.011268268437446587\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.01118295628732126\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.011099754543794443\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.011018590007818137\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010939396231792431\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010862118770036963\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010786727249681608\n",
      "\n",
      "train loss: 0.010713242503351077\n",
      "\n",
      "train loss: 0.010641722018200744\n",
      "\n",
      "train loss: 0.010571911868258904\n",
      "\n",
      "train loss: 0.010503423661685513\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010436255911184163\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010370549625920877\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010306216951803174\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010243255003999864\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010181579919939352\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010121184038778874\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010061992012384963\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.010003999977759201\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.009947134421643508\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.009891397763836112\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.00983671579473057\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.009783097355259016\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.00973046814417083\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.009678840647760962\n",
      "\n",
      "train loss: 0.009628144821976187\n",
      "모델 저장\n",
      "\n",
      "train loss: 0.009578390473253125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMAutoencoder(\n",
       "  (encoder_lstm): LSTM(13, 6, num_layers=2, batch_first=True)\n",
       "  (encoder_fc): Linear(in_features=6, out_features=3, bias=True)\n",
       "  (decoder_fc): Linear(in_features=3, out_features=6, bias=True)\n",
       "  (decoder_lstm): LSTM(6, 13, num_layers=2, batch_first=True)\n",
       "  (fc1): Linear(in_features=13, out_features=6, bias=True)\n",
       "  (fc2): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### 공통 파라미터 설정 ################################################\n",
    "# tag table 이름 설정\n",
    "table = 'wind_elec_gen'\n",
    "# tag name 설정\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# 시간 포멧 설정 \n",
    "timeformat = 'default'\n",
    "# 배치 사이즈 설정\n",
    "batch_size = 16\n",
    "# window size 설정\n",
    "window_size = 8\n",
    "# step size 설정 -> window size 보다 크거나 같으면 안됨 \n",
    "step_size = 1\n",
    "# epoch 설정\n",
    "epochs = trange(100, desc='training')\n",
    "# Min-Max scaler 설정 \n",
    "scaler_data = MinMaxScaler_custom()\n",
    "scaler_target = MinMaxScaler_custom()\n",
    "\n",
    "########################################### 학습 파라미터 설정 ################################################\n",
    "# 학습 시작 시간 설정\n",
    "start_time_train = '2014-01-01 06:00:00'\n",
    "# 학습 끝 시간 설정\n",
    "end_time_train = '2016-01-01 00:00:00'\n",
    "# 최대, 최소값 설정 \n",
    "Min, Max = set_minmax_value(table, name, start_time_train, end_time_train)\n",
    "# 학습 시간 리스트 로드 \n",
    "time_df_train, min, max = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "########################################### 검증 파라미터 설정 ################################################\n",
    "# 검증 시작 시간 설정\n",
    "start_time_valid = '2016-01-01 00:00:00'\n",
    "# 검증 끝 시간 설정\n",
    "end_time_valid = '2016-04-03 03:00:00'\n",
    "# valid 시간 리스트 로드 \n",
    "time_df_valid,_ ,_= time_data_load(table, name, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "\n",
    "# 학습 및 검증 진행\n",
    "train(table, name, timeformat, model, start_time_train, end_time_train, start_time_valid, end_time_valid, batch_size, window_size, step_size, epochs, Min, Max, min, max, scaler_data, scaler_target, time_df_train, time_df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "model_ = torch.load(f'./result/Wind_Elec_Gen_New_Batch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(table, name, timeformat, model, start_time_test, end_time_test, batch_size, window_size, step_size, Min, Max, min, max, scaler_data, scaler_target, time_df_test):\n",
    "    with torch.no_grad():\n",
    "                \n",
    "        model.eval()\n",
    "        \n",
    "        # 초기 설정 \n",
    "        output_test = []\n",
    "        output_target = []\n",
    "            \n",
    "        # 초기 시작 시간 설정\n",
    "        start_time_t = start_time_test\n",
    "        \n",
    "        # 끝 시간 설정\n",
    "        end_time_test = str(time_df_test.index[-1])\n",
    "        \n",
    "        # while 문을 통해 데이터 호출 \n",
    "        while start_time_t < end_time_test:\n",
    "            \n",
    "            # 배치 크기에 따라 데이터 로드 \n",
    "            start_time_t, end_time_t, next_start_time_t, index_next_t = add_time(time_df_test, start_time_t, batch_size, window_size, step_size)\n",
    "            \n",
    "            # 데이터 로드 \n",
    "            data_t = data_load(table, name, start_time_t, end_time_t, timeformat)\n",
    "\n",
    "            # MinMax scaler 적용 \n",
    "            data_scaled_t = scaler_data.fit_transform(data_t.iloc[:,:-1], Min, Max)\n",
    "            target_scaled_t = scaler_target.fit_transform(data_t.iloc[:,-1:], min, max)\n",
    "            \n",
    "            # 데이터 프레임 + target 설정\n",
    "            data_scaled_t = pd.DataFrame(data_scaled_t)\n",
    "            data_scaled_t['target'] = target_scaled_t\n",
    "\n",
    "            # window 설정 \n",
    "            windows_t, targets_t = make_window(data_scaled_t , window_size, step_size)\n",
    "            \n",
    "            # 로드한 데이터가 비어 있을 경우 출력 \n",
    "            if len(data_t) == 0:\n",
    "                print(\"데이터가 없습니다.\")\n",
    "            \n",
    "            # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "            if len(windows_t) == batch_size:\n",
    "                \n",
    "                # 데이터를 numpy 배열로 변환\n",
    "                input_data_t = np.array(windows_t)\n",
    "                input_target_t = np.array(targets_t)\n",
    "\n",
    "                # 데이터를 Tensor로 변환\n",
    "                input_data_t = torch.tensor(input_data_t, dtype=torch.float32).to(device).float()\n",
    "                input_target_t = torch.tensor(input_target_t, dtype=torch.float32).to(device).float()\n",
    "                \n",
    "                # 모델 입력\n",
    "                outputs_t = model(input_data_t)\n",
    "                \n",
    "                output_test.append(outputs_t)\n",
    "                output_target.append(input_target_t)\n",
    "                \n",
    "                # 배치 리셋\n",
    "                windows_t = []\n",
    "\n",
    "            # 다음 시작 시간 설정    \n",
    "            start_time_t = unquote(next_start_time_t)\n",
    "            \n",
    "            # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "            if index_next_t + (batch_size * step_size)+ window_size - step_size - 1 > len(time_df_test):\n",
    "                break\n",
    "            \n",
    "        # 텐서들을 하나로 합치기\n",
    "        combined_tensor = torch.cat(output_test, dim=0)\n",
    "        combined_target = torch.cat(output_target, dim=0)\n",
    "\n",
    "        # window의 마지막 값만 사용하기\n",
    "        last_values = combined_tensor[:, -1, :].cpu()\n",
    "        last_target = combined_target.unsqueeze(2)[:, -1, :].cpu()\n",
    "\n",
    "        # 데이터 프레임 생성\n",
    "        final_scaled = pd.DataFrame(last_target, columns=['real'])\n",
    "        final_scaled['pred'] = last_values\n",
    "\n",
    "        # MinMaxScaler 역변환 \n",
    "        last_values_ = scaler_target.inverse_transform(last_values)\n",
    "        last_target_ = scaler_target.inverse_transform(last_target)\n",
    "\n",
    "        # 데이터 프레임 생성\n",
    "        final= pd.DataFrame(last_target_, columns=['real'])\n",
    "        final['pred'] = last_values_\n",
    "        \n",
    "        return final_scaled, final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### 테스트 파라미터 설정 ################################################\n",
    "# 검증 시작 시간 설정\n",
    "start_time_test = '2016-01-01 06:00:00'\n",
    "# 검증 끝 시간 설정\n",
    "end_time_test = '2017-12-10 06:00:00'\n",
    "# test 시간 리스트 로드 \n",
    "time_df_test,_ ,_ = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "final_scaled, final = test(table, name, timeformat, model_, start_time_test, end_time_test, batch_size, window_size, step_size, Min, Max, min, max, scaler_data, scaler_target, time_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0038355803\n",
      "R² Score: 0.9595717178958093\n"
     ]
    }
   ],
   "source": [
    "# MSE 계산\n",
    "mse = mean_squared_error(final_scaled['real'].values, final_scaled['pred'].values)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# R² 스코어 계산\n",
    "r2 = r2_score(final['real'].values, final['pred'].values)\n",
    "print(\"R² Score:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
