{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "import statistics\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'bearing'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list\n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s1-c1',\n",
       " 's1-c2',\n",
       " 's1-c3',\n",
       " 's1-c4',\n",
       " 's1-c5',\n",
       " 's1-c6',\n",
       " 's1-c7',\n",
       " 's1-c8',\n",
       " 's2-c1',\n",
       " 's2-c2',\n",
       " 's2-c3',\n",
       " 's2-c4',\n",
       " 's3-c1',\n",
       " 's3-c2',\n",
       " 's3-c3',\n",
       " 's3-c4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the Nasa bearing dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names related to the s1-c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s1-c5'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = ['s1-c5']\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Nasa Bearing Dataset\n",
    "* Load the train, validation, and test datasets separately when loading the data.\n",
    "* The example focuses on anomaly detection for the 3rd bearing -> using 's1-c5' as the Tag Name.\n",
    "* Label all states except for the faulty condition as normal for the labeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the status of each bearing based on time ranges\n",
    "B1 ={\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-23 09:14:13\"],\n",
    "    \"suspect\" : [\"2003-10-23 09:24:13\" , \"2003-11-08 12:11:44\"],\n",
    "    \"normal\" : [\"2003-11-08 12:21:44\" , \"2003-11-19 21:06:07\"],\n",
    "    \"suspect_1\" : [\"2003-11-19 21:16:07\" , \"2003-11-24 20:47:32\"],\n",
    "    \"imminent_failure\" : [\"2003-11-24 20:57:32\",\"2003-11-25 23:39:56\"]\n",
    "}\n",
    "B2 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n",
    "    \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-24 01:01:24\"],\n",
    "    \"suspect\" : [\"2003-11-24 01:11:24\" , \"2003-11-25 10:47:32\"],\n",
    "    \"imminient_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n",
    "}\n",
    "\n",
    "B3 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n",
    "    \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-22 09:16:56\"],\n",
    "    \"suspect\" : [\"2003-11-22 09:26:56\" , \"2003-11-25 10:47:32\"],\n",
    "    \"Inner_race_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n",
    "}\n",
    "\n",
    "B4 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-29 21:39:46\"],\n",
    "    \"normal\" : [\"2003-10-29 21:49:46\" , \"2003-11-15 05:08:46\"],\n",
    "    \"suspect\" : [\"2003-11-15 05:18:46\" , \"2003-11-18 19:12:30\"],\n",
    "    \"Rolling_element_failure\" : [\"2003-11-19 09:06:09\" , \"2003-11-22 17:36:56\"],\n",
    "    \"Stage_two_failure\" : [\"2003-11-22 17:46:56\" , \"2003-11-25 23:39:56\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading parameter settings\n",
    "\n",
    "# Set the tag table name\n",
    "table = 'bearing'\n",
    "# Set the tag names\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set the time format \n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Set the data start time\n",
    "start_time = quote('2003-10-22 12:06:24')\n",
    "# Set the data end time\n",
    "end_time = quote('2003-11-25 23:39:56')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # Set TIME column\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Group by seconds and count the number of records\n",
    "    df_counts = df.groupby(df['TIME'].dt.floor('S')).size().reset_index(name='count')\n",
    "\n",
    "    # Filter groups with the same number of records\n",
    "    # Select the most common count values\n",
    "    most_common_count = df_counts['count'].mode()[0]\n",
    "\n",
    "    # Filter by the most common count value\n",
    "    filtered_df_counts = df_counts[df_counts['count'] == most_common_count]\n",
    "\n",
    "    # Convert filtered time values to a list\n",
    "    filtered_times = filtered_df_counts['TIME'].tolist()\n",
    "\n",
    "    # Select only the filtered time values from the original DataFrame\n",
    "    filtered_data = df[df['TIME'].dt.floor('S').isin(filtered_times)]\n",
    "\n",
    "    # Group by TIME\n",
    "    # Round to the nearest second\n",
    "    filtered_data_ = filtered_data.copy()\n",
    "    filtered_data_.loc[:, 'TIME'] = filtered_data_['TIME'].dt.floor('S')\n",
    "    grouped = filtered_data_.groupby('TIME')['s1-c5'].apply(list).reset_index()\n",
    "\n",
    "    # Split the list into individual columns\n",
    "    s1_c5_df = pd.DataFrame(grouped['s1-c5'].tolist())\n",
    "\n",
    "    # Merge with the 'TIME' column\n",
    "    result_df = pd.concat([grouped[['TIME']], s1_c5_df], axis=1)\n",
    "    \n",
    "    # Set labels\n",
    "    # Assign labels based on abnormal time ranges for each channel data\n",
    "    result_df['label'] = np.where((result_df['TIME'] >= \"2003-11-25 10:57:32\") & (result_df['TIME'] <= \"2003-11-25 23:39:56\"), 1, 0)\n",
    "    \n",
    "    result_df = result_df.drop(['TIME'], axis=1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20471</th>\n",
       "      <th>20472</th>\n",
       "      <th>20473</th>\n",
       "      <th>20474</th>\n",
       "      <th>20475</th>\n",
       "      <th>20476</th>\n",
       "      <th>20477</th>\n",
       "      <th>20478</th>\n",
       "      <th>20479</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>0.432</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.505</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>0.107</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2155 rows × 20481 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4      5      6      7      8      9  \\\n",
       "0    -0.105 -0.049 -0.005 -0.100 -0.151  0.046 -0.132 -0.164 -0.110 -0.100   \n",
       "1    -0.083 -0.132 -0.081 -0.022 -0.129 -0.110 -0.149 -0.168 -0.225 -0.217   \n",
       "2    -0.122 -0.244 -0.156 -0.076  0.046 -0.098 -0.142  0.083 -0.195 -0.088   \n",
       "3    -0.210 -0.125 -0.090 -0.215 -0.225 -0.139 -0.042 -0.090 -0.142 -0.232   \n",
       "4    -0.088 -0.088 -0.110 -0.269  0.024 -0.054 -0.156 -0.205 -0.056 -0.132   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2150 -0.212 -0.095  0.044  0.005 -0.364 -0.239 -0.098 -0.066 -0.081 -0.164   \n",
       "2151  0.432  0.217 -0.627 -0.308  0.073  0.107 -0.359  0.056 -0.159 -0.181   \n",
       "2152 -0.112 -0.510  0.037  0.063 -0.115 -0.066  0.117  0.405 -0.374 -0.339   \n",
       "2153 -0.593  0.366 -0.393 -0.312 -0.156  0.288  0.122 -0.105  0.242 -0.166   \n",
       "2154  0.107  0.193 -0.176 -0.273 -0.015  0.630 -0.535 -0.596 -0.413  0.225   \n",
       "\n",
       "      ...  20471  20472  20473  20474  20475  20476  20477  20478  20479  \\\n",
       "0     ... -0.088 -0.107 -0.078 -0.093 -0.200 -0.159 -0.237 -0.027 -0.002   \n",
       "1     ... -0.129 -0.149 -0.046  0.007 -0.132 -0.073  0.056 -0.186 -0.049   \n",
       "2     ... -0.107 -0.061 -0.090 -0.049 -0.125 -0.056 -0.137 -0.247 -0.229   \n",
       "3     ... -0.046 -0.195 -0.085 -0.112 -0.049 -0.146 -0.154 -0.220 -0.090   \n",
       "4     ...  0.044 -0.122 -0.115 -0.056 -0.078 -0.002 -0.342 -0.173 -0.161   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2150  ... -0.574  0.127  0.767  0.173  0.217 -0.271  0.085  0.364 -0.166   \n",
       "2151  ... -0.161 -0.046 -0.098 -0.186 -0.251 -0.032 -0.017 -0.073 -0.437   \n",
       "2152  ... -0.188 -0.002 -0.085 -0.149 -0.195 -0.227 -0.095 -0.115 -0.293   \n",
       "2153  ...  0.510 -0.776 -0.483  0.288  0.359  0.120 -0.173 -0.400  0.505   \n",
       "2154  ... -0.244 -0.767 -0.994 -0.405  0.525 -0.010 -0.166 -0.415  0.046   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "2150      1  \n",
       "2151      1  \n",
       "2152      1  \n",
       "2153      1  \n",
       "2154      1  \n",
       "\n",
       "[2155 rows x 20481 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = data_load(table, name, start_time, end_time, timeformat)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train, test = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "valid, test = train_test_split(test, test_size=0.5, shuffle=False)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 hanning window\n",
    "* 2 FFT\n",
    "* 3 MinMax Scaling\n",
    "* 4 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying Hanning Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanning window function setup \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Generate Hanning window\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # Apply Hanning window to each row\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling period -> Number of data points per second\n",
    "window_length = len(df.columns[:-1])\n",
    "\n",
    "# Applying Hanning Window\n",
    "train_ = set_hanning_window(window_length, train.iloc[:,:-1])\n",
    "valid_ = set_hanning_window(window_length, valid.iloc[:,:-1])\n",
    "test_ = set_hanning_window(window_length, test.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying FFT (Fast Fourier Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT transformation function\n",
    "def change_fft(sample_rate, df):\n",
    "    # Total number of samples in the signal\n",
    "    N = sample_rate\n",
    "    \n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # Apply FFT to each row\n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        # Calculate FFT for each row\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # Compute the absolute value of the FFT results and normalize (only the meaningful part)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # Convert FFT results to a DataFrame\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling period -> Number of data points per second\n",
    "sampling_rate = len(df.columns[:-1])\n",
    "\n",
    "# Applying FFT (Fast Fourier Transform)\n",
    "train_ = change_fft(sampling_rate, train_)\n",
    "valid_ = change_fft(sampling_rate, valid_)\n",
    "test_ = change_fft(sampling_rate, test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Applying MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler Setup\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Scaler\n",
    "train_ = scaler.fit_transform(train_.values)\n",
    "valid_ = scaler.transform(valid_.values)\n",
    "test_ = scaler.transform(test_.values)\n",
    "\n",
    "# Set DataFrames\n",
    "train_scaled = pd.DataFrame(train_)\n",
    "valid_scaled = pd.DataFrame(valid_)\n",
    "test_scaled = pd.DataFrame(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Applying PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1724\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    215\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    181\n",
      "1     35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Applying PCA\n",
    "# Select principal components explaining 95% of the variance\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Apply PCA\n",
    "train_scaled_ = pca.fit_transform(train_scaled)\n",
    "valid_scaled_ = pca.transform(valid_scaled)\n",
    "test_scaled_ = pca.transform(test_scaled)\n",
    "\n",
    "# Set DataFrames\n",
    "train_scaled_ = pd.DataFrame(train_scaled_)\n",
    "valid_scaled_ = pd.DataFrame(valid_scaled_)\n",
    "test_scaled_ = pd.DataFrame(test_scaled_)\n",
    "\n",
    "# Add labels\n",
    "train_scaled_['label'] = train['label'].values\n",
    "valid_scaled_['label'] = valid['label'].values\n",
    "test_scaled_['label'] = test['label'].values\n",
    "\n",
    "print(train_scaled_['label'].value_counts())\n",
    "print(valid_scaled_['label'].value_counts())\n",
    "print(test_scaled_['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Loader Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Dataset Configuration\n",
    "* To train on time series data, you need to set the window size and the sliding step.\n",
    "\n",
    "* Window size: Determines how many time points to group together.\n",
    "* Step size: The time interval by which the window moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding Window Dataset setup\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, data, window_size, step_size):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.windows, self.labels = self._create_windows()\n",
    "    \n",
    "    # Set up sliding windows\n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        labels = []\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, self.step_size):\n",
    "            window = self.data.iloc[i:i + self.window_size, :-1].values  # Exclude the last column\n",
    "            label_array = self.data.iloc[i:i + self.window_size, -1].values  # The last column is the label\n",
    "            \n",
    "            # Set the label to 1 if there is any abnormal value in the label array\n",
    "            if (label_array == 1).any():\n",
    "                label = 1  \n",
    "            else:\n",
    "                label = 0\n",
    "                \n",
    "            windows.append(torch.Tensor(window))\n",
    "            labels.append(torch.Tensor([label]))  # Convert label to Tensor\n",
    "        return windows, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.windows[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding window configuration\n",
    "window_size = 3\n",
    "step_size = 1 \n",
    "\n",
    "# Set up datasets \n",
    "train_ = SlidingWindowDataset(train_scaled_, window_size, step_size)\n",
    "valid_ = SlidingWindowDataset(valid_scaled_, window_size, step_size)\n",
    "test_ = SlidingWindowDataset(test_scaled_, window_size, step_size)\n",
    "\n",
    "# Set up data loaders\n",
    "train_dataloader = DataLoader(train_, batch_size=32, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_, batch_size=1, shuffle=False)\n",
    "test_dataloader = DataLoader(test_, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 1487])\n"
     ]
    }
   ],
   "source": [
    "# Verify DataLoader application and check the shape of the input data\n",
    "print(list(train_dataloader)[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using LSTM AE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder class definition\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.encoder_fc = nn.Linear(hidden_dim, 2*hidden_dim)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.decoder_fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder part\n",
    "        _, (h, _) = self.encoder_lstm(x)\n",
    "        latent = self.encoder_fc(h[-1])\n",
    "        \n",
    "        # Decoder part\n",
    "        hidden = self.decoder_fc(latent).unsqueeze(0).repeat(x.size(1), 1, 1).permute(1, 0, 2)\n",
    "        output, _ = self.decoder_lstm(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAutoencoder(\n",
      "  (encoder_lstm): LSTM(1487, 256, num_layers=3, batch_first=True)\n",
      "  (encoder_fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (decoder_fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (decoder_lstm): LSTM(256, 1487, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "\n",
    "# number of input data columns\n",
    "# last number in print(list(train_dataloader)[0][0].shape)\n",
    "input_dim = 1487\n",
    "\n",
    "# LSMT hidden state size\n",
    "hidden_dim = 256\n",
    "\n",
    "# layer size\n",
    "num_layers = 3\n",
    "\n",
    "# Learning rate \n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = LSTMAutoencoder(input_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "* Save the model with the Best Loss based on the training data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c74cb498a664ffe9182f848e16137a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.1874480980137984\n",
      "Model saved\n",
      "\n",
      "train loss: 0.18224455574872317\n",
      "Model saved\n",
      "\n",
      "train loss: 0.18051004501772516\n",
      "Model saved\n",
      "\n",
      "train loss: 0.17964278979019987\n",
      "Model saved\n",
      "\n",
      "train loss: 0.17912243715039006\n",
      "Model saved\n",
      "\n",
      "train loss: 0.17877553511456945\n",
      "Model saved\n",
      "\n",
      "train loss: 0.17852774798554719\n",
      "Model saved\n",
      "\n",
      "train loss: 0.17834190784574105\n",
      "Model saved\n",
      "\n",
      "train loss: 0.17819736530015498\n",
      "Model saved\n",
      "\n",
      "train loss: 0.1780817310705229\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Initialize loss\n",
    "train_loss = []\n",
    "# Initialize total step\n",
    "total_step = len(train_dataloader)\n",
    "# Set number of epochs\n",
    "epoch_in = trange(10, desc='training')\n",
    "# Initialize best Loss value\n",
    "best_Loss= np.inf\n",
    "\n",
    "# Start model training\n",
    "for epoch in epoch_in:\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = train_data[0].to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Input to the model\n",
    "        outputs = model(inputs)\n",
    " \n",
    "        # Calculate loss \n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss)}')\n",
    "\n",
    "    # Save the best model\n",
    "    if best_Loss > np.mean(train_loss):\n",
    "        best_Loss = np.mean(train_loss)\n",
    "        torch.save(model, f'./result/Nasa_Bearing_LSTM_AE_General.pt')\n",
    "        print('Model saved')\n",
    "    epoch_in.set_postfix_str(f\"epoch = {epoch}, best_Loss = {best_Loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Setting\n",
    "* Calculate the threshold using validation data\n",
    "  * Max + K × Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_ = torch.load(f'./result/Nasa_Bearing_LSTM_AE_General.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18937314544431605\n"
     ]
    }
   ],
   "source": [
    "# Calculate validation data reconstruction loss\n",
    "valid_loss = []\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for batch_idx, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "        inputs_val = valid_data[0].to(device).float()\n",
    "\n",
    "        outputs_val = model_(inputs_val)\n",
    "        loss = criterion(outputs_val, inputs_val)\n",
    "        \n",
    "        valid_loss.append(loss.item())\n",
    "        \n",
    "# Threshold setting\n",
    "# The threshold should be adjusted according to your own criteria\n",
    "# This threshold is composed of Max + K × Standard Deviation, and the K value is adjusted based on validation\n",
    "threshold =  max(valid_loss) + 10*statistics.stdev(valid_loss)\n",
    "\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "* Proceed with model testing on the test data based on the threshold calculated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the test data\n",
    "test_loss = []\n",
    "test_label = []\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    for batch_idx, test_data in enumerate(test_dataloader):\n",
    "\n",
    "        inputs_test = test_data[0].to(device).float()\n",
    "        label = test_data[1].to(device).long()\n",
    "\n",
    "        outputs_test = model_(inputs_test)\n",
    "        loss = criterion(outputs_test, inputs_test)\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "        test_label.append(label.item())\n",
    "\n",
    "# Create a DataFrame for the test results\n",
    "result = pd.DataFrame(test_loss, columns=['Reconst_Loss'])\n",
    "\n",
    "# Set the actual labels\n",
    "result['label'] = test_label\n",
    "\n",
    "# Classify normal and abnormal based on each threshold\n",
    "result['pred'] = np.where(result['Reconst_Loss'] > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       179\n",
      "           1       1.00      1.00      1.00        35\n",
      "\n",
      "    accuracy                           1.00       214\n",
      "   macro avg       1.00      1.00      1.00       214\n",
      "weighted avg       1.00      1.00      1.00       214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print F1 Score based on testing data\n",
    "print(classification_report(result['label'], result['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Check\n",
    "\n",
    "* Calculate the F1 score using not only the test data but also the train and validation datasets.\n",
    "* If the results for the train and validation are not satisfactory, it can be determined that the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model to train data\n",
    "train_loss = []\n",
    "train_label = []\n",
    "train_dataloader = DataLoader(train_, batch_size=1, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "\n",
    "        inputs_train = train_data[0].to(device).float()\n",
    "        label = train_data[1].to(device).long()\n",
    "\n",
    "        outputs_train = model_(inputs_train)\n",
    "        loss = criterion(outputs_train, inputs_train)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        train_label.append(label.item())\n",
    "\n",
    "# Create a DataFrame for the train results\n",
    "result = pd.DataFrame(train_loss, columns=['Reconst_Loss'])\n",
    "\n",
    "# Set actual labels\n",
    "result['label'] = train_label\n",
    "\n",
    "# Classify normal and abnormal based on the threshold\n",
    "result['pred'] = np.where(result['Reconst_Loss'] > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.90      1722\n",
      "\n",
      "   micro avg       1.00      0.83      0.90      1722\n",
      "   macro avg       1.00      0.83      0.90      1722\n",
      "weighted avg       1.00      0.83      0.90      1722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print F1 Score based on training data\n",
    "print(classification_report(result['label'], result['pred'],labels=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model on validation data\n",
    "val_loss = []\n",
    "val_label = []\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    for batch_idx, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "        inputs_val = valid_data[0].to(device).float()\n",
    "        label = valid_data[1].to(device).long()\n",
    "\n",
    "        outputs_val = model_(inputs_val)\n",
    "        loss = criterion(outputs_val, inputs_val)\n",
    "        \n",
    "        val_loss.append(loss.item())\n",
    "        val_label.append(label.item())\n",
    "\n",
    "# Create a DataFrame for the validation results\n",
    "result = pd.DataFrame(val_loss, columns=['Reconst_Loss'])\n",
    "\n",
    "# Set the actual label\n",
    "result['label'] = val_label\n",
    "\n",
    "# Classify normal and abnormal based on the threshold\n",
    "result['pred'] = np.where(result['Reconst_Loss'] > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       213\n",
      "\n",
      "    accuracy                           1.00       213\n",
      "   macro avg       1.00      1.00      1.00       213\n",
      "weighted avg       1.00      1.00      1.00       213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print F1 Score based on validation data\n",
    "print(classification_report(result['label'], result['pred'],labels=[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
