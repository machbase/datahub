{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## 사용 라이브러리 호출\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## 모델 사용 라이브러리 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "## 모델 학습 사용 라이브러리\n",
    "from tqdm.notebook import trange\n",
    "import statistics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## 모델 학습 결과 경로 설정 \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Cuda 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## 랜덤 시드 설정\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 시드 설정 \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 데이터 컬럼 선택\n",
    "\n",
    "* tag name 이 많은 경우 tag name을 지정하는 것에 있어서 변수 설정이 다소 유연해짐\n",
    "* tag name 은 순서대로 불러와짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag name 출력 함수 \n",
    "def show_column(URL):\n",
    "    \n",
    "    # Tag name 데이터 로드\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # List 형식으로 변환\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tag name 출력 파라미터 설정\n",
    "table = 'bearing'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## tag name list 생성 \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s1-c1',\n",
       " 's1-c2',\n",
       " 's1-c3',\n",
       " 's1-c4',\n",
       " 's1-c5',\n",
       " 's1-c6',\n",
       " 's1-c7',\n",
       " 's1-c8',\n",
       " 's2-c1',\n",
       " 's2-c2',\n",
       " 's2-c3',\n",
       " 's2-c4',\n",
       " 's3-c1',\n",
       " 's3-c2',\n",
       " 's3-c3',\n",
       " 's3-c4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAG Name format 변환 \n",
    "\n",
    "* 위의 과정에서 Smart home dataset의 모든 Tag Name 을 확인후 사용할 컬럼만 뽑아서 입력할 파라미터 형태로 변환\n",
    "\n",
    "* s1-c5 Tag Name 사용하여 예제 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s1-c5'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 원하는 tag name 설정\n",
    "# 여기서 tag name 은 컬럼을 의미\n",
    "tags = ['s1-c5']\n",
    "\n",
    "# 리스트의 각 항목을 작은따옴표로 감싸고, 쉼표로 구분\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# 사용 tag name 확인\n",
    "print(tags_)\n",
    "\n",
    "# 해당 값을 모델의 input shape로 설정 \n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nasa Bearing Dataset 로드\n",
    "\n",
    "* 데이터 로드시 train, validation, test 데이터 셋을 각각 Load\n",
    "\n",
    "* 예제는 3번쨰 bearing의 이상탐지로 진행 -> 's1-c5' Tag Name 사용\n",
    "\n",
    "* 나머지 상태 빼고 고장이 아니면 전부 정상으로 labeling 해서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 bearing의 상태를 시간 범위로 설정\n",
    "B1 ={\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-23 09:14:13\"],\n",
    "    \"suspect\" : [\"2003-10-23 09:24:13\" , \"2003-11-08 12:11:44\"],\n",
    "    \"normal\" : [\"2003-11-08 12:21:44\" , \"2003-11-19 21:06:07\"],\n",
    "    \"suspect_1\" : [\"2003-11-19 21:16:07\" , \"2003-11-24 20:47:32\"],\n",
    "    \"imminent_failure\" : [\"2003-11-24 20:57:32\",\"2003-11-25 23:39:56\"]\n",
    "}\n",
    "B2 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n",
    "    \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-24 01:01:24\"],\n",
    "    \"suspect\" : [\"2003-11-24 01:11:24\" , \"2003-11-25 10:47:32\"],\n",
    "    \"imminient_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n",
    "}\n",
    "\n",
    "B3 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n",
    "    \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-22 09:16:56\"],\n",
    "    \"suspect\" : [\"2003-11-22 09:26:56\" , \"2003-11-25 10:47:32\"],\n",
    "    \"Inner_race_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n",
    "}\n",
    "\n",
    "B4 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-29 21:39:46\"],\n",
    "    \"normal\" : [\"2003-10-29 21:49:46\" , \"2003-11-15 05:08:46\"],\n",
    "    \"suspect\" : [\"2003-11-15 05:18:46\" , \"2003-11-18 19:12:30\"],\n",
    "    \"Rolling_element_failure\" : [\"2003-11-19 09:06:09\" , \"2003-11-22 17:36:56\"],\n",
    "    \"Stage_two_failure\" : [\"2003-11-22 17:46:56\" , \"2003-11-25 23:39:56\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "   * hanning window, FFT, MinMaxScaling 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택한 tag name 별 최대, 최소값 계산 함수\n",
    "def set_minmax_value(table, name, start_time_train, end_time_train):\n",
    "    \n",
    "    # URL 인코딩\n",
    "    start = quote(start_time_train)\n",
    "    end = quote(end_time_train)\n",
    "    \n",
    "    # min max 데이터 로드\n",
    "    df_ = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-scale.tql?table={table}&name={name}&start={start}&end={end}')\n",
    "    \n",
    "    ## Min , Max values 설정 \n",
    "    Min = df_.iloc[:,1:-1].T\n",
    "    Max = df_.iloc[:,2:].T\n",
    "    \n",
    "    return Min, Max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시간 로드 함수\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'time'\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # resample을 위해 임의의 value 컬럼 생성\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # resample 진행\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df = df.resample('1s').mean()\n",
    "    \n",
    "    # 결측값 제거\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # 임의의 value 컬럼 제거\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 함수\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # 같은 시간대 별 데이터로 전환\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # time 설정\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    # TIME을 기준으로 그룹화\n",
    "    # 초 단위로 반올림\n",
    "    filtered_data = df.copy()\n",
    "    filtered_data.loc[:, 'TIME'] = filtered_data['TIME'].dt.floor('S')\n",
    "    grouped = filtered_data.groupby('TIME')['s1-c5'].apply(list).reset_index()\n",
    "\n",
    "    # 리스트를 개별 열로 나누기\n",
    "    s1_c5_df = pd.DataFrame(grouped['s1-c5'].tolist())\n",
    "\n",
    "    # 'TIME' 열과 병합\n",
    "    result_df = pd.concat([grouped[['TIME']], s1_c5_df], axis=1)\n",
    "    \n",
    "    # label 설정\n",
    "    # 각 채널데이터 비정상부분의 시간 값을 기준으로 label 설정 \n",
    "    result_df['label'] = np.where((result_df['TIME'] >= \"2003-11-25 10:57:32\") & (result_df['TIME'] <= \"2003-11-25 23:39:56\"), 1, 0)\n",
    "    \n",
    "    result_df = result_df.drop(['TIME'], axis=1)\n",
    "    \n",
    "    result_df = result_df.dropna()\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 변환 함수\n",
    "# 시간 추가하려면 해당 과정 필요\n",
    "# window_size : 데이터를 묶어서 수집되는 주기 \n",
    "# step_size : 데이터의 간격 \n",
    "def add_time(time_df, start_time, batch_size, window_size, step_size):\n",
    "    \n",
    "    # 몇개의 데이터를 로드해야 되는지 계산\n",
    "    time = (batch_size * step_size)+ window_size - step_size - 1\n",
    "    \n",
    "    # 현재 시간의 인덱스 번호를 확인\n",
    "    # 없는 경우는 맨처음 시간이 없는 경우이기 때문에 맨처음 인덱스로 지정함 \n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # 현재 시간 기준 배치 데이터의 마지막 시간 설정 \n",
    "    end_time_ = str(time_df.index[index_now + time] + timedelta(seconds=1))\n",
    "    \n",
    "    # 다음 시작 시간의 인덱스 번호 설정\n",
    "    index_next = index_now + time - abs(window_size - step_size - 1)\n",
    "    \n",
    "    # 다음 시작 시간 설정\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL 인코딩\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanning window 함수 설정 \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Hanning 윈도우 생성\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # 각 행에 Hanning 윈도우 적용\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT 변환 함수\n",
    "def change_fft(sample_rate, df):\n",
    "    # 신호의 총 샘플 수\n",
    "    N = sample_rate\n",
    "    \n",
    "    # 각 행에 대해 FFT 적용\n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        # 각 행의 FFT 계산\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # FFT 결과의 절댓값을 계산하고 정규화 (유의미한 부분만)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # FFT 결과를 데이터 프레임으로 변환\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window(data, window_size, step_size):\n",
    "    \n",
    "    # 윈도우 슬라이딩 결과를 저장할 리스트\n",
    "    windows = []\n",
    "    labels = []\n",
    "    \n",
    "    # 슬라이딩 윈도우 적용\n",
    "    for i in range(0, data.shape[0] - window_size + 1, step_size):\n",
    "        window = data.iloc[i:i + window_size, :-1].values\n",
    "        label_array = data.iloc[i:i + window_size, -1].values\n",
    "        \n",
    "        # 레이블 배열에서 하나라도 비정상 값이 있으면 레이블을 1로 설정\n",
    "        if (label_array == 1).any():\n",
    "            label = 1  \n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        windows.append(window)\n",
    "        labels.append([label])\n",
    "        \n",
    "    return windows, labels    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling 설정\n",
    "\n",
    "* Process 컨셉상 전체 데이터를 불러오지 않기 때문에 최대 최소값을 사용하는 방식의 Min-Max Sclare를 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler 클래스 정의\n",
    "class MinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "\n",
    "    # 설정 파라미터 기반 scale 값 설정 \n",
    "    def transform(self, X, min_values, max_values):\n",
    "        X = np.array(X)\n",
    "        self.min_ = np.array(min_values)\n",
    "        self.max_ = np.array(max_values)\n",
    "        \n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        # scale 값이 0이되는 것을 막기 위해 1e-6 을 더해줌 \n",
    "        scale = (self.max_ - self.min_) + 1e-6\n",
    "        if np.any(scale == 0):\n",
    "            raise ValueError(\"Min and Max values are the same, resulting in a scale of 0.\")\n",
    "        \n",
    "        return (X - self.min_) / scale\n",
    "    \n",
    "    # 계산한 scale 값 기준 데이터 정규화 \n",
    "    def fit_transform(self, X, min_values, max_values):\n",
    "        \"\"\"Set parameters and then transform X\"\"\"\n",
    "        return self.transform(X,min_values, max_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 모델 설정 \n",
    "\n",
    "* LSTM AE 기본 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder 클래스 정의\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        # 인코더 LSTM\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.encoder_fc = nn.Linear(hidden_dim, 2*hidden_dim)\n",
    "        \n",
    "        # 디코더 LSTM\n",
    "        self.decoder_fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 인코더 부분\n",
    "        _, (h, _) = self.encoder_lstm(x)\n",
    "        latent = self.encoder_fc(h[-1])\n",
    "        \n",
    "        # 디코더 부분\n",
    "        hidden = self.decoder_fc(latent).unsqueeze(0).repeat(x.size(1), 1, 1).permute(1, 0, 2)\n",
    "        output, _ = self.decoder_lstm(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAutoencoder(\n",
      "  (encoder_lstm): LSTM(31, 15, num_layers=3, batch_first=True)\n",
      "  (encoder_fc): Linear(in_features=15, out_features=30, bias=True)\n",
      "  (decoder_fc): Linear(in_features=30, out_features=15, bias=True)\n",
      "  (decoder_lstm): LSTM(15, 31, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정 파라미터\n",
    "# 입력 데이터 컬럼 수\n",
    "# PCA 한값 \n",
    "input_dim = 31\n",
    "\n",
    "# LSMT hidden state 크기\n",
    "hidden_dim = 15\n",
    "\n",
    "# layer 수\n",
    "num_layers = 3\n",
    "\n",
    "# 학습률 \n",
    "learning_rate = 0.01\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMAutoencoder(input_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 구조 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "\n",
    "* 필요한 배치 크기의 데이터만 불러와서 학습하는 방법으로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수 설정\n",
    "def train(table, name, timeformat, model, start_time_train, end_time_train, batch_size, window_size, step_size, epochs, sample_rate, Min, Max, scaler, pca, time_df_train):\n",
    "        \n",
    "    # 초기 train loss 설정\n",
    "    train_loss = []\n",
    "\n",
    "    # 베스트 loss 초기화 \n",
    "    best_Loss=np.inf\n",
    "\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        # 학습 모드 설정 \n",
    "        model.train()\n",
    "        \n",
    "        # 초기 loss 초기화\n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        \n",
    "        # 초기 시작 시간 설정\n",
    "        start_time_ = start_time_train\n",
    "        \n",
    "        # 끝 시간 설정\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "\n",
    "        # while 문을 통해 데이터 호출 \n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # 배치 크기에 따라 데이터 로드 \n",
    "            start_time_, end_time_, next_start_time_, index_next= add_time(time_df_train, start_time_, batch_size, window_size, step_size)\n",
    "            \n",
    "            # 데이터 로드 \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # hanning window 적용\n",
    "            data_ = set_hanning_window(sample_rate, data.iloc[:,:-1])\n",
    "            \n",
    "            # FFT 적용 \n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # MinMax scaler 적용 \n",
    "            data_ = scaler.fit_transform(data_, Min, Max)\n",
    "            \n",
    "            # PCA 적용\n",
    "            data_ = pca.fit_transform(data_)\n",
    "            \n",
    "            # 데이터 프레임 + label 설정\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data['label'].values\n",
    "\n",
    "            # window 설정 \n",
    "            windows, labels = make_window(data_ , window_size, step_size)\n",
    "            \n",
    "            # 로드한 데이터가 비어 있을 경우 출력 \n",
    "            if len(data_) == 0:\n",
    "                print(\"데이터가 없습니다.\")\n",
    "            \n",
    "            # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "            if len(windows) == batch_size:\n",
    "                \n",
    "                # 총 배치수 체크용  \n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # 데이터를 numpy 배열로 변환\n",
    "                input_data = np.array(windows)\n",
    "\n",
    "                # 데이터를 Tensor로 변환\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "\n",
    "                # 옵티마이저 최적화 \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 모델 입력\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # loss 계산\n",
    "                loss = criterion(outputs, input_data)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # 배치 리셋\n",
    "                windows = []\n",
    "            \n",
    "            # 다음 시작 시간 설정    \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "            \n",
    "            # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "            if index_next + (batch_size * step_size)+ window_size - step_size - 1 > len(time_df_train):\n",
    "                break\n",
    "        \n",
    "        if total_step > 0:\n",
    "            train_loss.append(running_loss / total_step)\n",
    "            print(f'\\ntrain loss: {np.mean(train_loss)}')\n",
    "            \n",
    "        if best_Loss > np.mean(train_loss):\n",
    "            best_Loss = np.mean(train_loss)\n",
    "            torch.save(model, f'./result/Nasa_Bearing_LSTM_AE_New_batch_비지도.pt')\n",
    "            print('베스트 모델 저장') \n",
    "            \n",
    "        epochs.set_postfix_str(f\"epoch = {epoch}, best_Loss = {best_Loss}\")\n",
    "               \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fcab756b8048c4a436922cf222b078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.0001948600776426314\n",
      "베스트 모델 저장\n",
      "\n",
      "train loss: 0.00010036500810252214\n",
      "베스트 모델 저장\n",
      "\n",
      "train loss: 6.882995255440574e-05\n",
      "베스트 모델 저장\n",
      "\n",
      "train loss: 5.305689636732798e-05\n",
      "베스트 모델 저장\n",
      "\n",
      "train loss: 4.359030993631071e-05\n",
      "베스트 모델 저장\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMAutoencoder(\n",
       "  (encoder_lstm): LSTM(31, 15, num_layers=3, batch_first=True)\n",
       "  (encoder_fc): Linear(in_features=15, out_features=30, bias=True)\n",
       "  (decoder_fc): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (decoder_lstm): LSTM(15, 31, num_layers=3, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### 학습 파라미터 설정 ################################################\n",
    "# tag table 이름 설정\n",
    "table = 'bearing'\n",
    "# tag name 설정\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# 학습 시작 시간 설정\n",
    "start_time_train = '2003-10-22 12:06:24'\n",
    "# 학습 끝 시간 설정\n",
    "end_time_train = '2003-11-22 00:00:00'\n",
    "# 시간 포멧 설정 \n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# 배치 사이즈 설정\n",
    "batch_size = 32\n",
    "# window size 설정\n",
    "window_size = 3\n",
    "# step size 설정 -> window size 보다 크거나 같으면 안됨 \n",
    "step_size = 1\n",
    "# epoch 설정\n",
    "epochs = trange(5, desc='training')\n",
    "# 최대, 최소값 설정 \n",
    "Min, Max = set_minmax_value(table, name, start_time_train, end_time_train)\n",
    "# sample rate 설정\n",
    "sample_rate = 20480\n",
    "# Min-Max scaler 설정 \n",
    "scaler = MinMaxScaler()\n",
    "# PCA 설정\n",
    "# 95%의 분산을 설명하는 주성분 선택\n",
    "pca = PCA(n_components=31)\n",
    "# 학습 시간 리스트 로드 \n",
    "time_df_train = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "################################################ 학습 진행 ####################################################\n",
    "train(table, name, timeformat, model, start_time_train, end_time_train, batch_size, window_size, step_size, epochs, sample_rate, Min, Max, scaler, pca, time_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임계값 설정\n",
    "\n",
    "* validation data를 사용하여 임계값 계산 \n",
    "  * Max + K x 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "model_ = torch.load(f'./result/Nasa_Bearing_LSTM_AE_New_batch_비지도.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_set(table, name, timeformat, model, start_time_valid, end_time_valid, batch_size, window_size, step_size, k, sample_rate, Min, Max, scaler,time_df_valid):\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "         \n",
    "        # 초기 시작 시간 설정\n",
    "        start_time_v = start_time_valid\n",
    "        \n",
    "        # 끝 시간 설정\n",
    "        end_time_valid = str(time_df_valid.index[-1])\n",
    "        \n",
    "        # while 문을 통해 데이터 호출 \n",
    "        while start_time_v < end_time_valid:\n",
    "            \n",
    "            # 배치 크기에 따라 데이터 로드 \n",
    "            start_time_v, end_time_v, next_start_time_v, index_next_v = add_time(time_df_valid, start_time_v, batch_size, window_size, step_size)\n",
    "            \n",
    "            # 데이터 로드 \n",
    "            data_v = data_load(table, name, start_time_v, end_time_v, timeformat)\n",
    "            \n",
    "            # hanning window 적용\n",
    "            data_ = set_hanning_window(sample_rate, data_v.iloc[:,:-1])\n",
    "            \n",
    "            # FFT 적용 \n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # MinMax scaler 적용 \n",
    "            data_ = scaler.fit_transform(data_, Min, Max)\n",
    "            \n",
    "            # PCA 적용\n",
    "            data_ = pca.fit_transform(data_)\n",
    "            \n",
    "            # 데이터 프레임 + label 설정\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data_v['label'].values\n",
    "\n",
    "            # window 설정 \n",
    "            windows_v, labels_v = make_window(data_ , window_size, step_size)\n",
    "            \n",
    "            # 로드한 데이터가 비어 있을 경우 출력 \n",
    "            if len(data_) == 0:\n",
    "                print(\"데이터가 없습니다.\")\n",
    "            \n",
    "            # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "            if len(windows_v) == batch_size:\n",
    "                \n",
    "                # 데이터를 numpy 배열로 변환\n",
    "                input_data_val = np.array(windows_v)\n",
    "                \n",
    "                # 데이터를 Tensor로 변환\n",
    "                input_data_val = torch.tensor(input_data_val, dtype=torch.float32).to(device).float()\n",
    "\n",
    "                # 모델 입력\n",
    "                outputs_val = model(input_data_val)\n",
    "                \n",
    "                # loss 계산\n",
    "                loss_val = criterion(outputs_val, input_data_val)\n",
    "            \n",
    "                valid_loss.append(loss_val.item())\n",
    "                \n",
    "                # 배치 리셋\n",
    "                windows_v = []\n",
    "                \n",
    "            # 다음 시작 시간 설정    \n",
    "            start_time_v = unquote(next_start_time_v)\n",
    "            \n",
    "            # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "            if index_next_v + (batch_size * step_size)+ window_size - step_size - 1 > len(time_df_valid):\n",
    "                break\n",
    "            \n",
    "            \n",
    "        # 임계값 계산\n",
    "        threshold =  max(valid_loss) + k * statistics.stdev(valid_loss)\n",
    "      \n",
    "                    \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2756313173981703e-05\n"
     ]
    }
   ],
   "source": [
    "######################################## validation 파라미터 설정 #############################################\n",
    "\n",
    "# 검증 시작 시간 설정\n",
    "start_time_valid = '2003-11-22 00:00:00'\n",
    "# 검증 끝 시간 설정\n",
    "end_time_valid = '2003-11-23 13:00:00'\n",
    "# 임계값 파라미터 설정\n",
    "k = 10\n",
    "\n",
    "# 검증 시간 리스트 로드\n",
    "time_df_valid = time_data_load(table, name, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "\n",
    "################################################ 학습 진행 ####################################################\n",
    "threshold = threshold_set(table, name, timeformat, model_, start_time_valid, end_time_valid , batch_size, window_size, step_size, k, sample_rate, Min, Max, scaler,time_df_valid)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 테스트 함수 \n",
    "## 테스트 함수 \n",
    "def test(table, name, timeformat, model, start_time_test, end_time_test, batch_size, window_size, step_size, threshold, sample_rate, Min, Max, scaler, pca, time_df_test):\n",
    "    test_loss = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # 초기 시작 시간 설정\n",
    "        start_time_t = start_time_test\n",
    "        \n",
    "        # 끝 시간 설정\n",
    "        end_time_test = str(time_df_test.index[-1])\n",
    "        \n",
    "        # while 문을 통해 데이터 호출 \n",
    "        while start_time_t < end_time_test:\n",
    "            \n",
    "            # 배치 크기에 따라 데이터 로드 \n",
    "            start_time_t, end_time_t, next_start_time_t, index_next_t = add_time(time_df_test, start_time_t, batch_size, window_size, step_size)\n",
    "            \n",
    "            # 데이터 로드 \n",
    "            data_t = data_load(table, name, start_time_t, end_time_t, timeformat)\n",
    "            \n",
    "            # hanning window 적용\n",
    "            data_ = set_hanning_window(sample_rate, data_t.iloc[:,:-1])\n",
    "            \n",
    "            # FFT 적용 \n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # MinMax scaler 적용 \n",
    "            data_ = scaler.fit_transform(data_, Min, Max)\n",
    "            \n",
    "            # PCA 적용\n",
    "            data_ = pca.fit_transform(data_)\n",
    "            \n",
    "            # 데이터 프레임 + label 설정\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data_t['label'].values\n",
    "\n",
    "            # window 설정 \n",
    "            windows_t, labels_t = make_window(data_ , window_size, step_size)\n",
    "            \n",
    "            # 로드한 데이터가 비어 있을 경우 출력 \n",
    "            if len(data_) == 0:\n",
    "                print(\"데이터가 없습니다.\")\n",
    "            \n",
    "            # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "            if len(windows_t) == batch_size:\n",
    "                \n",
    "                # 데이터를 numpy 배열로 변환\n",
    "                input_data_test = np.array(windows_t)\n",
    "                input_data_label = np.array(labels_t)\n",
    "                \n",
    "                # 데이터를 Tensor로 변환\n",
    "                input_data_test = torch.tensor(input_data_test, dtype=torch.float32).to(device).float()\n",
    "                input_data_label = torch.tensor(input_data_label, dtype=torch.float32).to(device).long()\n",
    "                \n",
    "                # DataLoader 생성\n",
    "                dataset = TensorDataset(input_data_test, input_data_label)\n",
    "                data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "                \n",
    "                for batch_input, batch_label in data_loader:\n",
    "\n",
    "                    # 모델 입력\n",
    "                    outputs_test = model(batch_input)\n",
    "                    \n",
    "                    # loss 계산\n",
    "                    loss_test = criterion(outputs_test, batch_input)\n",
    "                    \n",
    "                    test_loss.append(loss_test.item())\n",
    "                    labels.append(batch_label.item())\n",
    "                \n",
    "                # 배치 리셋\n",
    "                windows_t = []\n",
    "                \n",
    "            # 다음 시작 시간 설정    \n",
    "            start_time_t = unquote(next_start_time_t) \n",
    "            \n",
    "            # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "            if index_next_t + (batch_size * step_size)+ window_size - step_size - 1 > len(time_df_test):\n",
    "                break\n",
    "            \n",
    "    # 최종 결과 생성 \n",
    "    final_df = pd.DataFrame(test_loss, columns=['reconst_score'])\n",
    "    final_df['label'] = labels\n",
    "\n",
    "    # 각 임계값 별 label 설정 \n",
    "    final_df['pred_label'] = np.where(final_df['reconst_score']>threshold,1,0)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## 테스트 파라미터 설정 #############################################\n",
    "\n",
    "# 검증 시작 시간 설정\n",
    "start_time_test = '2003-11-23 13:00:00'\n",
    "# 검증 끝 시간 설정\n",
    "end_time_test = '2003-11-26 00:00:00'\n",
    "# 검증 시간 리스트 로드\n",
    "time_df_test = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "######################################## 테스트 진행 #############################################\n",
    "final_df = test(table, name, timeformat, model_, start_time_test, end_time_test, batch_size, window_size, step_size, threshold, sample_rate, Min, Max, scaler, pca, time_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       178\n",
      "           1       0.93      1.00      0.97        14\n",
      "\n",
      "    accuracy                           0.99       192\n",
      "   macro avg       0.97      1.00      0.98       192\n",
      "weighted avg       1.00      0.99      0.99       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(final_df['label'], final_df['pred_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    178\n",
       "1     14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_label\n",
       "0    177\n",
       "1     15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['pred_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과적합 체크\n",
    "\n",
    "* test 데이터뿐만 아니라 train, validation 데이터를 활용해 F1 score 계산\n",
    "* 만약 train, validation 결과가 좋지 않다면 모델 과적합으로 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_train = test(table, name, timeformat, model_, start_time_train, end_time_train, batch_size, window_size, step_size, threshold, sample_rate, Min, Max, scaler, pca, time_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1728\n",
      "\n",
      "    accuracy                           1.00      1728\n",
      "   macro avg       1.00      1.00      1.00      1728\n",
      "weighted avg       1.00      1.00      1.00      1728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(final_df_train['label'], final_df_train['pred_label'],labels=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_valid = test(table, name, timeformat, model_, start_time_valid, end_time_valid, batch_size, window_size, step_size, threshold, sample_rate, Min, Max, scaler, pca, time_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       192\n",
      "\n",
      "    accuracy                           1.00       192\n",
      "   macro avg       1.00      1.00      1.00       192\n",
      "weighted avg       1.00      1.00      1.00       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(final_df_valid['label'], final_df_valid['pred_label'],labels=[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
