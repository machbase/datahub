{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bearing Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "import statistics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results\n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'bearing'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list\n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s1-c1',\n",
       " 's1-c2',\n",
       " 's1-c3',\n",
       " 's1-c4',\n",
       " 's1-c5',\n",
       " 's1-c6',\n",
       " 's1-c7',\n",
       " 's1-c8',\n",
       " 's2-c1',\n",
       " 's2-c2',\n",
       " 's2-c3',\n",
       " 's2-c4',\n",
       " 's3-c1',\n",
       " 's3-c2',\n",
       " 's3-c3',\n",
       " 's3-c4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the Nasa bearing dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names related to the s1-c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s1-c5'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = ['s1-c5']\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Nasa Bearing Dataset\n",
    "* Load the train, validation, and test datasets separately when loading the data.\n",
    "* The example focuses on anomaly detection for the 3rd bearing -> using 's1-c5' as the Tag Name.\n",
    "* Label all states except for the faulty condition as normal for the labeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the status of each bearing based on time ranges\n",
    "B1 ={\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-23 09:14:13\"],\n",
    "    \"suspect\" : [\"2003-10-23 09:24:13\" , \"2003-11-08 12:11:44\"],\n",
    "    \"normal\" : [\"2003-11-08 12:21:44\" , \"2003-11-19 21:06:07\"],\n",
    "    \"suspect_1\" : [\"2003-11-19 21:16:07\" , \"2003-11-24 20:47:32\"],\n",
    "    \"imminent_failure\" : [\"2003-11-24 20:57:32\",\"2003-11-25 23:39:56\"]\n",
    "}\n",
    "B2 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n",
    "    \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-24 01:01:24\"],\n",
    "    \"suspect\" : [\"2003-11-24 01:11:24\" , \"2003-11-25 10:47:32\"],\n",
    "    \"imminient_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n",
    "}\n",
    "\n",
    "B3 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-11-01 21:41:44\"],\n",
    "    \"normal\" : [\"2003-11-01 21:51:44\" , \"2003-11-22 09:16:56\"],\n",
    "    \"suspect\" : [\"2003-11-22 09:26:56\" , \"2003-11-25 10:47:32\"],\n",
    "    \"Inner_race_failure\" : [\"2003-11-25 10:57:32\" , \"2003-11-25 23:39:56\"]\n",
    "}\n",
    "\n",
    "B4 = {\n",
    "    \"early\" : [\"2003-10-22 12:06:24\" , \"2003-10-29 21:39:46\"],\n",
    "    \"normal\" : [\"2003-10-29 21:49:46\" , \"2003-11-15 05:08:46\"],\n",
    "    \"suspect\" : [\"2003-11-15 05:18:46\" , \"2003-11-18 19:12:30\"],\n",
    "    \"Rolling_element_failure\" : [\"2003-11-19 09:06:09\" , \"2003-11-22 17:36:56\"],\n",
    "    \"Stage_two_failure\" : [\"2003-11-22 17:46:56\" , \"2003-11-25 23:39:56\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data  \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # Set TIME column\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    # Group by TIME\n",
    "    # Round to the nearest second\n",
    "    filtered_data = df.copy()\n",
    "    filtered_data.loc[:, 'TIME'] = filtered_data['TIME'].dt.floor('S')\n",
    "    grouped = filtered_data.groupby('TIME')['s1-c5'].apply(list).reset_index()\n",
    "\n",
    "    # Split the list into individual columns\n",
    "    s1_c5_df = pd.DataFrame(grouped['s1-c5'].tolist())\n",
    "\n",
    "    # Merge with the 'TIME' column\n",
    "    result_df = pd.concat([grouped[['TIME']], s1_c5_df], axis=1)\n",
    "    \n",
    "    # Set labels\n",
    "    # Assign labels based on abnormal time ranges for each channel data\n",
    "    result_df['label'] = np.where((result_df['TIME'] >= \"2003-11-25 10:57:32\") & (result_df['TIME'] <= \"2003-11-25 23:39:56\"), 1, 0)\n",
    "    \n",
    "    result_df = result_df.drop(['TIME'], axis=1)\n",
    "    \n",
    "    result_df = result_df.dropna()\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data time loading function\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'time'\n",
    "    \n",
    "    # Load the data  \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # Create a dummy value column for resampling\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # Perform resampling\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df = df.resample('1s').mean()\n",
    "    \n",
    "    # Remove missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove the dummy value column\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time update function\n",
    "# Update start and end times based on batch size\n",
    "# window_size: The period over which data is collected in batches\n",
    "# step_size: The interval between the data points\n",
    "def update_time(time_df, start_time, batch_size, window_size, step_size):\n",
    "    \n",
    "    # Calculate how many data points need to be loaded\n",
    "    time = (batch_size * step_size)+ window_size - step_size - 1\n",
    "    \n",
    "    # Check the index number of the current time\n",
    "    # If not found, set to the first index as there is no data for the current time\n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # Set the end time for the batch data based on the current time \n",
    "    end_time_ = str(time_df.index[index_now + time] + timedelta(seconds=1))\n",
    "    \n",
    "    # Set the index number for the next start time\n",
    "    index_next = index_now + time - abs(window_size - step_size - 1)\n",
    "    \n",
    "    # Set the next start time\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL encoding\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the maximum and minimum values for selected tag names\n",
    "def set_minmax_value(table, name, start_time_train, end_time_train):\n",
    "    \n",
    "    # URL encoding\n",
    "    start = quote(start_time_train)\n",
    "    end = quote(end_time_train)\n",
    "    \n",
    "    # Load Min, Max data\n",
    "    df_ = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-scale.tql?table={table}&name={name}&start={start}&end={end}')\n",
    "    \n",
    "    # Set Min, Max values\n",
    "    Min = df_.iloc[:,1:-1].T\n",
    "    Max = df_.iloc[:,2:].T\n",
    "    \n",
    "    return Min, Max  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 hanning window\n",
    "* 2 FFT\n",
    "* 3 MinMax Scaling\n",
    "* 4 PCA -> Apply during training\n",
    "* 5 Window sliding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hanning Window Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanning window function setup \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Generate Hanning window\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # Apply Hanning window to each row\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FFT (Fast Fourier Transform) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT transformation function\n",
    "def change_fft(sample_rate, df):\n",
    "    # Total number of samples in the signal\n",
    "    N = sample_rate\n",
    "    \n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # Apply FFT to each row\n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        # Calculate FFT for each row\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # Compute the absolute value of the FFT results and normalize (only the meaningful part)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # Convert FFT results to a DataFrame\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Min-Max Scaling Setup\n",
    "* Set up a Min-Max Scaler that uses the maximum and minimum values, as the entire dataset is not loaded due to the process concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the MinMaxScaler class\n",
    "class MinMaxScaler_custom:\n",
    "    def __init__(self):\n",
    "        self.min_ = None\n",
    "        self.max_ = None\n",
    "\n",
    "    # Set scale values based on the specified parameters\n",
    "    def transform(self, X, min_values, max_values):\n",
    "        X = np.array(X)\n",
    "        self.min_ = np.array(min_values)\n",
    "        self.max_ = np.array(max_values)\n",
    "        \n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        # Add 1e-6 to prevent the scale value from becoming 0\n",
    "        scale = (self.max_ - self.min_) + 1e-6\n",
    "        if np.any(scale == 0):\n",
    "            raise ValueError(\"Min and Max values are the same, resulting in a scale of 0.\")\n",
    "        \n",
    "        return (X - self.min_) / scale\n",
    "    \n",
    "    # Normalize data based on calculated scale values\n",
    "    def fit_transform(self, X, min_values, max_values):\n",
    "        \"\"\"Set parameters and then transform X\"\"\"\n",
    "        return self.transform(X, min_values, max_values)\n",
    "\n",
    "    # Inverse the normalized data back to original values\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"Inverse the transformation and return original values\"\"\"\n",
    "        if self.min_ is None or self.max_ is None:\n",
    "            raise ValueError(\"Min and Max values are not set.\")\n",
    "        \n",
    "        X_scaled = np.array(X_scaled)\n",
    "        scale = (self.max_ - self.min_) + 1e-6\n",
    "        \n",
    "        return X_scaled * scale + self.min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Window sliding setup\n",
    "* Window size: Determines how many time points to group together.\n",
    "* Step size: The time interval by which the window moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window(data, window_size, step_size):\n",
    "    \n",
    "    # List to store the results of sliding windows\n",
    "    windows = []\n",
    "    labels = []\n",
    "    \n",
    "    # Apply sliding window\n",
    "    for i in range(0, data.shape[0] - window_size + 1, step_size):\n",
    "        window = data.iloc[i:i + window_size, :-1].values\n",
    "        label_array = data.iloc[i:i + window_size, -1].values\n",
    "        \n",
    "        # Set the label to 1 if there is any abnormal value in the label array\n",
    "        if (label_array == 1).any():\n",
    "            label = 1  \n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        windows.append(window)\n",
    "        labels.append([label])\n",
    "        \n",
    "    return windows, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using LSTM AE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder class definition\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.encoder_fc = nn.Linear(hidden_dim, 2*hidden_dim)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.decoder_fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder part\n",
    "        _, (h, _) = self.encoder_lstm(x)\n",
    "        latent = self.encoder_fc(h[-1])\n",
    "        \n",
    "        # Decoder part\n",
    "        hidden = self.decoder_fc(latent).unsqueeze(0).repeat(x.size(1), 1, 1).permute(1, 0, 2)\n",
    "        output, _ = self.decoder_lstm(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAutoencoder(\n",
      "  (encoder_lstm): LSTM(31, 15, num_layers=3, batch_first=True)\n",
      "  (encoder_fc): Linear(in_features=15, out_features=30, bias=True)\n",
      "  (decoder_fc): Linear(in_features=30, out_features=15, bias=True)\n",
      "  (decoder_lstm): LSTM(15, 31, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "\n",
    "# number of input data columns\n",
    "input_dim = 31\n",
    "\n",
    "# LSMT hidden state size\n",
    "hidden_dim = 15\n",
    "\n",
    "# layer size\n",
    "num_layers = 3\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = LSTMAutoencoder(input_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "* Save the model with the Best Loss based on the training data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train(table, name, timeformat, model, batch_size, window_size, step_size, epochs, sample_rate, Min, Max, scaler, pca, time_df_train):\n",
    "        \n",
    "    # Initialize training loss\n",
    "    train_loss = []\n",
    "\n",
    "    # Initialize Best Loss value\n",
    "    best_Loss=np.inf\n",
    "\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        \n",
    "        # Set initial start time\n",
    "        start_time_ = str(time_df_train.index[0])\n",
    "        \n",
    "        # Set end time\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "\n",
    "        # Use a while loop to call data \n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_, end_time_, next_start_time_, index_next= update_time(time_df_train, start_time_, batch_size, window_size, step_size)\n",
    "            \n",
    "            # Load batch data \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # Apply hanning window\n",
    "            data_ = set_hanning_window(sample_rate, data.iloc[:,:-1])\n",
    "            \n",
    "            # Apply FFT\n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # Apply MinMax scaler\n",
    "            data_ = scaler.fit_transform(data_, Min, Max)\n",
    "            \n",
    "            # Apply PCA\n",
    "            data_ = pca.fit_transform(data_)\n",
    "            \n",
    "            # Setting up DataFrame + label\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data['label'].values\n",
    "\n",
    "            # Set window \n",
    "            windows, labels = make_window(data_ , window_size, step_size)\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(data_) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(windows) == batch_size:\n",
    "                \n",
    "                # Check total batch count  \n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data = np.array(windows)\n",
    "\n",
    "                # Convert data to Tensor\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "\n",
    "                # Optimize the optimizer\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Input to the model\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, input_data)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Reset batch data\n",
    "                windows = []\n",
    "            \n",
    "            # Set the next start time   \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next + (batch_size * step_size)+ window_size - step_size - 1 >= len(time_df_train):\n",
    "                break\n",
    "        \n",
    "        if total_step > 0:\n",
    "            train_loss.append(running_loss / total_step)\n",
    "            print(f'\\ntrain loss: {np.mean(train_loss)}')\n",
    "            \n",
    "        # best model save     \n",
    "        if best_Loss > np.mean(train_loss):\n",
    "            best_Loss = np.mean(train_loss)\n",
    "            torch.save(model, f'./result/Nasa_Bearing_LSTM_AE_New_batch.pt')\n",
    "            print('Save the best model.') \n",
    "            \n",
    "        epochs.set_postfix_str(f\"epoch = {epoch}, best_Loss = {best_Loss}\")\n",
    "               \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a93a76d4be438e95d38c1a59915046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.0001948600776426314\n",
      "Save the best model.\n",
      "\n",
      "train loss: 0.00010036500810252214\n",
      "Save the best model.\n",
      "\n",
      "train loss: 6.882995255440574e-05\n",
      "Save the best model.\n",
      "\n",
      "train loss: 5.305689636732798e-05\n",
      "Save the best model.\n",
      "\n",
      "train loss: 4.359030993631071e-05\n",
      "Save the best model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMAutoencoder(\n",
       "  (encoder_lstm): LSTM(31, 15, num_layers=3, batch_first=True)\n",
       "  (encoder_fc): Linear(in_features=15, out_features=30, bias=True)\n",
       "  (decoder_fc): Linear(in_features=30, out_features=15, bias=True)\n",
       "  (decoder_lstm): LSTM(15, 31, num_layers=3, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### Training Parameter Settings ################################################\n",
    "# Set tag table name\n",
    "table = 'bearing'\n",
    "# Set tag name\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set the start time for the train data\n",
    "start_time_train = '2003-10-22 12:06:24'\n",
    "# Set the end time for the train data\n",
    "end_time_train = '2003-11-22 00:00:00'\n",
    "# Set time format\n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "# Set window size\n",
    "window_size = 3\n",
    "# Set step size -> it must be less than or equal to the window size\n",
    "step_size = 1\n",
    "# Set number of epochs\n",
    "epochs = trange(5, desc='training')\n",
    "# Set Maximum and Minimum Values \n",
    "Min, Max = set_minmax_value(table, name, start_time_train, end_time_train)\n",
    "# Set sample rate\n",
    "sample_rate = 20480\n",
    "# Set Min-Max scaler\n",
    "scaler = MinMaxScaler_custom()\n",
    "# Set PCA\n",
    "# Select principal components that explain 95% of the variance\n",
    "pca = PCA(n_components=31)\n",
    "# Load training time list \n",
    "time_df_train = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "########################################### Proceed with training ################################################\n",
    "train(table, name, timeformat, model, batch_size, window_size, step_size, epochs, sample_rate, Min, Max, scaler, pca, time_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Setting\n",
    "* Calculate the threshold using validation data\n",
    "  * Max + K × Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation function\n",
    "def threshold_set(table, name, timeformat, model, batch_size, window_size, step_size, k, sample_rate, Min, Max, scaler,time_df_valid):\n",
    "    \n",
    "    # Initialize validation loss\n",
    "    valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "         \n",
    "        # Set initial start time\n",
    "        start_time_v = str(time_df_valid.index[0])\n",
    "        \n",
    "        # Set end time\n",
    "        end_time_valid = str(time_df_valid.index[-1])\n",
    "        \n",
    "        # Use a while loop to call data \n",
    "        while start_time_v < end_time_valid:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_v, end_time_v, next_start_time_v, index_next_v = update_time(time_df_valid, start_time_v, batch_size, window_size, step_size)\n",
    "            \n",
    "            # Load batch data \n",
    "            data_v = data_load(table, name, start_time_v, end_time_v, timeformat)\n",
    "            \n",
    "            # Apply hanning window\n",
    "            data_ = set_hanning_window(sample_rate, data_v.iloc[:,:-1])\n",
    "            \n",
    "            # Apply FFT\n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # Apply MinMax scaler \n",
    "            data_ = scaler.fit_transform(data_, Min, Max)\n",
    "            \n",
    "            # Apply PCA\n",
    "            data_ = pca.fit_transform(data_)\n",
    "            \n",
    "            # Setting up DataFrame + label\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data_v['label'].values\n",
    "\n",
    "            # Set window \n",
    "            windows_v, labels_v = make_window(data_ , window_size, step_size)\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(data_) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(windows_v) == batch_size:\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data_val = np.array(windows_v)\n",
    "                \n",
    "                # Convert data to Tensor\n",
    "                input_data_val = torch.tensor(input_data_val, dtype=torch.float32).to(device).float()\n",
    "\n",
    "                # Input to the model\n",
    "                outputs_val = model(input_data_val)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss_val = criterion(outputs_val, input_data_val)\n",
    "            \n",
    "                valid_loss.append(loss_val.item())\n",
    "                \n",
    "                # Reset batch data\n",
    "                windows_v = []\n",
    "                \n",
    "            # Set the next start time  \n",
    "            start_time_v = unquote(next_start_time_v)\n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next_v + (batch_size * step_size)+ window_size - step_size - 1 >= len(time_df_valid):\n",
    "                break\n",
    "            \n",
    "            \n",
    "        # Calculate the threshold\n",
    "        threshold =  max(valid_loss) + k * statistics.stdev(valid_loss)\n",
    "      \n",
    "                    \n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2756313173981703e-05\n"
     ]
    }
   ],
   "source": [
    "########################################### validation Parameter Settings ################################################\n",
    "# Load the best model\n",
    "model_ = torch.load(f'./result/Nasa_Bearing_LSTM_AE_New_batch.pt') \n",
    "# Set the start time for the validation data\n",
    "start_time_valid = '2003-11-22 00:00:00'\n",
    "# Set the end time for the validation data\n",
    "end_time_valid = '2003-11-23 13:00:00'\n",
    "# Set the threshold parameters\n",
    "k = 10\n",
    "# Load validation time list\n",
    "time_df_valid = time_data_load(table, name, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "\n",
    "########################################### Proceed with validation ################################################\n",
    "threshold = threshold_set(table, name, timeformat, model_, batch_size, window_size, step_size, k, sample_rate, Min, Max, scaler, time_df_valid)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing function\n",
    "def test(table, name, timeformat, model, batch_size, window_size, step_size, threshold, sample_rate, Min, Max, scaler, pca, time_df_test):\n",
    "    \n",
    "    # Initial settings \n",
    "    test_loss = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Set the initial start time\n",
    "        start_time_t = str(time_df_test.index[0])\n",
    "        \n",
    "        # Set the end time\n",
    "        end_time_test = str(time_df_test.index[-1])\n",
    "        \n",
    "        # Use a while loop to call data   \n",
    "        while start_time_t < end_time_test:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_t, end_time_t, next_start_time_t, index_next_t = update_time(time_df_test, start_time_t, batch_size, window_size, step_size)\n",
    "            \n",
    "            # Load batch data\n",
    "            data_t = data_load(table, name, start_time_t, end_time_t, timeformat)\n",
    "            \n",
    "            # Apply hanning window\n",
    "            data_ = set_hanning_window(sample_rate, data_t.iloc[:,:-1])\n",
    "            \n",
    "            # Apply FFT \n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # Apply MinMax scaler \n",
    "            data_ = scaler.fit_transform(data_, Min, Max)\n",
    "            \n",
    "            # Apply PCA\n",
    "            data_ = pca.fit_transform(data_)\n",
    "            \n",
    "            # Setting up DataFrame + label\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data_t['label'].values\n",
    "\n",
    "            # Set window \n",
    "            windows_t, labels_t = make_window(data_ , window_size, step_size)\n",
    "            \n",
    "            # Print if the loaded data is empty  \n",
    "            if len(data_) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(windows_t) == batch_size:\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data_test = np.array(windows_t)\n",
    "                input_data_label = np.array(labels_t)\n",
    "                \n",
    "                # Convert data to Tensor\n",
    "                input_data_test = torch.tensor(input_data_test, dtype=torch.float32).to(device).float()\n",
    "                input_data_label = torch.tensor(input_data_label, dtype=torch.float32).to(device).long()\n",
    "                \n",
    "                # Create DataLoader\n",
    "                dataset = TensorDataset(input_data_test, input_data_label)\n",
    "                data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "                \n",
    "                for batch_input, batch_label in data_loader:\n",
    "\n",
    "                    # Input to the model\n",
    "                    outputs_test = model(batch_input)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss_test = criterion(outputs_test, batch_input)\n",
    "                    \n",
    "                    test_loss.append(loss_test.item())\n",
    "                    labels.append(batch_label.item())\n",
    "                \n",
    "                # Reset batch data\n",
    "                windows_t = []\n",
    "                \n",
    "            # Set the next start time   \n",
    "            start_time_t = unquote(next_start_time_t) \n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next_t + (batch_size * step_size)+ window_size - step_size - 1 >= len(time_df_test):\n",
    "                break\n",
    "            \n",
    "    # Generate final results\n",
    "    final_df = pd.DataFrame(test_loss, columns=['reconst_score'])\n",
    "    final_df['label'] = labels\n",
    "\n",
    "    # Set labels based on each threshold\n",
    "    final_df['pred_label'] = np.where(final_df['reconst_score'] > threshold, 1, 0)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Test Parameter Settings ################################################\n",
    "\n",
    "# Set the start time for the test data\n",
    "start_time_test = '2003-11-23 13:00:00'\n",
    "# Set the end time for the test data\n",
    "end_time_test = '2003-11-26 00:00:00'\n",
    "# Load the test time list\n",
    "time_df_test = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "######################################## Proceed with testing #############################################\n",
    "final_df = test(table, name, timeformat, model_, batch_size, window_size, step_size, threshold, sample_rate, Min, Max, scaler, pca, time_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       178\n",
      "           1       0.93      1.00      0.97        14\n",
      "\n",
      "    accuracy                           0.99       192\n",
      "   macro avg       0.97      1.00      0.98       192\n",
      "weighted avg       1.00      0.99      0.99       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print F1 Score based on testing data\n",
    "print(classification_report(final_df['label'], final_df['pred_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Check\n",
    "\n",
    "* Calculate the F1 score using not only the test data but also the train and validation datasets.\n",
    "* If the results for the train and validation are not satisfactory, it can be determined that the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct testing on the training data\n",
    "final_df_train = test(table, name, timeformat, model_, batch_size, window_size, step_size, threshold, sample_rate, Min, Max, scaler, pca, time_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1728\n",
      "\n",
      "    accuracy                           1.00      1728\n",
      "   macro avg       1.00      1.00      1.00      1728\n",
      "weighted avg       1.00      1.00      1.00      1728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print F1 Score based on training data\n",
    "print(classification_report(final_df_train['label'], final_df_train['pred_label'],labels=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct testing on the validation data\n",
    "final_df_valid = test(table, name, timeformat, model_, batch_size, window_size, step_size, threshold, sample_rate, Min, Max, scaler, pca, time_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       192\n",
      "\n",
      "    accuracy                           1.00       192\n",
      "   macro avg       1.00      1.00      1.00       192\n",
      "weighted avg       1.00      1.00      1.00       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print F1 Score based on validation data\n",
    "print(classification_report(final_df_valid['label'], final_df_valid['pred_label'],labels=[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
