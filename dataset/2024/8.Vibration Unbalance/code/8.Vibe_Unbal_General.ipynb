{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Set path for saving model training results \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'vibe_unbal'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list\n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_Measured_RPM',\n",
       " '0_V_in',\n",
       " '0_Vibration_1',\n",
       " '0_Vibration_2',\n",
       " '0_Vibration_3',\n",
       " '0_unbalance_Factor',\n",
       " '1_Measured_RPM',\n",
       " '1_V_in',\n",
       " '1_Vibration_1',\n",
       " '1_Vibration_2',\n",
       " '1_Vibration_3',\n",
       " '1_unbalance_Factor',\n",
       " '2_Measured_RPM',\n",
       " '2_V_in',\n",
       " '2_Vibration_1',\n",
       " '2_Vibration_2',\n",
       " '2_Vibration_3',\n",
       " '2_unbalance_Factor',\n",
       " '3_Measured_RPM',\n",
       " '3_V_in',\n",
       " '3_Vibration_1',\n",
       " '3_Vibration_2',\n",
       " '3_Vibration_3',\n",
       " '3_unbalance_Factor',\n",
       " '4_Measured_RPM',\n",
       " '4_V_in',\n",
       " '4_Vibration_1',\n",
       " '4_Vibration_2',\n",
       " '4_Vibration_3',\n",
       " '4_unbalance_Factor',\n",
       " '5_Measured_RPM',\n",
       " '5_V_in',\n",
       " '5_Vibration_1',\n",
       " '5_Vibration_2',\n",
       " '5_Vibration_3',\n",
       " '5_unbalance_Factor',\n",
       " '6_Measured_RPM',\n",
       " '6_V_in',\n",
       " '6_Vibration_1',\n",
       " '6_Vibration_2',\n",
       " '6_Vibration_3',\n",
       " '6_unbalance_Factor',\n",
       " '7_Measured_RPM',\n",
       " '7_V_in',\n",
       " '7_Vibration_1',\n",
       " '7_Vibration_2',\n",
       " '7_Vibration_3',\n",
       " '7_unbalance_Factor',\n",
       " '8_Measured_RPM',\n",
       " '8_V_in',\n",
       " '8_Vibration_1',\n",
       " '8_Vibration_2',\n",
       " '8_Vibration_3',\n",
       " '8_unbalance_Factor',\n",
       " '9_Measured_RPM',\n",
       " '9_V_in',\n",
       " '9_Vibration_1',\n",
       " '9_Vibration_2',\n",
       " '9_Vibration_3',\n",
       " '9_unbalance_Factor']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the Vibration Unbalance dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names related to the 0 & 1 for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0_Measured_RPM','0_V_in','0_Vibration_1','0_Vibration_2','0_Vibration_3','0_unbalance_Factor'\n",
      "'1_Measured_RPM','1_V_in','1_Vibration_1','1_Vibration_2','1_Vibration_3','1_unbalance_Factor'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired each tag names\n",
    "tags_0 = name[:6]\n",
    "tags_1 = name[6:12]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_0 = \",\".join(f\"'{tag}'\" for tag in tags_0)\n",
    "tags_1 = \",\".join(f\"'{tag}'\" for tag in tags_1)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_0)\n",
    "print(tags_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vibration Unbalance Dataset\n",
    "* Load the data using the Tag Names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading parameter settings\n",
    "\n",
    "# Set the tag table name\n",
    "table = 'vibe_unbal'\n",
    "# Set the tag names\n",
    "name_normal = quote(tags_0, safe=\":/\")\n",
    "name_abnomal = quote(tags_1, safe=\":/\")\n",
    "# Set the time format\n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Set the data start time\n",
    "start_time = quote('2024-10-07 00:00:00')\n",
    "# Set the data end time\n",
    "end_time = quote('2024-10-07 02:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "# Preprocess for each vibration\n",
    "# Rotation speed, voltage, and unbalance factor are combined into each vibration DataFrame\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "\n",
    "    # Convert to data grouped by time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "\n",
    "    # Select rotation speed, voltage, and unbalance factor\n",
    "    df_non_vibe = df.iloc[:, [1, 2, -1]].copy()\n",
    "\n",
    "    # Convert 'TIME' column to datetime format (skip if already in datetime format)\n",
    "    df_non_vibe['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    # Set 'TIME' column as index (resample operates based on the index)\n",
    "    df_non_vibe.set_index('TIME', inplace=True)\n",
    "\n",
    "    # Resample to 1-second intervals\n",
    "    df_non_vibe = df_non_vibe.resample('1S').mean().reset_index()\n",
    "    \n",
    "    # Set up a list for vibration data \n",
    "    vibe = []\n",
    "    \n",
    "    # Process each vibration column \n",
    "    for i in range(3):\n",
    "    \n",
    "        # Separate the DataFrame for vibration data\n",
    "        df_vibe = df[df.columns[3+i:4+i]].copy()\n",
    "\n",
    "        # Set 'TIME' column\n",
    "        df_vibe['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        \n",
    "        # Group by seconds and count the number of records\n",
    "        df_counts = df_vibe.groupby(df_vibe['TIME'].dt.floor('S')).size().reset_index(name='count')\n",
    "\n",
    "        # Filter groups with the same number of records\n",
    "        # Select the most common count values\n",
    "        most_common_count = df_counts['count'].mode()[0]\n",
    "\n",
    "        # Filter by the most common count value\n",
    "        filtered_df_counts = df_counts[df_counts['count'] == most_common_count]\n",
    "\n",
    "        # Convert filtered time values to a list\n",
    "        filtered_times = filtered_df_counts['TIME'].tolist()\n",
    "\n",
    "        # Select only the filtered time values from the original DataFrame\n",
    "        filtered_data = df_vibe[df_vibe['TIME'].dt.floor('S').isin(filtered_times)]\n",
    "\n",
    "        # Group by TIME\n",
    "        # Round to the nearest second\n",
    "        filtered_data_ = filtered_data.copy()\n",
    "        filtered_data_.loc[:, 'TIME'] = filtered_data_['TIME'].dt.floor('S')\n",
    "        grouped = filtered_data_.groupby('TIME')[df.columns[3+i:4+i].item()].apply(list).reset_index()\n",
    "\n",
    "        # Split the list into individual columns\n",
    "        df_vibe_1 = pd.DataFrame(grouped[df.columns[3+i:4+i].item()].tolist())\n",
    "\n",
    "        # Merge with the 'TIME' column\n",
    "        result_df = pd.concat([grouped[['TIME']], df_non_vibe.iloc[:, 1:], df_vibe_1], axis=1)\n",
    "\n",
    "        # Remove missing values -> last line \n",
    "        result_df = result_df.dropna()\n",
    "        \n",
    "        # drop Time column\n",
    "        result_df.drop(columns=['TIME'], inplace=True)\n",
    "        \n",
    "        # Remove numbers and underscores from the beginning of the first three column names\n",
    "        columns_to_modify = result_df.columns[:3]\n",
    "        \n",
    "        # Create updated column names\n",
    "        new_columns = columns_to_modify.str.replace(r'^\\d+_', '', regex=True)\n",
    "        \n",
    "        # Update the entire column names\n",
    "        result_df.columns = new_columns.tolist() + result_df.columns[3:].tolist()\n",
    "        \n",
    "        # set label \n",
    "        result_df['label'] = (result_df['unbalance_Factor'] != 0.0).astype(int)\n",
    "        \n",
    "        # drop unbalance_Factor column\n",
    "        result_df.drop(columns=['unbalance_Factor'], inplace=True)\n",
    "        \n",
    "        # Save to the list\n",
    "        vibe.append(result_df)\n",
    "    \n",
    "    return vibe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_nomal_list = data_load(table, name_normal, start_time, end_time, timeformat)\n",
    "df_abnomal_list = data_load(table, name_abnomal, start_time, end_time, timeformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data by vibration\n",
    "# Set label based on unbalance_Factor\n",
    "df_vibe_1 = pd.concat([df_nomal_list[0], df_abnomal_list[0]], axis=0)\n",
    "df_vibe_2 = pd.concat([df_nomal_list[1], df_abnomal_list[1]], axis=0)\n",
    "df_vibe_3 = pd.concat([df_nomal_list[2], df_abnomal_list[2]], axis=0)\n",
    "\n",
    "# Randomly shuffle each DataFrame\n",
    "df_vibe_1 = df_vibe_1.sample(frac=1, random_state=77).reset_index(drop=True)\n",
    "df_vibe_2 = df_vibe_2.sample(frac=1, random_state=77).reset_index(drop=True)\n",
    "df_vibe_3 = df_vibe_3.sample(frac=1, random_state=77).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the each vibration data into train, test sets\n",
    "train_1, test_1 = train_test_split(df_vibe_1, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_1 = train_1.reset_index(drop=True)\n",
    "test_1 = test_1.reset_index(drop=True)\n",
    "\n",
    "train_2, test_2 = train_test_split(df_vibe_2, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_2 = train_2.reset_index(drop=True)\n",
    "test_2 = test_2.reset_index(drop=True)\n",
    "\n",
    "train_3, test_3 = train_test_split(df_vibe_3, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_3 = train_3.reset_index(drop=True)\n",
    "test_3 = test_3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 hanning window\n",
    "* 2 FFT\n",
    "* 3 MinMax Scaling\n",
    "* 4 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying Hanning Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanning window function setup \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Generate Hanning window\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # Apply Hanning window to each row\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling period -> Number of data points per second\n",
    "window_length = 4096\n",
    "\n",
    "# Applying Hanning Window each data\n",
    "train_1_ = set_hanning_window(window_length, train_1.iloc[:,2:-1])\n",
    "test_1_ = set_hanning_window(window_length, test_1.iloc[:,2:-1])\n",
    "\n",
    "\n",
    "train_2_ = set_hanning_window(window_length, train_2.iloc[:,2:-1])\n",
    "test_2_ = set_hanning_window(window_length, test_2.iloc[:,2:-1])\n",
    "\n",
    "\n",
    "train_3_ = set_hanning_window(window_length, train_3.iloc[:,2:-1])\n",
    "test_3_ = set_hanning_window(window_length, test_3.iloc[:,2:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying FFT (Fast Fourier Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT transformation function\n",
    "def change_fft(sample_rate, df):\n",
    "    # Total number of samples in the signal\n",
    "    N = sample_rate\n",
    "    \n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # Apply FFT to each row\n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        # Calculate FFT for each row\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # Compute the absolute value of the FFT results and normalize (only the meaningful part)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # Convert FFT results to a DataFrame\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling period -> Number of data points per second\n",
    "sampling_rate = 4096\n",
    "\n",
    "# Applying FFT(Fast Fourier Transform) each data\n",
    "train_FFT_1 = change_fft(sampling_rate, train_1_)\n",
    "test_FFT_1 = change_fft(sampling_rate, test_1_)\n",
    "\n",
    "train_FFT_2 = change_fft(sampling_rate, train_2_)\n",
    "test_FFT_2 = change_fft(sampling_rate, test_2_)\n",
    "\n",
    "train_FFT_3 = change_fft(sampling_rate, train_3_)\n",
    "test_FFT_3 = change_fft(sampling_rate, test_3_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Applying MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each Scaler Setup\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler3 = MinMaxScaler()\n",
    "\n",
    "# Apply each Scaler\n",
    "train_s1 = scaler1.fit_transform(pd.concat([train_1.iloc[:,:2], train_FFT_1], axis=1).values)\n",
    "test_s1 = scaler1.transform(pd.concat([test_1.iloc[:,:2], test_FFT_1], axis=1).values)\n",
    "\n",
    "train_s2 = scaler2.fit_transform(pd.concat([train_2.iloc[:,:2], train_FFT_2], axis=1).values)\n",
    "test_s2 = scaler2.transform(pd.concat([test_2.iloc[:,:2], test_FFT_2], axis=1).values)\n",
    "\n",
    "train_s3 = scaler3.fit_transform(pd.concat([train_3.iloc[:,:2], train_FFT_3], axis=1).values)\n",
    "test_s3 = scaler3.transform(pd.concat([test_3.iloc[:,:2], test_FFT_3], axis=1).values)\n",
    "\n",
    "# Set each DataFrames\n",
    "train_scaled_1 = pd.DataFrame(train_s1)\n",
    "test_scaled_1 = pd.DataFrame(test_s1)\n",
    "\n",
    "train_scaled_2 = pd.DataFrame(train_s2)\n",
    "test_scaled_2 = pd.DataFrame(test_s2)\n",
    "\n",
    "train_scaled_3 = pd.DataFrame(train_s3)\n",
    "test_scaled_3 = pd.DataFrame(test_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Applying PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying each PCA\n",
    "# Select principal components explaining 95% of the variance\n",
    "pca1 = PCA(n_components=0.95)\n",
    "pca2 = PCA(n_components=0.95)\n",
    "pca3 = PCA(n_components=0.95)\n",
    "\n",
    "# Apply each PCA\n",
    "train_PCA_1 = pca1.fit_transform(train_scaled_1)\n",
    "test_PCA_1 = pca1.transform(test_scaled_1)\n",
    "\n",
    "train_PCA_2 = pca2.fit_transform(train_scaled_2)\n",
    "test_PCA_2 = pca2.transform(test_scaled_2)\n",
    "\n",
    "train_PCA_3 = pca3.fit_transform(train_scaled_3)\n",
    "test_PCA_3 = pca3.transform(test_scaled_3)\n",
    "\n",
    "# Set each DataFrames\n",
    "train_PCA_1 = pd.DataFrame(train_PCA_1)\n",
    "test_PCA_1 = pd.DataFrame(test_PCA_1)\n",
    "\n",
    "train_PCA_2 = pd.DataFrame(train_PCA_2)\n",
    "test_PCA_2 = pd.DataFrame(test_PCA_2)\n",
    "\n",
    "train_PCA_3 = pd.DataFrame(train_PCA_3)\n",
    "test_PCA_3 = pd.DataFrame(test_PCA_3)\n",
    "\n",
    "# Add each labels\n",
    "train_PCA_1['label'] = train_1['label'].values\n",
    "test_PCA_1['label'] = test_1['label'].values\n",
    "\n",
    "train_PCA_2['label'] = train_2['label'].values\n",
    "test_PCA_2['label'] = test_2['label'].values\n",
    "\n",
    "train_PCA_3['label'] = train_3['label'].values\n",
    "test_PCA_3['label'] = test_3['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using XGBoost model\n",
    "* Train three XGBoost models on three vibration datasets, then ensemble the results to make a final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the each XGBoost model\n",
    "model1 = xgb.XGBClassifier()\n",
    "model2 = xgb.XGBClassifier()\n",
    "model3 = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model\n",
    "model1.fit(train_PCA_1.iloc[:,:-1].values, train_PCA_1.iloc[:,-1:].values)\n",
    "model2.fit(train_PCA_2.iloc[:,:-1].values, train_PCA_2.iloc[:,-1:].values)\n",
    "model3.fit(train_PCA_3.iloc[:,:-1].values, train_PCA_3.iloc[:,-1:].values)\n",
    "\n",
    "# Save each model\n",
    "model1.save_model(f'./result/vibe_unval_XGBoost_General_1.json')\n",
    "model2.save_model(f'./result/vibe_unval_XGBoost_General_2.json')\n",
    "model3.save_model(f'./result/vibe_unval_XGBoost_General_3.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1264\n",
      "           1       1.00      1.00      1.00      1298\n",
      "\n",
      "    accuracy                           1.00      2562\n",
      "   macro avg       1.00      1.00      1.00      2562\n",
      "weighted avg       1.00      1.00      1.00      2562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make each predictions probas\n",
    "y_pred1 = model1.predict_proba(test_PCA_1.iloc[:,:-1].values)\n",
    "y_pred2 = model2.predict_proba(test_PCA_2.iloc[:,:-1].values)\n",
    "y_pred3 = model3.predict_proba(test_PCA_3.iloc[:,:-1].values)\n",
    "\n",
    "# Average the predicted probabilities\n",
    "final_pred_probs = (y_pred1 + y_pred2 + y_pred3) / 3\n",
    "\n",
    "# Make final predictions based on the averaged probabilities\n",
    "final_predictions = final_pred_probs.argmax(axis=1)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "report = classification_report(test_PCA_3['label'].values, final_predictions)\n",
    "\n",
    "print('Ensemble Classification Report:')\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
