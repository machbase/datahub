{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotating Shaft Anomlay Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import timedelta\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xgboost as xgb\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Set path for saving model training results \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'vibe_unbal'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list\n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_Measured_RPM',\n",
       " '0_V_in',\n",
       " '0_Vibration_1',\n",
       " '0_Vibration_2',\n",
       " '0_Vibration_3',\n",
       " '0_unbalance_Factor',\n",
       " '1_Measured_RPM',\n",
       " '1_V_in',\n",
       " '1_Vibration_1',\n",
       " '1_Vibration_2',\n",
       " '1_Vibration_3',\n",
       " '1_unbalance_Factor',\n",
       " '2_Measured_RPM',\n",
       " '2_V_in',\n",
       " '2_Vibration_1',\n",
       " '2_Vibration_2',\n",
       " '2_Vibration_3',\n",
       " '2_unbalance_Factor',\n",
       " '3_Measured_RPM',\n",
       " '3_V_in',\n",
       " '3_Vibration_1',\n",
       " '3_Vibration_2',\n",
       " '3_Vibration_3',\n",
       " '3_unbalance_Factor',\n",
       " '4_Measured_RPM',\n",
       " '4_V_in',\n",
       " '4_Vibration_1',\n",
       " '4_Vibration_2',\n",
       " '4_Vibration_3',\n",
       " '4_unbalance_Factor',\n",
       " '5_Measured_RPM',\n",
       " '5_V_in',\n",
       " '5_Vibration_1',\n",
       " '5_Vibration_2',\n",
       " '5_Vibration_3',\n",
       " '5_unbalance_Factor',\n",
       " '6_Measured_RPM',\n",
       " '6_V_in',\n",
       " '6_Vibration_1',\n",
       " '6_Vibration_2',\n",
       " '6_Vibration_3',\n",
       " '6_unbalance_Factor',\n",
       " '7_Measured_RPM',\n",
       " '7_V_in',\n",
       " '7_Vibration_1',\n",
       " '7_Vibration_2',\n",
       " '7_Vibration_3',\n",
       " '7_unbalance_Factor',\n",
       " '8_Measured_RPM',\n",
       " '8_V_in',\n",
       " '8_Vibration_1',\n",
       " '8_Vibration_2',\n",
       " '8_Vibration_3',\n",
       " '8_unbalance_Factor',\n",
       " '9_Measured_RPM',\n",
       " '9_V_in',\n",
       " '9_Vibration_1',\n",
       " '9_Vibration_2',\n",
       " '9_Vibration_3',\n",
       " '9_unbalance_Factor']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the Vibration Unbalance dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names related to the 0 & 1 for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0_Measured_RPM','0_V_in','0_Vibration_1','0_Vibration_2','0_Vibration_3','0_unbalance_Factor'\n",
      "'1_Measured_RPM','1_V_in','1_Vibration_1','1_Vibration_2','1_Vibration_3','1_unbalance_Factor'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired each tag names\n",
    "tags_0 = name[:6]\n",
    "tags_1 = name[6:12]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_0 = \",\".join(f\"'{tag}'\" for tag in tags_0)\n",
    "tags_1 = \",\".join(f\"'{tag}'\" for tag in tags_1)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_0)\n",
    "print(tags_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vibration Unbalance Dataset\n",
    "* Load the data using the Tag Names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "# Preprocess for each vibration\n",
    "# Rotation speed, voltage, and unbalance factor are combined into each vibration DataFrame\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "\n",
    "    # Convert to data grouped by time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "\n",
    "    # Select rotation speed, voltage, and unbalance factor\n",
    "    df_non_vibe = df.iloc[:, [1, 2, -1]].copy()\n",
    "\n",
    "    # Convert 'TIME' column to datetime format (skip if already in datetime format)\n",
    "    df_non_vibe['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    # Set 'TIME' column as index (resample operates based on the index)\n",
    "    df_non_vibe.set_index('TIME', inplace=True)\n",
    "\n",
    "    # Resample to 1-second intervals\n",
    "    df_non_vibe = df_non_vibe.resample('1S').mean().reset_index()\n",
    "    \n",
    "    # Set up a list for vibration data \n",
    "    vibe = []\n",
    "    \n",
    "    # Process each vibration column \n",
    "    for i in range(3):\n",
    "    \n",
    "        # Separate the DataFrame for vibration data\n",
    "        df_vibe = df[df.columns[3+i:4+i]].copy()\n",
    "\n",
    "        # Set 'TIME' column\n",
    "        df_vibe['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        \n",
    "        # Group by seconds and count the number of records\n",
    "        df_counts = df_vibe.groupby(df_vibe['TIME'].dt.floor('S')).size().reset_index(name='count')\n",
    "\n",
    "        # Filter groups with the same number of records\n",
    "        # Select the most common count values\n",
    "        most_common_count = df_counts['count'].mode()[0]\n",
    "\n",
    "        # Filter by the most common count value\n",
    "        filtered_df_counts = df_counts[df_counts['count'] == most_common_count]\n",
    "\n",
    "        # Convert filtered time values to a list\n",
    "        filtered_times = filtered_df_counts['TIME'].tolist()\n",
    "\n",
    "        # Select only the filtered time values from the original DataFrame\n",
    "        filtered_data = df_vibe[df_vibe['TIME'].dt.floor('S').isin(filtered_times)]\n",
    "\n",
    "        # Group by TIME\n",
    "        # Round to the nearest second\n",
    "        filtered_data_ = filtered_data.copy()\n",
    "        filtered_data_.loc[:, 'TIME'] = filtered_data_['TIME'].dt.floor('S')\n",
    "        grouped = filtered_data_.groupby('TIME')[df.columns[3+i:4+i].item()].apply(list).reset_index()\n",
    "\n",
    "        # Split the list into individual columns\n",
    "        df_vibe_1 = pd.DataFrame(grouped[df.columns[3+i:4+i].item()].tolist())\n",
    "\n",
    "        # Merge with the 'TIME' column\n",
    "        result_df = pd.concat([grouped[['TIME']], df_non_vibe.iloc[:, 1:], df_vibe_1], axis=1)\n",
    "\n",
    "        # Remove missing values -> last line \n",
    "        result_df = result_df.dropna()\n",
    "        \n",
    "        # drop Time column\n",
    "        result_df.drop(columns=['TIME'], inplace=True)\n",
    "        \n",
    "        # Remove numbers and underscores from the beginning of the first three column names\n",
    "        columns_to_modify = result_df.columns[:3]\n",
    "        \n",
    "        # Create updated column names\n",
    "        new_columns = columns_to_modify.str.replace(r'^\\d+_', '', regex=True)\n",
    "        \n",
    "        # Update the entire column names\n",
    "        result_df.columns = new_columns.tolist() + result_df.columns[3:].tolist()\n",
    "        \n",
    "        # set label \n",
    "        result_df['label'] = (result_df['unbalance_Factor'] != 0.0).astype(int)\n",
    "        \n",
    "        # drop unbalance_Factor column\n",
    "        result_df.drop(columns=['unbalance_Factor'], inplace=True)\n",
    "        \n",
    "        # Save to the list\n",
    "        vibe.append(result_df)\n",
    "    \n",
    "    return vibe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data time loading function\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'TIME'\n",
    "    \n",
    "    # Load the data  \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # Create a dummy value column for resampling\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # Perform resampling\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'])\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.resample('1s').mean()\n",
    "    \n",
    "    # Remove missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove the dummy value column\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time update function\n",
    "# Update start and end times based on batch size\n",
    "def update_time(time_df, start_time, batch_size):\n",
    "    \n",
    "    # Calculate how many data points need to be loaded\n",
    "    time = batch_size - 1\n",
    "    \n",
    "    # Check the index number of the current time\n",
    "    # If not found, set to the first index as there is no data for the current time\n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # Set the end time for the batch data based on the current time \n",
    "    end_time_ = str(time_df.index[index_now + time] + timedelta(seconds=1))\n",
    "    \n",
    "    # Set the index number for the next start time\n",
    "    index_next = index_now + time\n",
    "    \n",
    "    # Set the next start time\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL encoding\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the maximum and minimum values for selected tag names\n",
    "def set_minmax_value(table, name, start_time_train, end_time_train):\n",
    "    \n",
    "    # URL encoding\n",
    "    start = quote(start_time_train)\n",
    "    end = quote(end_time_train)\n",
    "    \n",
    "    # Load Min, Max data\n",
    "    df_ = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-scale.tql?table={table}&name={name}&start={start}&end={end}')\n",
    "    \n",
    "    # Set Min, Max values\n",
    "    Min = df_.iloc[:,1:-1].T\n",
    "    Max = df_.iloc[:,2:].T\n",
    "    \n",
    "    return Min, Max  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 hanning window\n",
    "* 2 FFT\n",
    "* 3 MinMax Scaling -> Apply during training\n",
    "* 4 PCA -> Apply during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hanning Window Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanning window function setup \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Generate Hanning window\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # Apply Hanning window to each row\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FFT (Fast Fourier Transform) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT transformation function\n",
    "def change_fft(sample_rate, df):\n",
    "    # Total number of samples in the signal\n",
    "    N = sample_rate\n",
    "    \n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # Apply FFT to each row\n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        # Calculate FFT for each row\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # Compute the absolute value of the FFT results and normalize (only the meaningful part)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # Convert FFT results to a DataFrame\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using XGBoost model\n",
    "* Train three XGBoost models on three vibration datasets, then ensemble the results to make a final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the each XGBoost model\n",
    "model1 = xgb.XGBClassifier()\n",
    "model2 = xgb.XGBClassifier()\n",
    "model3 = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "* Training three models simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(table, name_normal, name_abnomal, timeformat, model1, model2, model3, batch_size, sample_rate, scaler1, scaler2, scaler3, pca1, pca2 , pca3, time_df_train):\n",
    "    \n",
    "    # Set initial start time\n",
    "    start_time_ = str(time_df_train.index[0])\n",
    "\n",
    "    # Set end time\n",
    "    end_time_train = str(time_df_train.index[-1])\n",
    "\n",
    "    # Initialize tqdm with the total number of iterations (or time steps)\n",
    "    total_steps = (time_df_train.index[-1] - time_df_train.index[0]).total_seconds() // batch_size\n",
    "\n",
    "    # Use 'with' statement to ensure proper closing of the tqdm object\n",
    "    with tqdm(total=int(total_steps), desc=\"Processing Data\") as progress_bar:\n",
    "        \n",
    "        # Use a while loop to call data \n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_, end_time_, next_start_time_, index_next= update_time(time_df_train, start_time_, batch_size)\n",
    "            \n",
    "            # Load batch data \n",
    "            data_normal = data_load(table, name_normal, start_time_, end_time_, timeformat)\n",
    "            data_abnomal = data_load(table, name_abnomal, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # Combine data by vibration\n",
    "            # Set label based on unbalance_Factor\n",
    "            df_vibe_1 = pd.concat([data_normal[0], data_abnomal[0]], axis=0)\n",
    "            df_vibe_2 = pd.concat([data_normal[1], data_abnomal[1]], axis=0)\n",
    "            df_vibe_3 = pd.concat([data_normal[2], data_abnomal[2]], axis=0)\n",
    "\n",
    "            # Randomly shuffle each DataFrame\n",
    "            df_vibe_1 = df_vibe_1.sample(frac=1, random_state=77).reset_index(drop=True)\n",
    "            df_vibe_2 = df_vibe_2.sample(frac=1, random_state=77).reset_index(drop=True)\n",
    "            df_vibe_3 = df_vibe_3.sample(frac=1, random_state=77).reset_index(drop=True)\n",
    "            \n",
    "            # Applying Hanning Window each data\n",
    "            train_1_ = set_hanning_window(sample_rate, df_vibe_1.iloc[:,2:-1])\n",
    "            train_2_ = set_hanning_window(sample_rate, df_vibe_2.iloc[:,2:-1])\n",
    "            train_3_ = set_hanning_window(sample_rate, df_vibe_3.iloc[:,2:-1])\n",
    "            \n",
    "            # Applying FFT(Fast Fourier Transform) each data\n",
    "            train_FFT_1 = change_fft(sample_rate, train_1_)\n",
    "            train_FFT_2 = change_fft(sample_rate, train_2_)\n",
    "            train_FFT_3 = change_fft(sample_rate, train_3_)\n",
    "            \n",
    "            # Apply each Scaler\n",
    "            train_s1 = scaler1.fit_transform(pd.concat([df_vibe_1.iloc[:,:2], train_FFT_1], axis=1).values)\n",
    "            train_s2 = scaler2.fit_transform(pd.concat([df_vibe_2.iloc[:,:2], train_FFT_2], axis=1).values)\n",
    "            train_s3 = scaler3.fit_transform(pd.concat([df_vibe_3.iloc[:,:2], train_FFT_3], axis=1).values)\n",
    "            \n",
    "            # Apply PCA\n",
    "            train_s1 = pca1.fit_transform(train_s1)\n",
    "            train_s2 = pca2.fit_transform(train_s2)\n",
    "            train_s3 = pca3.fit_transform(train_s3)\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(train_s1) == 0:\n",
    "                print(\"No data available.\")\n",
    "                \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(train_s1) == batch_size * 2:\n",
    "                \n",
    "                # Train each model\n",
    "                model1.fit(train_s1, df_vibe_1.iloc[:, -1:].values)\n",
    "                model2.fit(train_s2, df_vibe_2.iloc[:,-1:].values)\n",
    "                model3.fit(train_s3, df_vibe_3.iloc[:,-1:].values)\n",
    "                \n",
    "                # Reset batch data\n",
    "                train_s1 = 0\n",
    "                train_s2 = 0\n",
    "                train_s3 = 0\n",
    "                \n",
    "            # Set the next start time   \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "                    \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next + batch_size - 1 >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "            # Update tqdm progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Save each model\n",
    "        model1.save_model(f'./result/vibe_unval_XGBoost_New_Batch_1.json')\n",
    "        model2.save_model(f'./result/vibe_unval_XGBoost_New_Batch_2.json')\n",
    "        model3.save_model(f'./result/vibe_unval_XGBoost_New_Batch_3.json')\n",
    "        \n",
    "    return model1, model2, model3, scaler1, scaler2, scaler3, pca1, pca2 , pca3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce147261497462bb4f1db203de4a666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Data:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################### Training Parameter Settings ################################################\n",
    "# Set the tag table name\n",
    "table = 'vibe_unbal'\n",
    "# Set the tag names\n",
    "name_normal = quote(tags_0, safe=\":/\")\n",
    "name_abnomal = quote(tags_1, safe=\":/\")\n",
    "# Set the start time for the train data\n",
    "start_time_train = '2024-10-07 00:00:00'\n",
    "# Set the end time for the train data\n",
    "end_time_train = '2024-10-07 01:20:00'\n",
    "# Set time format\n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "# Set sample rate\n",
    "sample_rate = 4096\n",
    "# Set each Min-Max scaler\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler3 = MinMaxScaler()\n",
    "# Set PCA\n",
    "# Select principal components that explain 95% of the variance\n",
    "pca1 = PCA(n_components=0.95)\n",
    "pca2 = PCA(n_components=0.95)\n",
    "pca3 = PCA(n_components=0.95)\n",
    "# Load training time list \n",
    "time_df_train = time_data_load(table, name_normal, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "########################################### Proceed with training ################################################\n",
    "model1, model2, model3, scaler1, scaler2, scaler3, pca1, pca2 , pca3 = train(table, name_normal, name_abnomal, timeformat, model1, model2, model3, batch_size, sample_rate, scaler1, scaler2, scaler3, pca1, pca2 , pca3, time_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(table, name_normal, name_abnomal, timeformat, model1, model2, model3, batch_size, sample_rate, scaler1, scaler2, scaler3, pca1, pca2 , pca3, time_df_test):\n",
    "    # Set pred & label list\n",
    "    pred = []\n",
    "    label = []\n",
    "    \n",
    "    # Set the initial start time\n",
    "    start_time_t = str(time_df_test.index[0])\n",
    "\n",
    "    # Set the end time\n",
    "    end_time_test = str(time_df_test.index[-1])\n",
    "\n",
    "    # Initialize tqdm with the total number of iterations (or time steps)\n",
    "    total_steps = (time_df_test.index[-1] - time_df_test.index[0]).total_seconds() // batch_size\n",
    "\n",
    "    # Use 'with' statement to ensure proper closing of the tqdm object\n",
    "    with tqdm(total=int(total_steps), desc=\"Processing Data\") as progress_bar:\n",
    "\n",
    "        # Use a while loop to call data   \n",
    "        while start_time_t < end_time_test:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_t, end_time_t, next_start_time_t, index_next_t = update_time(time_df_test, start_time_t, batch_size)\n",
    "            \n",
    "            # Load batch data \n",
    "            data_normal = data_load(table, name_normal, start_time_t, end_time_t, timeformat)\n",
    "            data_abnomal = data_load(table, name_abnomal, start_time_t, end_time_t, timeformat)\n",
    "            \n",
    "            # Combine data by vibration\n",
    "            # Set label based on unbalance_Factor\n",
    "            df_vibe_1 = pd.concat([data_normal[0], data_abnomal[0]], axis=0).reset_index(drop=True)\n",
    "            df_vibe_2 = pd.concat([data_normal[1], data_abnomal[1]], axis=0).reset_index(drop=True)\n",
    "            df_vibe_3 = pd.concat([data_normal[2], data_abnomal[2]], axis=0).reset_index(drop=True)\n",
    "            \n",
    "            # Applying Hanning Window each data\n",
    "            test_1_ = set_hanning_window(sample_rate, df_vibe_1.iloc[:,2:-1])\n",
    "            test_2_ = set_hanning_window(sample_rate, df_vibe_2.iloc[:,2:-1])\n",
    "            test_3_ = set_hanning_window(sample_rate, df_vibe_3.iloc[:,2:-1])\n",
    "            \n",
    "            # Applying FFT(Fast Fourier Transform) each data\n",
    "            test_FFT_1 = change_fft(sample_rate, test_1_)\n",
    "            test_FFT_2 = change_fft(sample_rate, test_2_)\n",
    "            test_FFT_3 = change_fft(sample_rate, test_3_)\n",
    "            \n",
    "            # Apply each Scaler\n",
    "            test_s1 = scaler1.transform(pd.concat([df_vibe_1.iloc[:,:2], test_FFT_1], axis=1).values)\n",
    "            test_s2 = scaler2.transform(pd.concat([df_vibe_2.iloc[:,:2], test_FFT_2], axis=1).values)\n",
    "            test_s3 = scaler3.transform(pd.concat([df_vibe_3.iloc[:,:2], test_FFT_3], axis=1).values)\n",
    "            \n",
    "            # Apply PCA\n",
    "            test_s1 = pca1.transform(test_s1)\n",
    "            test_s2 = pca2.transform(test_s2)\n",
    "            test_s3 = pca3.transform(test_s3)\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(test_s1) == 0:\n",
    "                print(\"No data available.\")\n",
    "                \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(test_s1) == batch_size * 2:\n",
    "                \n",
    "                # Make each predictions probas\n",
    "                y_pred1 = model1.predict_proba(test_s1)\n",
    "                y_pred2 = model2.predict_proba(test_s2)\n",
    "                y_pred3 = model3.predict_proba(test_s3)\n",
    "                \n",
    "                # Average the predicted probabilities\n",
    "                final_pred_probs = (y_pred1 + y_pred2 + y_pred3) / 3\n",
    "\n",
    "                # Make final predictions based on the averaged probabilities\n",
    "                final_predictions = final_pred_probs.argmax(axis=1)\n",
    "                \n",
    "                pred.append(final_predictions)\n",
    "                label.append(df_vibe_1['label'].values)\n",
    "                \n",
    "                # Reset batch data\n",
    "                test_s1 = 0\n",
    "                test_s2 = 0\n",
    "                test_s3 = 0\n",
    "                    \n",
    "            # Set the next start time   \n",
    "            start_time_t = unquote(next_start_time_t)\n",
    "                        \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next_t + batch_size - 1 >= len(time_df_test):\n",
    "                break\n",
    "                \n",
    "            # Update tqdm progress bar\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "        # Concatenating by converting to NumPy arrays\n",
    "        label = np.concatenate(label)\n",
    "        pred = np.concatenate(pred)\n",
    "        \n",
    "        # Generate final results\n",
    "        final_df = pd.DataFrame(label, columns=['label'])\n",
    "        final_df['pred'] = pred\n",
    "            \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada2b0d7fbd047bdb1a09433276d3aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Data:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################### Test Parameter Settings ################################################\n",
    "\n",
    "# Set the start time for the test data\n",
    "start_time_test = '2024-10-07 01:20:00'\n",
    "# Set the end time for the test data\n",
    "end_time_test = '2024-10-07 02:00:00'\n",
    "# Load the test time list\n",
    "time_df_test = time_data_load(table, name_normal, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "######################################## Proceed with testing #############################################\n",
    "final_df = test(table, name_normal, name_abnomal, timeformat, model1, model2, model3, batch_size, sample_rate, scaler1, scaler2, scaler3, pca1, pca2 , pca3, time_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91      1632\n",
      "           1       0.87      0.98      0.92      1632\n",
      "\n",
      "    accuracy                           0.92      3264\n",
      "   macro avg       0.93      0.92      0.92      3264\n",
      "weighted avg       0.93      0.92      0.92      3264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print F1 Score based on testing data\n",
    "print(classification_report(final_df['label'], final_df['pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
