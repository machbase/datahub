{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotor condition classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results  \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'rotor'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g1_sensor1_normal',\n",
       " 'g1_sensor1_type1',\n",
       " 'g1_sensor1_type2',\n",
       " 'g1_sensor1_type3',\n",
       " 'g1_sensor2_normal',\n",
       " 'g1_sensor2_type1',\n",
       " 'g1_sensor2_type2',\n",
       " 'g1_sensor2_type3',\n",
       " 'g1_sensor3_normal',\n",
       " 'g1_sensor3_type1',\n",
       " 'g1_sensor3_type2',\n",
       " 'g1_sensor3_type3',\n",
       " 'g1_sensor4_normal',\n",
       " 'g1_sensor4_type1',\n",
       " 'g1_sensor4_type2',\n",
       " 'g1_sensor4_type3',\n",
       " 'g2_sensor1_normal',\n",
       " 'g2_sensor1_type1',\n",
       " 'g2_sensor1_type2',\n",
       " 'g2_sensor1_type3',\n",
       " 'g2_sensor2_normal',\n",
       " 'g2_sensor2_type1',\n",
       " 'g2_sensor2_type2',\n",
       " 'g2_sensor2_type3',\n",
       " 'g2_sensor3_normal',\n",
       " 'g2_sensor3_type1',\n",
       " 'g2_sensor3_type2',\n",
       " 'g2_sensor3_type3',\n",
       " 'g2_sensor4_normal',\n",
       " 'g2_sensor4_type1',\n",
       " 'g2_sensor4_type2',\n",
       " 'g2_sensor4_type3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the rotor dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names related to the g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'g1_sensor1_normal','g1_sensor1_type1','g1_sensor1_type2','g1_sensor1_type3','g1_sensor2_normal','g1_sensor2_type1','g1_sensor2_type2','g1_sensor2_type3','g1_sensor3_normal','g1_sensor3_type1','g1_sensor3_type2','g1_sensor3_type3','g1_sensor4_normal','g1_sensor4_type1','g1_sensor4_type2','g1_sensor4_type3'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = name[:16]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Rotor Dataset\n",
    "* Load the entire dataset upon data loading.\n",
    "\n",
    "    * Label description:\n",
    "\n",
    "        * normal: Normal\n",
    "        * type1: Rotational imbalance on Disk 2 (bolt and nut attached at the 270-degree position)\n",
    "        * type2: Support imbalance on Support 4\n",
    "        * type3: Combination of Type 1 and Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading parameter settings\n",
    "\n",
    "# Set the tag table name\n",
    "table = 'rotor'\n",
    "# Set the tag names\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set the time format  \n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Set the data start time\n",
    "start_time = quote('2024-01-01 00:00:00')\n",
    "# Set the data end time\n",
    "end_time = quote('2024-01-01 00:02:19.999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data  \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # Set TIME column\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    df_result = pd.DataFrame()\n",
    "    \n",
    "    # Interpolate data for each tag name\n",
    "    for i in range(len(df.columns[1:])):\n",
    "        \n",
    "        # Set time\n",
    "        start = pd.to_datetime(unquote(start_time))\n",
    "        end = pd.to_datetime(unquote(end_time))\n",
    "        \n",
    "        df_ = df.iloc[ : , [0] + list(range(i+1, i+2))].dropna()\n",
    "        \n",
    "        # Create a new time range for interpolation (1000 points for each second)\n",
    "        # In this case, the original data was measured at 1 ms intervals, so we create 1000 points at 1-second intervals\n",
    "        # Generate range from 0 to 140 -> 140,000\n",
    "        new_time_range = pd.date_range(start=start, end=end, freq='1ms')\n",
    "        new_time_range_ = pd.date_range(start=start, end=end, freq='1s')\n",
    "\n",
    "        # Use linear interpolation to fill in the data\n",
    "        # Convert datetime to numeric (epoch time in seconds)\n",
    "        time_numeric = pd.to_numeric(df_['TIME'])\n",
    "        new_time_numeric = pd.to_numeric(new_time_range)\n",
    "\n",
    "        value = df_[df_.columns[1:].values]\n",
    "\n",
    "        # Create linear interpolation object\n",
    "        interpolator = interp1d(time_numeric, value.values.reshape(-1), kind='linear', fill_value='extrapolate')\n",
    "        interpolated_values = interpolator(new_time_numeric)\n",
    "        interpolated_values = np.clip(interpolated_values, min(value.values), max(value.values))\n",
    "\n",
    "        # Create DataFrame\n",
    "        df_remake = pd.DataFrame(interpolated_values.reshape(-1,1000))\n",
    "        df_remake['time'] = new_time_range_\n",
    "        df_remake['sensor'] = f'{df_.columns[1:].item()}'\n",
    "\n",
    "        # Specify columns to move and the new order\n",
    "        cols = df_remake.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('time')))\n",
    "        cols.insert(1, cols.pop(cols.index('sensor')))\n",
    "        df_remake = df_remake[cols]\n",
    "\n",
    "        # Append to the empty DataFrame\n",
    "        df_result = pd.concat([df_result, df_remake], ignore_index=True)\n",
    "        \n",
    "    # Sort by time\n",
    "    df_result = df_result.sort_values(by='time').reset_index(drop=True)\n",
    "        \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>sensor</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor1_normal</td>\n",
       "      <td>-0.853307</td>\n",
       "      <td>-0.524641</td>\n",
       "      <td>-0.003741</td>\n",
       "      <td>-0.297684</td>\n",
       "      <td>-0.091203</td>\n",
       "      <td>-0.045372</td>\n",
       "      <td>-0.060902</td>\n",
       "      <td>0.508235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139246</td>\n",
       "      <td>0.696438</td>\n",
       "      <td>-0.508470</td>\n",
       "      <td>0.264728</td>\n",
       "      <td>-0.399669</td>\n",
       "      <td>-0.316289</td>\n",
       "      <td>-0.763595</td>\n",
       "      <td>-1.010909</td>\n",
       "      <td>-0.718536</td>\n",
       "      <td>-0.720669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor1_type2</td>\n",
       "      <td>0.555219</td>\n",
       "      <td>-0.153753</td>\n",
       "      <td>-0.197844</td>\n",
       "      <td>-0.972652</td>\n",
       "      <td>-0.913384</td>\n",
       "      <td>-1.390481</td>\n",
       "      <td>-1.414697</td>\n",
       "      <td>-1.586338</td>\n",
       "      <td>...</td>\n",
       "      <td>1.346492</td>\n",
       "      <td>1.246483</td>\n",
       "      <td>1.169959</td>\n",
       "      <td>1.310134</td>\n",
       "      <td>1.402345</td>\n",
       "      <td>1.329445</td>\n",
       "      <td>1.220657</td>\n",
       "      <td>1.077944</td>\n",
       "      <td>0.662889</td>\n",
       "      <td>0.673957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor1_type3</td>\n",
       "      <td>3.919664</td>\n",
       "      <td>3.713706</td>\n",
       "      <td>2.698885</td>\n",
       "      <td>1.338952</td>\n",
       "      <td>0.701167</td>\n",
       "      <td>-0.333580</td>\n",
       "      <td>-0.488003</td>\n",
       "      <td>-2.033326</td>\n",
       "      <td>...</td>\n",
       "      <td>6.453300</td>\n",
       "      <td>6.381317</td>\n",
       "      <td>6.292205</td>\n",
       "      <td>6.023239</td>\n",
       "      <td>5.512099</td>\n",
       "      <td>5.003446</td>\n",
       "      <td>3.997718</td>\n",
       "      <td>3.273269</td>\n",
       "      <td>1.813393</td>\n",
       "      <td>1.391992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor2_normal</td>\n",
       "      <td>0.048823</td>\n",
       "      <td>-0.029477</td>\n",
       "      <td>-0.004731</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.096184</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>...</td>\n",
       "      <td>1.224365</td>\n",
       "      <td>0.973323</td>\n",
       "      <td>0.398562</td>\n",
       "      <td>0.589916</td>\n",
       "      <td>0.520218</td>\n",
       "      <td>-0.442281</td>\n",
       "      <td>-0.729798</td>\n",
       "      <td>-1.159330</td>\n",
       "      <td>-1.270378</td>\n",
       "      <td>-1.268479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor2_type1</td>\n",
       "      <td>-1.054255</td>\n",
       "      <td>-1.173785</td>\n",
       "      <td>-1.142896</td>\n",
       "      <td>-1.357203</td>\n",
       "      <td>-0.848193</td>\n",
       "      <td>-0.319767</td>\n",
       "      <td>-1.745371</td>\n",
       "      <td>0.480831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252693</td>\n",
       "      <td>-1.117656</td>\n",
       "      <td>0.199578</td>\n",
       "      <td>-0.209522</td>\n",
       "      <td>-0.181900</td>\n",
       "      <td>-0.995153</td>\n",
       "      <td>-0.854758</td>\n",
       "      <td>0.305171</td>\n",
       "      <td>-1.650853</td>\n",
       "      <td>-0.284521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor1_type2</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>0.586425</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.480463</td>\n",
       "      <td>0.595780</td>\n",
       "      <td>0.716434</td>\n",
       "      <td>0.976936</td>\n",
       "      <td>0.963343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073815</td>\n",
       "      <td>0.539034</td>\n",
       "      <td>0.110189</td>\n",
       "      <td>-0.139597</td>\n",
       "      <td>0.531263</td>\n",
       "      <td>0.664176</td>\n",
       "      <td>1.050553</td>\n",
       "      <td>0.868198</td>\n",
       "      <td>0.792942</td>\n",
       "      <td>1.197780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor1_type1</td>\n",
       "      <td>1.904225</td>\n",
       "      <td>2.580567</td>\n",
       "      <td>2.227736</td>\n",
       "      <td>2.198483</td>\n",
       "      <td>2.072673</td>\n",
       "      <td>2.332377</td>\n",
       "      <td>2.170863</td>\n",
       "      <td>1.553462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112187</td>\n",
       "      <td>-0.015991</td>\n",
       "      <td>-0.595242</td>\n",
       "      <td>-0.885469</td>\n",
       "      <td>-0.792080</td>\n",
       "      <td>-1.389177</td>\n",
       "      <td>-1.368793</td>\n",
       "      <td>-2.281878</td>\n",
       "      <td>-1.887351</td>\n",
       "      <td>-2.443287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor1_normal</td>\n",
       "      <td>0.917832</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>0.723296</td>\n",
       "      <td>0.487312</td>\n",
       "      <td>1.017593</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>0.449253</td>\n",
       "      <td>0.847483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261695</td>\n",
       "      <td>-0.025523</td>\n",
       "      <td>-0.376261</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>-0.685341</td>\n",
       "      <td>-1.057457</td>\n",
       "      <td>-0.513635</td>\n",
       "      <td>-1.085233</td>\n",
       "      <td>-0.677111</td>\n",
       "      <td>-1.447941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor4_type2</td>\n",
       "      <td>-0.734751</td>\n",
       "      <td>-1.679474</td>\n",
       "      <td>-0.205754</td>\n",
       "      <td>-0.770393</td>\n",
       "      <td>0.273960</td>\n",
       "      <td>0.291936</td>\n",
       "      <td>0.355024</td>\n",
       "      <td>0.864662</td>\n",
       "      <td>...</td>\n",
       "      <td>2.318283</td>\n",
       "      <td>1.554217</td>\n",
       "      <td>1.711223</td>\n",
       "      <td>0.320252</td>\n",
       "      <td>0.483345</td>\n",
       "      <td>-0.423499</td>\n",
       "      <td>-1.127343</td>\n",
       "      <td>-0.805819</td>\n",
       "      <td>-1.208654</td>\n",
       "      <td>-1.151504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor4_type3</td>\n",
       "      <td>1.105399</td>\n",
       "      <td>0.855382</td>\n",
       "      <td>-0.168068</td>\n",
       "      <td>-0.113933</td>\n",
       "      <td>-0.138020</td>\n",
       "      <td>-1.075471</td>\n",
       "      <td>-0.289690</td>\n",
       "      <td>-0.019060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.325286</td>\n",
       "      <td>0.390614</td>\n",
       "      <td>0.928405</td>\n",
       "      <td>1.126668</td>\n",
       "      <td>1.435119</td>\n",
       "      <td>1.799479</td>\n",
       "      <td>1.283169</td>\n",
       "      <td>0.758537</td>\n",
       "      <td>0.934503</td>\n",
       "      <td>0.412345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time             sensor         0         1         2  \\\n",
       "0    2024-01-01 00:00:00  g1_sensor1_normal -0.853307 -0.524641 -0.003741   \n",
       "1    2024-01-01 00:00:00   g1_sensor1_type2  0.555219 -0.153753 -0.197844   \n",
       "2    2024-01-01 00:00:00   g1_sensor1_type3  3.919664  3.713706  2.698885   \n",
       "3    2024-01-01 00:00:00  g1_sensor2_normal  0.048823 -0.029477 -0.004731   \n",
       "4    2024-01-01 00:00:00   g1_sensor2_type1 -1.054255 -1.173785 -1.142896   \n",
       "...                  ...                ...       ...       ...       ...   \n",
       "2235 2024-01-01 00:02:19   g1_sensor1_type2  0.101884  0.586425  0.301300   \n",
       "2236 2024-01-01 00:02:19   g1_sensor1_type1  1.904225  2.580567  2.227736   \n",
       "2237 2024-01-01 00:02:19  g1_sensor1_normal  0.917832  0.091463  0.723296   \n",
       "2238 2024-01-01 00:02:19   g1_sensor4_type2 -0.734751 -1.679474 -0.205754   \n",
       "2239 2024-01-01 00:02:19   g1_sensor4_type3  1.105399  0.855382 -0.168068   \n",
       "\n",
       "             3         4         5         6         7  ...       990  \\\n",
       "0    -0.297684 -0.091203 -0.045372 -0.060902  0.508235  ...  0.139246   \n",
       "1    -0.972652 -0.913384 -1.390481 -1.414697 -1.586338  ...  1.346492   \n",
       "2     1.338952  0.701167 -0.333580 -0.488003 -2.033326  ...  6.453300   \n",
       "3     0.009673  0.096184  0.009673  0.009673  0.024096  ...  1.224365   \n",
       "4    -1.357203 -0.848193 -0.319767 -1.745371  0.480831  ...  0.252693   \n",
       "...        ...       ...       ...       ...       ...  ...       ...   \n",
       "2235  0.480463  0.595780  0.716434  0.976936  0.963343  ... -0.073815   \n",
       "2236  2.198483  2.072673  2.332377  2.170863  1.553462  ...  0.112187   \n",
       "2237  0.487312  1.017593  0.800933  0.449253  0.847483  ...  0.261695   \n",
       "2238 -0.770393  0.273960  0.291936  0.355024  0.864662  ...  2.318283   \n",
       "2239 -0.113933 -0.138020 -1.075471 -0.289690 -0.019060  ... -0.325286   \n",
       "\n",
       "           991       992       993       994       995       996       997  \\\n",
       "0     0.696438 -0.508470  0.264728 -0.399669 -0.316289 -0.763595 -1.010909   \n",
       "1     1.246483  1.169959  1.310134  1.402345  1.329445  1.220657  1.077944   \n",
       "2     6.381317  6.292205  6.023239  5.512099  5.003446  3.997718  3.273269   \n",
       "3     0.973323  0.398562  0.589916  0.520218 -0.442281 -0.729798 -1.159330   \n",
       "4    -1.117656  0.199578 -0.209522 -0.181900 -0.995153 -0.854758  0.305171   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2235  0.539034  0.110189 -0.139597  0.531263  0.664176  1.050553  0.868198   \n",
       "2236 -0.015991 -0.595242 -0.885469 -0.792080 -1.389177 -1.368793 -2.281878   \n",
       "2237 -0.025523 -0.376261 -0.542628 -0.685341 -1.057457 -0.513635 -1.085233   \n",
       "2238  1.554217  1.711223  0.320252  0.483345 -0.423499 -1.127343 -0.805819   \n",
       "2239  0.390614  0.928405  1.126668  1.435119  1.799479  1.283169  0.758537   \n",
       "\n",
       "           998       999  \n",
       "0    -0.718536 -0.720669  \n",
       "1     0.662889  0.673957  \n",
       "2     1.813393  1.391992  \n",
       "3    -1.270378 -1.268479  \n",
       "4    -1.650853 -0.284521  \n",
       "...        ...       ...  \n",
       "2235  0.792942  1.197780  \n",
       "2236 -1.887351 -2.443287  \n",
       "2237 -0.677111 -1.447941  \n",
       "2238 -1.208654 -1.151504  \n",
       "2239  0.934503  0.412345  \n",
       "\n",
       "[2240 rows x 1002 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = data_load(table, name, start_time, end_time, timeformat)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "\n",
    "# Label mapping dictionary\n",
    "label_mapping = {\n",
    "    'normal': 0,\n",
    "    'type1': 1,\n",
    "    'type2': 2,\n",
    "    'type3': 3\n",
    "}\n",
    "\n",
    "# Function to extract labels from column names\n",
    "def get_label(column_name):\n",
    "    column_name = str(column_name)\n",
    "    for key in label_mapping.keys():\n",
    "        if key in column_name:\n",
    "            return label_mapping[key]\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    560\n",
      "2    560\n",
      "3    560\n",
      "1    560\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply labels to each column and create a new series\n",
    "labels = pd.Series(df['sensor']).map(get_label)\n",
    "\n",
    "# Add the label to the DataFrame\n",
    "df['label'] = labels.values\n",
    "\n",
    "# Print the count of each label\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    400\n",
      "2    400\n",
      "3    400\n",
      "1    400\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    80\n",
      "2    80\n",
      "3    80\n",
      "1    80\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    80\n",
      "2    80\n",
      "3    80\n",
      "1    80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "\n",
    "# Extract data for each sensor\n",
    "# 100 samples for training\n",
    "# 20 samples for validation\n",
    "# 20 samples for testing\n",
    "train = pd.DataFrame()\n",
    "valid = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df['sensor'].value_counts().index)):\n",
    "    \n",
    "    df_train = df[df['sensor'] == df['sensor'].value_counts().index[i]][:100].iloc[:,2:]\n",
    "    df_valid = df[df['sensor'] == df['sensor'].value_counts().index[i]][100:120].iloc[:,2:]\n",
    "    df_test = df[df['sensor'] == df['sensor'].value_counts().index[i]][120:].iloc[:,2:]\n",
    "    \n",
    "    train = pd.concat([train, df_train])\n",
    "    valid = pd.concat([valid, df_valid])\n",
    "    test = pd.concat([test, df_test])\n",
    "    \n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "print(train['label'].value_counts())\n",
    "print(valid['label'].value_counts())\n",
    "print(test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 hanning window\n",
    "* 2 FFT\n",
    "* 3 MinMax Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying Hanning Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanning window function setup \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Generate Hanning window\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # Apply Hanning window to each row\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling period -> Number of data points per second\n",
    "window_length = len(df.columns[2:-1])\n",
    "\n",
    "# Applying Hanning Window\n",
    "train_ = set_hanning_window(window_length, train.iloc[:,:-1])\n",
    "valid_ = set_hanning_window(window_length, valid.iloc[:,:-1])\n",
    "test_ = set_hanning_window(window_length, test.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying FFT (Fast Fourier Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT transformation function\n",
    "def change_fft(sample_rate, df):\n",
    "    # Total number of samples in the signal\n",
    "    N = sample_rate\n",
    "    \n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # Apply FFT to each row\n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        # Calculate FFT for each row\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # Compute the absolute value of the FFT results and normalize (only the meaningful part)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # Convert FFT results to a DataFrame\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling period -> Number of data points per second\n",
    "sampling_rate = len(df.columns[2:-1])\n",
    "\n",
    "# Applying FFT (Fast Fourier Transform)\n",
    "train_ = change_fft(sampling_rate, train_)\n",
    "valid_ = change_fft(sampling_rate, valid_)\n",
    "test_ = change_fft(sampling_rate, test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Applying MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    400\n",
      "2    400\n",
      "3    400\n",
      "1    400\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    80\n",
      "2    80\n",
      "3    80\n",
      "1    80\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    80\n",
      "2    80\n",
      "3    80\n",
      "1    80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Scaler Setup\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Scaler\n",
    "train_ = scaler.fit_transform(train_.values)\n",
    "valid_ = scaler.transform(valid_.values)\n",
    "test_ = scaler.transform(test_.values)\n",
    "\n",
    "# Set DataFrames\n",
    "train_scaled = pd.DataFrame(train_)\n",
    "valid_scaled = pd.DataFrame(valid_)\n",
    "test_scaled = pd.DataFrame(test_)\n",
    "\n",
    "# Add label\n",
    "train_scaled['label'] = train['label'].values\n",
    "valid_scaled['label'] = valid['label'].values\n",
    "test_scaled['label'] = test['label'].values\n",
    "\n",
    "print(train_scaled['label'].value_counts())\n",
    "print(valid_scaled['label'].value_counts())\n",
    "print(test_scaled['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Loader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotor_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.freq_data = df.iloc[:,:-1]\n",
    "        self.label = df.iloc[:,-1:].squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.freq_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        input_time_data = self.freq_data.iloc[index,:]\n",
    "        input_time_data = torch.Tensor(input_time_data).expand(1, input_time_data.shape[0])\n",
    "        label = self.label[index]\n",
    "\n",
    "        return input_time_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up datasets  \n",
    "train_ = Rotor_Dataset(train_scaled)\n",
    "valid_ = Rotor_Dataset(valid_scaled)\n",
    "test_ = Rotor_Dataset(test_scaled)\n",
    "\n",
    "# Set up data loaders\n",
    "train_dataloader = DataLoader(train_, batch_size=8, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 501])\n"
     ]
    }
   ],
   "source": [
    "# Verify DataLoader application and check the shape of the input data\n",
    "print(list(train_dataloader)[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using ResNet1d model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet 1D Model Setup\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=4):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet1D(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = ResNet1D(ResidualBlock, [2, 2, 2, 2], num_classes=4).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "* Save the model with the Best F1 Score based on the validation data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e05cf52a0854900ab76b2ce3f2953c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.3326324446499347, train acc: 45.3125\n",
      "\n",
      "train loss: 1.0101663764566182, train acc: 70.8750\n",
      "\n",
      "train loss: 0.8098243155765036, train acc: 83.9375\n",
      "\n",
      "train loss: 0.6831355629535392, train acc: 88.5000\n",
      "\n",
      "train loss: 0.5867879580068401, train acc: 92.5000\n",
      "\n",
      "train loss: 0.5234523606393485, train acc: 92.5000\n",
      "\n",
      "train loss: 0.47297797634657135, train acc: 94.5625\n",
      "\n",
      "train loss: 0.4280194197935634, train acc: 96.8125\n",
      "\n",
      "train loss: 0.3937211614839422, train acc: 95.7500\n",
      "\n",
      "train loss: 0.36322635618166527, train acc: 96.3125\n",
      "\n",
      "train loss: 0.3389954990106211, train acc: 96.6875\n",
      "\n",
      "train loss: 0.3187843384031899, train acc: 96.3750\n",
      "\n",
      "train loss: 0.29943007610246863, train acc: 97.6875\n",
      "\n",
      "train loss: 0.28015993621888413, train acc: 99.1875\n",
      "\n",
      "train loss: 0.2678981447784741, train acc: 96.5625\n",
      "\n",
      "train loss: 0.2576470710222463, train acc: 96.6875\n",
      "\n",
      "train loss: 0.2463012431532875, train acc: 97.9375\n",
      "\n",
      "train loss: 0.2344595020104508, train acc: 99.2500\n",
      "\n",
      "train loss: 0.22256130556602183, train acc: 99.8125\n",
      "\n",
      "train loss: 0.21515888657551474, train acc: 97.4375\n",
      "\n",
      "train loss: 0.20644419775825637, train acc: 98.9375\n",
      "\n",
      "train loss: 0.20044713819881957, train acc: 97.8750\n",
      "\n",
      "train loss: 0.19550921721127015, train acc: 96.9375\n",
      "\n",
      "train loss: 0.1887698637945122, train acc: 98.6250\n",
      "\n",
      "train loss: 0.18308005004652259, train acc: 98.6875\n",
      "\n",
      "train loss: 0.1773144124075293, train acc: 99.0625\n",
      "\n",
      "train loss: 0.17269909902599448, train acc: 98.3125\n",
      "\n",
      "train loss: 0.1675385753721865, train acc: 99.2500\n",
      "\n",
      "train loss: 0.16216506369108347, train acc: 99.4375\n",
      "\n",
      "train loss: 0.1569012935180537, train acc: 99.8750\n",
      "\n",
      "train loss: 0.15448529678046272, train acc: 97.5000\n",
      "\n",
      "train loss: 0.15054135976583877, train acc: 99.1250\n",
      "\n",
      "train loss: 0.1465015008832827, train acc: 99.5000\n",
      "\n",
      "train loss: 0.14412656444096883, train acc: 97.9375\n",
      "\n",
      "train loss: 0.1412007258397486, train acc: 98.7500\n",
      "\n",
      "train loss: 0.13767405397045837, train acc: 99.5625\n",
      "\n",
      "train loss: 0.13478524734107972, train acc: 99.3125\n",
      "\n",
      "train loss: 0.13210694841921033, train acc: 98.8125\n",
      "\n",
      "train loss: 0.12970249534694303, train acc: 98.9375\n",
      "\n",
      "train loss: 0.12761608614986591, train acc: 98.4375\n",
      "\n",
      "train loss: 0.12553220636592025, train acc: 99.0000\n",
      "\n",
      "train loss: 0.12295640746094987, train acc: 99.5625\n",
      "\n",
      "train loss: 0.1209746297260204, train acc: 99.1875\n",
      "\n",
      "train loss: 0.11846116934373607, train acc: 99.7500\n",
      "\n",
      "train loss: 0.11652999250763976, train acc: 99.2500\n",
      "\n",
      "train loss: 0.11427853674909555, train acc: 99.5000\n",
      "\n",
      "train loss: 0.11201677122371331, train acc: 99.8125\n",
      "\n",
      "train loss: 0.11038771432754342, train acc: 99.0625\n",
      "\n",
      "train loss: 0.10819027598799923, train acc: 99.8750\n",
      "\n",
      "train loss: 0.10604242628043657, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1039842452853781, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10215550974658798, train acc: 99.8750\n",
      "\n",
      "train loss: 0.10305864136927317, train acc: 95.5000\n",
      "\n",
      "train loss: 0.10160830906896569, train acc: 99.4375\n",
      "\n",
      "train loss: 0.09983995060670102, train acc: 99.8750\n",
      "\n",
      "train loss: 0.09819621878770628, train acc: 99.6875\n",
      "\n",
      "train loss: 0.09667584505923349, train acc: 99.6250\n",
      "\n",
      "train loss: 0.09538541361949461, train acc: 99.2500\n",
      "\n",
      "train loss: 0.09397849023239825, train acc: 99.5000\n",
      "\n",
      "train loss: 0.09259664752398465, train acc: 99.7500\n",
      "\n",
      "train loss: 0.09125224948849107, train acc: 99.7500\n",
      "\n",
      "train loss: 0.09032689950849994, train acc: 99.3125\n",
      "\n",
      "train loss: 0.08980891686848176, train acc: 98.3125\n",
      "\n",
      "train loss: 0.08849441164436818, train acc: 99.8125\n",
      "\n",
      "train loss: 0.08726230901687296, train acc: 99.6250\n",
      "\n",
      "train loss: 0.08626441707039065, train acc: 99.5000\n",
      "\n",
      "train loss: 0.08501887106782531, train acc: 99.9375\n",
      "\n",
      "train loss: 0.08378863867750538, train acc: 99.9375\n",
      "\n",
      "train loss: 0.08261145377845537, train acc: 99.9375\n",
      "\n",
      "train loss: 0.08197530992701996, train acc: 98.8750\n",
      "\n",
      "train loss: 0.08151401837311162, train acc: 98.4375\n",
      "\n",
      "train loss: 0.08083375314932138, train acc: 98.9375\n",
      "\n",
      "train loss: 0.07982028537431822, train acc: 99.8750\n",
      "\n",
      "train loss: 0.07895952577027875, train acc: 99.8125\n",
      "\n",
      "train loss: 0.0779125923578425, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07700184959941335, train acc: 99.8125\n",
      "\n",
      "train loss: 0.07602597134345927, train acc: 99.9375\n",
      "\n",
      "train loss: 0.075053844403779, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07410785510920273, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07318831992494296, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07228685145321356, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07140595925073565, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07054738863897275, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07055482909041173, train acc: 97.9375\n",
      "\n",
      "train loss: 0.07030029842981582, train acc: 98.3750\n",
      "\n",
      "train loss: 0.06997930948843167, train acc: 98.8125\n",
      "\n",
      "train loss: 0.06918447324549817, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06848920755772343, train acc: 99.7500\n",
      "\n",
      "train loss: 0.06772371177223739, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06702271985592982, train acc: 99.8750\n",
      "\n",
      "train loss: 0.06629049242822176, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06557224237477623, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06486786930997242, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06421930427875987, train acc: 99.8750\n",
      "\n",
      "train loss: 0.06375940503217628, train acc: 99.3125\n",
      "\n",
      "train loss: 0.06376600437328876, train acc: 98.3125\n",
      "\n",
      "train loss: 0.06324163934601003, train acc: 99.6250\n",
      "\n",
      "train loss: 0.06260663982665038, train acc: 100.0000\n",
      "\n",
      "train loss: 0.0619846550444464, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06144306734678528, train acc: 99.6875\n"
     ]
    }
   ],
   "source": [
    "# Initialize training loss\n",
    "train_loss = []\n",
    "# Initialize training accuracy\n",
    "train_acc = []\n",
    "# Initialize total step\n",
    "total_step = len(train_dataloader)\n",
    "# Set number of epochs\n",
    "epoch_in = trange(100, desc='training')\n",
    "# Initialize best F1 Score value\n",
    "best_f1= 0\n",
    "\n",
    "# Start model training\n",
    "for epoch in epoch_in:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "\n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = train_data[0].to(device).float()\n",
    "        labels = train_data[1].to(device).long().squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Input to the model\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Set label predictions \n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==labels).item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "    \n",
    "    # Perform validation at the end of each epoch and save the model with the best performance\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "            inputs_v = valid_data[0].to(device).float()\n",
    "            labels_v = valid_data[1].to(device).long().squeeze() \n",
    "            \n",
    "            outputs_v = model(inputs_v)\n",
    "            \n",
    "            # Set label predictions\n",
    "            _,pred_v = torch.max(outputs_v, dim=1)\n",
    "            target_v = labels_v.view_as(pred_v)\n",
    "            \n",
    "            preds_.append(pred_v)\n",
    "            targets_.append(target_v)\n",
    "            \n",
    "        # Combine predictions and labels collected from all batches\n",
    "        preds_ = torch.cat(preds_).detach().cpu().numpy()\n",
    "        targets_ = torch.cat(targets_).detach().cpu().numpy()\n",
    "        \n",
    "        f1score = f1_score(targets_, preds_,  average='macro')\n",
    "        if best_f1 < f1score:\n",
    "            best_f1 = f1score\n",
    "            # Save the best model \n",
    "            with open(\"./result/Rotor_1d_ResNet_General.txt\", \"a\") as text_file:\n",
    "                print('epoch=====',epoch, file=text_file)\n",
    "                print(classification_report(targets_, preds_, digits=4), file=text_file)\n",
    "            torch.save(model, f'./result/Rotor_1d_ResNet_General.pt') \n",
    "        epoch_in.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_ = torch.load(f'./result/Rotor_1d_ResNet_General.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "# Model testing\n",
    "preds_test = []\n",
    "target_test = []\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    for batch_idx, test_data in enumerate(test_dataloader):\n",
    "        inputs_t = test_data[0].to(device).float()\n",
    "        labels_t =  test_data[1].to(device).long().squeeze() \n",
    "        \n",
    "        outputs_t = model_(inputs_t)\n",
    "        \n",
    "        _,pred_t = torch.max(outputs_t, dim=1)\n",
    "        targets_t = labels_t.view_as(pred_t).to(device)\n",
    "\n",
    "        preds_test.append(pred_t)\n",
    "        target_test.append(targets_t)\n",
    "        \n",
    "    # Combine predictions and labels collected from all batches\n",
    "    preds_test = torch.cat(preds_test).detach().cpu().numpy()\n",
    "    target_test = torch.cat(target_test).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94        80\n",
      "           1       0.96      0.93      0.94        80\n",
      "           2       1.00      1.00      1.00        80\n",
      "           3       1.00      1.00      1.00        80\n",
      "\n",
      "    accuracy                           0.97       320\n",
      "   macro avg       0.97      0.97      0.97       320\n",
      "weighted avg       0.97      0.97      0.97       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
