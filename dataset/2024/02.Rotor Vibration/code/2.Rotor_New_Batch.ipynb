{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results  \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'rotor'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list  \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g1_sensor1_normal',\n",
       " 'g1_sensor1_type1',\n",
       " 'g1_sensor1_type2',\n",
       " 'g1_sensor1_type3',\n",
       " 'g1_sensor2_normal',\n",
       " 'g1_sensor2_type1',\n",
       " 'g1_sensor2_type2',\n",
       " 'g1_sensor2_type3',\n",
       " 'g1_sensor3_normal',\n",
       " 'g1_sensor3_type1',\n",
       " 'g1_sensor3_type2',\n",
       " 'g1_sensor3_type3',\n",
       " 'g1_sensor4_normal',\n",
       " 'g1_sensor4_type1',\n",
       " 'g1_sensor4_type2',\n",
       " 'g1_sensor4_type3',\n",
       " 'g2_sensor1_normal',\n",
       " 'g2_sensor1_type1',\n",
       " 'g2_sensor1_type2',\n",
       " 'g2_sensor1_type3',\n",
       " 'g2_sensor2_normal',\n",
       " 'g2_sensor2_type1',\n",
       " 'g2_sensor2_type2',\n",
       " 'g2_sensor2_type3',\n",
       " 'g2_sensor3_normal',\n",
       " 'g2_sensor3_type1',\n",
       " 'g2_sensor3_type2',\n",
       " 'g2_sensor3_type3',\n",
       " 'g2_sensor4_normal',\n",
       " 'g2_sensor4_type1',\n",
       " 'g2_sensor4_type2',\n",
       " 'g2_sensor4_type3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the rotor dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names related to the g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'g1_sensor1_normal','g1_sensor1_type1','g1_sensor1_type2','g1_sensor1_type3','g1_sensor2_normal','g1_sensor2_type1','g1_sensor2_type2','g1_sensor2_type3','g1_sensor3_normal','g1_sensor3_type1','g1_sensor3_type2','g1_sensor3_type3','g1_sensor4_normal','g1_sensor4_type1','g1_sensor4_type2','g1_sensor4_type3'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = name[:16]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Rotor Dataset\n",
    "* Load the entire dataset upon data loading.\n",
    "\n",
    "    * Label description:\n",
    "\n",
    "        * normal: Normal\n",
    "        * type1: Rotational imbalance on Disk 2 (bolt and nut attached at the 270-degree position)\n",
    "        * type2: Support imbalance on Support 4\n",
    "        * type3: Combination of Type 1 and Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "\n",
    "# Label mapping dictionary\n",
    "label_mapping = {\n",
    "    'normal': 0,\n",
    "    'type1': 1,\n",
    "    'type2': 2,\n",
    "    'type3': 3\n",
    "}\n",
    "\n",
    "# Function to extract labels from column names\n",
    "def get_label(column_name):\n",
    "    column_name = str(column_name)\n",
    "    for key in label_mapping.keys():\n",
    "        if key in column_name:\n",
    "            return label_mapping[key]\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # Load data  \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # Set TIME column\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Create an empty DataFrame\n",
    "    data_result = pd.DataFrame()\n",
    "\n",
    "    # Interpolate data for each tag name\n",
    "    for i in range(len(df.columns[1:])):\n",
    "        \n",
    "        # Set time\n",
    "        start = pd.to_datetime(unquote(start_time))\n",
    "        end = pd.to_datetime(unquote(end_time))\n",
    "        \n",
    "        data_ = df.iloc[ : , [0] + list(range(i+1, i+2))].dropna()\n",
    "        \n",
    "        # Create a new time range for interpolation (1000 points for each second)\n",
    "        # In this case, the original data was measured at 1 ms intervals, so we create 1000 points at 1-second intervals\n",
    "        # Generate range from 0 to 140 -> 140,000\n",
    "        new_time_range = pd.date_range(start=start, end=end, freq='1ms')[:-1]\n",
    "        new_time_range_ = pd.date_range(start=start, end=end, freq='1s')[:-1]\n",
    "\n",
    "        # Use linear interpolation to fill in the data\n",
    "        # Convert datetime to numeric (epoch time in seconds)\n",
    "        time_numeric = pd.to_numeric(data_['TIME'])\n",
    "        new_time_numeric = pd.to_numeric(new_time_range)\n",
    "\n",
    "        value = data_[data_.columns[1:].values]\n",
    "\n",
    "        # Create linear interpolation object\n",
    "        interpolator = interp1d(time_numeric, value.values.reshape(-1), kind='linear', fill_value='extrapolate')\n",
    "        interpolated_values = interpolator(new_time_numeric)\n",
    "        interpolated_values = np.clip(interpolated_values, min(value.values), max(value.values))\n",
    "\n",
    "        # Create DataFrame\n",
    "        data_remake = pd.DataFrame(interpolated_values.reshape(-1,1000))\n",
    "        data_remake['time'] = new_time_range_\n",
    "        data_remake['sensor'] = f'{data_.columns[1:].item()}'\n",
    "\n",
    "        # Specify columns to move and the new order\n",
    "        cols = data_remake.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('time')))\n",
    "        cols.insert(1, cols.pop(cols.index('sensor')))\n",
    "        data_remake = data_remake[cols]\n",
    "\n",
    "        # Append to the empty DataFrame\n",
    "        data_result = pd.concat([data_result, data_remake], ignore_index=True)\n",
    "        \n",
    "    # Sort by time\n",
    "    data_result = data_result.sort_values(by='time').reset_index(drop=True)\n",
    "    \n",
    "    # Apply labels to each column and create a new series\n",
    "    labels = pd.Series(data_result['sensor']).map(get_label)\n",
    "\n",
    "    # Add the label to the DataFrame\n",
    "    data_result['label'] = labels.values\n",
    "\n",
    "    # Print the count of each label\n",
    "    data_result = data_result.iloc[:,2:]\n",
    "        \n",
    "    return data_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data time loading function\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'time'\n",
    "    \n",
    "    # Load the data  \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # Create a dummy value column for resampling\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # Perform resampling\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df = df.resample('1s').mean()\n",
    "    \n",
    "    # Remove missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove the dummy value column\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time update function\n",
    "# Update start and end times based on batch size\n",
    "def update_time(time_df, start_time, batch_size):\n",
    "    \n",
    "    # Calculate how many data points need to be loaded\n",
    "    time = int(batch_size / 16)\n",
    "    \n",
    "    # Check the index number of the current time\n",
    "    # If not found, set to the first index as there is no data for the current time\n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # Set the end time for the batch data based on the current time \n",
    "    end_time_ = str(time_df.index[index_now + time])\n",
    "    \n",
    "    # Set the index number for the next start time\n",
    "    index_next = index_now + time\n",
    "    \n",
    "    # Set the next start time\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL encoding\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "   * 1 Hanning Window\n",
    "   * 2 FFT \n",
    "   * 3 MinMaxScaling -> Apply during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying Hanning Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanning window function setup \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Generate Hanning window\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # Apply Hanning window to each row\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying FFT (Fast Fourier Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT transformation function\n",
    "def change_fft(sample_rate, df):\n",
    "    # Total number of samples in the signal\n",
    "    N = sample_rate\n",
    "    \n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # Apply FFT to each row\n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        # Calculate FFT for each row\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # Compute the absolute value of the FFT results and normalize (only the meaningful part)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # Convert FFT results to a DataFrame\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using ResNet1d model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet 1D Model Setup\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=4):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet1D(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = ResNet1D(ResidualBlock, [2, 2, 2, 2], num_classes=4).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "* Save the model with the Best F1 Score based on the validation data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train(table, name, timeformat, model, batch_size, epochs, scaler, time_df_train, time_df_valid, sample_rate):\n",
    "    \n",
    "    # Initialize training loss\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Initialize best F1 Score value\n",
    "    best_f1= -np.inf\n",
    "\n",
    "    # Start model training\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        correct = 0\n",
    "        total=0\n",
    "\n",
    "        # Set initial start time\n",
    "        start_time_ = str(time_df_train.index[0])\n",
    "\n",
    "        # Set end time\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "\n",
    "        # Use a while loop to call data\n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size \n",
    "            start_time_, end_time_, next_start_time_, index_next= update_time(time_df_train, start_time_, batch_size)\n",
    "            \n",
    "            # Load batch data \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # Apply Hanning window\n",
    "            data_ = set_hanning_window(sample_rate, data.iloc[:,:-1])\n",
    "            \n",
    "            # Apply FFT\n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # Apply MinMax scaler\n",
    "            data_ = scaler.fit_transform(data_)\n",
    "            \n",
    "            # Setting up DataFrame + label\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data['label'].values\n",
    "            \n",
    "            # Data Random Shuffle\n",
    "            data_ = data_.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(data_) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(data_) == batch_size:\n",
    "                \n",
    "                # Check total batch count  \n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data = np.array(data_.iloc[:,:-1]).reshape(batch_size, 1 , -1)\n",
    "                label = np.array(data_.iloc[:,-1:])\n",
    "\n",
    "                # Convert data to Tensor\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "                label = torch.tensor(label).to(device).long().squeeze()\n",
    "\n",
    "                # Optimize the optimizer\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Input to the model\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Set label predictions \n",
    "                _,pred = torch.max(outputs, dim=1)\n",
    "                correct += torch.sum(pred==label).item()\n",
    "                total += label.size(0)\n",
    "                \n",
    "                # Reset batch data\n",
    "                data_ = 0\n",
    "                \n",
    "            # Set the next start time     \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "                \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next + (int(batch_size /16)) >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "        train_acc.append(100 * correct / total)\n",
    "        train_loss.append(running_loss/total_step)\n",
    "        print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "\n",
    "        # Perform validation at the end of each epoch and save the model with the best performance\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            preds_ = []\n",
    "            targets_ = []\n",
    "                \n",
    "            # Set initial start time\n",
    "            start_time_v = str(time_df_valid.index[0])\n",
    "            \n",
    "            # Set end time\n",
    "            end_time_valid = str(time_df_valid.index[-1])\n",
    "            \n",
    "            # Use a while loop to call data \n",
    "            while start_time_v < end_time_valid:\n",
    "                \n",
    "                # Set the time for loading data based on the batch size\n",
    "                start_time_v, end_time_v, next_start_time_v, index_next_v = update_time(time_df_valid, start_time_v, batch_size)\n",
    "                \n",
    "                # Load batch data \n",
    "                data_v = data_load(table, name, start_time_v, end_time_v, timeformat)\n",
    "                \n",
    "                # Apply Hanning window\n",
    "                data_ = set_hanning_window(sample_rate, data_v.iloc[:,:-1])\n",
    "                \n",
    "                # Apply FFT \n",
    "                data_  = change_fft(sample_rate, data_ )\n",
    "                \n",
    "                # Apply MinMax scaler \n",
    "                data_ = scaler.fit_transform(data_)\n",
    "                \n",
    "                # Setting up DataFrame + label\n",
    "                data_ = pd.DataFrame(data_)\n",
    "                data_['label'] = data_v['label'].values\n",
    "                \n",
    "                # Data Random Shuffle\n",
    "                data_ = data_.sample(frac=1).reset_index(drop=True)\n",
    "                \n",
    "                # Print if the loaded data is empty\n",
    "                if len(data_) == 0:\n",
    "                    print(\"No data available.\")\n",
    "                \n",
    "                # Input the data into the model when it accumulates to the batch size\n",
    "                if len(data_) == batch_size:\n",
    "                    \n",
    "                    # Convert data to numpy arrays\n",
    "                    input_data_v = np.array(data_.iloc[:,:-1]).reshape(batch_size, 1 , -1)\n",
    "                    label_v = np.array(data_.iloc[:,-1:])\n",
    "\n",
    "                    # Convert data to Tensor\n",
    "                    input_data_v = torch.tensor(input_data_v, dtype=torch.float32).to(device).float()\n",
    "                    label_v = torch.tensor(label_v).to(device).long().squeeze()\n",
    "                    \n",
    "                    # Input to the model\n",
    "                    outputs_v = model(input_data_v)\n",
    "                    \n",
    "                    # Set label predictions \n",
    "                    _,pred_v = torch.max(outputs_v, dim=1)\n",
    "                    target_v = label_v.view_as(pred_v)\n",
    "      \n",
    "                    preds_.append(pred_v)\n",
    "                    targets_.append(target_v)\n",
    "                    \n",
    "                    # Reset batch data\n",
    "                    data_ = 0\n",
    "                    \n",
    "                # Set the next start time    \n",
    "                start_time_v = unquote(next_start_time_v)\n",
    "                \n",
    "                # Prevent fetching beyond the last time\n",
    "                if index_next_v + (int(batch_size /16)) >= len(time_df_valid):\n",
    "                    break\n",
    "                    \n",
    "            # Combine predictions and labels collected from all batches\n",
    "            preds_v = torch.cat(preds_).detach().cpu().numpy()\n",
    "            targets_v = torch.cat(targets_).detach().cpu().numpy()\n",
    "            \n",
    "            f1score = f1_score(targets_v, preds_v,  average='macro')\n",
    "            if best_f1 < f1score:\n",
    "                best_f1 = f1score\n",
    "                # Save the best model \n",
    "                with open(\"./result/rotor_1d_ResNet_New_Batch.txt\", \"a\") as text_file:\n",
    "                    print('epoch=====',epoch, file=text_file)\n",
    "                    print(classification_report(targets_v, preds_v, digits=4), file=text_file)\n",
    "                torch.save(model, f'./result/rotor_1d_ResNet_New_Batch.pt') \n",
    "            epochs.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")\n",
    "               \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d6a05a97f34d34a6dcd35e51c9ce70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.202284309328819, train acc: 48.8520\n",
      "\n",
      "train loss: 1.0962372036004553, train acc: 55.1020\n",
      "\n",
      "train loss: 0.9918817787551556, train acc: 65.5612\n",
      "\n",
      "train loss: 0.9008833318188483, train acc: 73.2143\n",
      "\n",
      "train loss: 0.8220373034781339, train acc: 79.7194\n",
      "\n",
      "train loss: 0.746996943401743, train acc: 85.3954\n",
      "\n",
      "train loss: 0.6775784237614396, train acc: 89.6046\n",
      "\n",
      "train loss: 0.619259966180983, train acc: 92.4745\n",
      "\n",
      "train loss: 0.5672647098919837, train acc: 94.6429\n",
      "\n",
      "train loss: 0.524029608508477, train acc: 94.7066\n",
      "\n",
      "train loss: 0.48373553668216207, train acc: 97.1301\n",
      "\n",
      "train loss: 0.44921779448016425, train acc: 97.2577\n",
      "\n",
      "train loss: 0.4222075491724207, train acc: 96.6199\n",
      "\n",
      "train loss: 0.3988481249123827, train acc: 96.6199\n",
      "\n",
      "train loss: 0.37548347991300424, train acc: 98.4056\n",
      "\n",
      "train loss: 0.3551015420195319, train acc: 98.5332\n",
      "\n",
      "train loss: 0.3373091416777511, train acc: 98.3418\n",
      "\n",
      "train loss: 0.32364740021720145, train acc: 96.1735\n",
      "\n",
      "train loss: 0.3100498602162167, train acc: 97.6403\n",
      "\n",
      "train loss: 0.29563611278142476, train acc: 99.1709\n",
      "\n",
      "train loss: 0.28176079063845655, train acc: 99.8724\n",
      "\n",
      "train loss: 0.26899269525835073, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2573087403235604, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2465961661463986, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2367392576788923, train acc: 100.0000\n",
      "\n",
      "train loss: 0.22763965415845583, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2192134108981079, train acc: 100.0000\n",
      "\n",
      "train loss: 0.21138851653773574, train acc: 100.0000\n",
      "\n",
      "train loss: 0.20410285053898233, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1973025537491374, train acc: 100.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet1D(\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### Training Parameter Settings ################################################\n",
    "# Set tag table name\n",
    "table = 'rotor'\n",
    "# Set tag name\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set the start time for the train data\n",
    "start_time_train = '2024-01-01 00:00:00'\n",
    "# Set the end time for the train data\n",
    "end_time_train = '2024-01-01 00:01:39'\n",
    "# Set time format\n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Set batch size\n",
    "# Fix the batch size to 16 for this rotor data\n",
    "batch_size = 16\n",
    "# Set number of epochs\n",
    "epochs = trange(30, desc='training')\n",
    "# Set sample rate\n",
    "sample_rate = 1000\n",
    "# Set Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# Load training time list \n",
    "time_df_train = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "########################################### validation Parameter Settings ################################################\n",
    "# Set the start time for the validation data\n",
    "start_time_valid = '2024-01-01 00:01:39'\n",
    "# Set the end time for the validation data\n",
    "end_time_valid = '2024-01-01 00:02:00'\n",
    "# Load validation time list\n",
    "time_df_valid = time_data_load(table, name, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "\n",
    "########################################### Proceed with training ################################################\n",
    "train(table, name, timeformat, model, batch_size, epochs, scaler, time_df_train, time_df_valid, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing function\n",
    "def test(table, name, timeformat, model, batch_size, sample_rate, scaler, time_df_test):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Initial settings\n",
    "        preds_test = []\n",
    "        target_test = []\n",
    "        \n",
    "        # Set the initial start time\n",
    "        start_time_t = str(time_df_test.index[0])\n",
    "        \n",
    "        # Set the end time\n",
    "        end_time_test = str(time_df_test.index[-1])\n",
    "        \n",
    "        # Use a while loop to call data  \n",
    "        while start_time_t < end_time_test:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size\n",
    "            start_time_t, end_time_t, next_start_time_t, index_next_t = update_time(time_df_test, start_time_t, batch_size)\n",
    "            \n",
    "            # Load batch data\n",
    "            data_v = data_load(table, name, start_time_t, end_time_t, timeformat)\n",
    "            \n",
    "            # Apply Hanning window\n",
    "            data_ = set_hanning_window(sample_rate, data_v.iloc[:,:-1])\n",
    "            \n",
    "            # Apply FFT\n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # Apply MinMax scaler\n",
    "            data_ = scaler.fit_transform(data_)\n",
    "            \n",
    "            # Setting up DataFrame + label\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data_v['label'].values\n",
    "            \n",
    "            # Print if the loaded data is empty\n",
    "            if len(data_) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(data_) == batch_size:\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data_test = np.array(data_.iloc[:,:-1]).reshape(batch_size, 1 , -1)\n",
    "                input_data_label = np.array(data_.iloc[:,-1:])\n",
    "                \n",
    "                # Convert data to Tensor\n",
    "                input_data_test = torch.tensor(input_data_test, dtype=torch.float32).to(device).float()\n",
    "                input_data_label = torch.tensor(input_data_label, dtype=torch.float32).to(device).long()\n",
    "                \n",
    "                # Create DataLoader\n",
    "                dataset = TensorDataset(input_data_test, input_data_label)\n",
    "                data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "                \n",
    "                for batch_input, batch_label in data_loader:\n",
    "\n",
    "                    # Input to the model\n",
    "                    outputs_t = model(batch_input)\n",
    "                    \n",
    "                    # Set label predictions\n",
    "                    _,pred_t = torch.max(outputs_t, dim=1)\n",
    "                    targets_t = batch_label.view_as(pred_t).to(device)\n",
    "                    \n",
    "                    preds_test.append(pred_t)\n",
    "                    target_test.append(targets_t)\n",
    "                \n",
    "                # Reset batch data\n",
    "                data_ = []\n",
    "                \n",
    "            # Set the next start time   \n",
    "            start_time_t = unquote(next_start_time_t) \n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next_t + (int(batch_size /16)) >= len(time_df_test):\n",
    "                break\n",
    "            \n",
    "    # Combine predictions and labels collected from all batches\n",
    "    preds_test = torch.cat(preds_test).detach().cpu().numpy()\n",
    "    target_test = torch.cat(target_test).detach().cpu().numpy()\n",
    "\n",
    "    # Create Result DataFrame\n",
    "    final_df = pd.DataFrame(target_test, columns=['label'])\n",
    "    final_df['pred'] = target_test\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "########################################### Test Parameter Settings ################################################\n",
    "# Load the best model\n",
    "model_ = torch.load(f'./result/rotor_1d_ResNet_New_Batch.pt') \n",
    "# Set the start time for the test data\n",
    "start_time_test = '2024-01-01 00:01:59'\n",
    "# Set the end time for the test data\n",
    "end_time_test = '2024-01-01 00:02:20'\n",
    "# Load the test time list\n",
    "time_df_test = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "######################################## Proceed with testing #############################################\n",
    "final_df = test(table, name, timeformat, model_, batch_size, sample_rate, scaler, time_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        80\n",
      "           1       1.00      1.00      1.00        80\n",
      "           2       1.00      1.00      1.00        80\n",
      "           3       1.00      1.00      1.00        80\n",
      "\n",
      "    accuracy                           1.00       320\n",
      "   macro avg       1.00      1.00      1.00       320\n",
      "weighted avg       1.00      1.00      1.00       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(final_df['label'].values, final_df['pred'].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
