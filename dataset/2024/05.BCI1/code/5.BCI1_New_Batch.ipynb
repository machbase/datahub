{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.fft import fft\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'bci1'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get-tag-names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list\n",
    "tag_name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test-s-0',\n",
       " 'test-s-1',\n",
       " 'test-s-10',\n",
       " 'test-s-11',\n",
       " 'test-s-12',\n",
       " 'test-s-13',\n",
       " 'test-s-14',\n",
       " 'test-s-15',\n",
       " 'test-s-16',\n",
       " 'test-s-17',\n",
       " 'test-s-18',\n",
       " 'test-s-19',\n",
       " 'test-s-2',\n",
       " 'test-s-20',\n",
       " 'test-s-21',\n",
       " 'test-s-22',\n",
       " 'test-s-23',\n",
       " 'test-s-24',\n",
       " 'test-s-25',\n",
       " 'test-s-26',\n",
       " 'test-s-27',\n",
       " 'test-s-28',\n",
       " 'test-s-29',\n",
       " 'test-s-3',\n",
       " 'test-s-30',\n",
       " 'test-s-31',\n",
       " 'test-s-32',\n",
       " 'test-s-33',\n",
       " 'test-s-34',\n",
       " 'test-s-35',\n",
       " 'test-s-36',\n",
       " 'test-s-37',\n",
       " 'test-s-38',\n",
       " 'test-s-39',\n",
       " 'test-s-4',\n",
       " 'test-s-40',\n",
       " 'test-s-41',\n",
       " 'test-s-42',\n",
       " 'test-s-43',\n",
       " 'test-s-44',\n",
       " 'test-s-45',\n",
       " 'test-s-46',\n",
       " 'test-s-47',\n",
       " 'test-s-48',\n",
       " 'test-s-49',\n",
       " 'test-s-5',\n",
       " 'test-s-50',\n",
       " 'test-s-51',\n",
       " 'test-s-52',\n",
       " 'test-s-53',\n",
       " 'test-s-54',\n",
       " 'test-s-55',\n",
       " 'test-s-56',\n",
       " 'test-s-57',\n",
       " 'test-s-58',\n",
       " 'test-s-59',\n",
       " 'test-s-6',\n",
       " 'test-s-60',\n",
       " 'test-s-61',\n",
       " 'test-s-62',\n",
       " 'test-s-63',\n",
       " 'test-s-7',\n",
       " 'test-s-8',\n",
       " 'test-s-9',\n",
       " 'train-s-0',\n",
       " 'train-s-1',\n",
       " 'train-s-10',\n",
       " 'train-s-11',\n",
       " 'train-s-12',\n",
       " 'train-s-13',\n",
       " 'train-s-14',\n",
       " 'train-s-15',\n",
       " 'train-s-16',\n",
       " 'train-s-17',\n",
       " 'train-s-18',\n",
       " 'train-s-19',\n",
       " 'train-s-2',\n",
       " 'train-s-20',\n",
       " 'train-s-21',\n",
       " 'train-s-22',\n",
       " 'train-s-23',\n",
       " 'train-s-24',\n",
       " 'train-s-25',\n",
       " 'train-s-26',\n",
       " 'train-s-27',\n",
       " 'train-s-28',\n",
       " 'train-s-29',\n",
       " 'train-s-3',\n",
       " 'train-s-30',\n",
       " 'train-s-31',\n",
       " 'train-s-32',\n",
       " 'train-s-33',\n",
       " 'train-s-34',\n",
       " 'train-s-35',\n",
       " 'train-s-36',\n",
       " 'train-s-37',\n",
       " 'train-s-38',\n",
       " 'train-s-39',\n",
       " 'train-s-4',\n",
       " 'train-s-40',\n",
       " 'train-s-41',\n",
       " 'train-s-42',\n",
       " 'train-s-43',\n",
       " 'train-s-44',\n",
       " 'train-s-45',\n",
       " 'train-s-46',\n",
       " 'train-s-47',\n",
       " 'train-s-48',\n",
       " 'train-s-49',\n",
       " 'train-s-5',\n",
       " 'train-s-50',\n",
       " 'train-s-51',\n",
       " 'train-s-52',\n",
       " 'train-s-53',\n",
       " 'train-s-54',\n",
       " 'train-s-55',\n",
       " 'train-s-56',\n",
       " 'train-s-57',\n",
       " 'train-s-58',\n",
       " 'train-s-59',\n",
       " 'train-s-6',\n",
       " 'train-s-60',\n",
       " 'train-s-61',\n",
       " 'train-s-62',\n",
       " 'train-s-63',\n",
       " 'train-s-7',\n",
       " 'train-s-8',\n",
       " 'train-s-9',\n",
       " 'train-s-answer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the BCI1 dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Select all training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'train-s-0','train-s-1','train-s-10','train-s-11','train-s-12','train-s-13','train-s-14','train-s-15','train-s-16','train-s-17','train-s-18','train-s-19','train-s-2','train-s-20','train-s-21','train-s-22','train-s-23','train-s-24','train-s-25','train-s-26','train-s-27','train-s-28','train-s-29','train-s-3','train-s-30','train-s-31','train-s-32','train-s-33','train-s-34','train-s-35','train-s-36','train-s-37','train-s-38','train-s-39','train-s-4','train-s-40','train-s-41','train-s-42','train-s-43','train-s-44','train-s-45','train-s-46','train-s-47','train-s-48','train-s-49','train-s-5','train-s-50','train-s-51','train-s-52','train-s-53','train-s-54','train-s-55','train-s-56','train-s-57','train-s-58','train-s-59','train-s-6','train-s-60','train-s-61','train-s-62','train-s-63','train-s-7','train-s-8','train-s-9','train-s-answer'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = tag_name[64:]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_= \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BCI1 Dataset\n",
    "* Load the data using the Tag Names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, tag_name, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # List to store the results\n",
    "    result_dfs = []\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "\n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "\n",
    "    # Separate target values\n",
    "    df_label = df.iloc[:, -1:].dropna()\n",
    "\n",
    "    # Remove target column\n",
    "    df = df.iloc[:, :-1]\n",
    "\n",
    "    for col_name in tag_name[64:-1]:\n",
    "\n",
    "        # Set TIME column\n",
    "        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "        # Group by 3-second intervals and count the number of records\n",
    "        df_counts = df.groupby(df['TIME'].dt.floor('3S')).size().reset_index(name='count')\n",
    "\n",
    "        # Filter only groups with the most common count\n",
    "        most_common_count = df_counts['count'].mode()[0]\n",
    "        filtered_df_counts = df_counts[df_counts['count'] == most_common_count]\n",
    "\n",
    "        # Convert filtered time values to a list\n",
    "        filtered_times = filtered_df_counts['TIME'].tolist()\n",
    "\n",
    "        # Select only the filtered time values from the original DataFrame\n",
    "        filtered_data = df[df['TIME'].dt.floor('3S').isin(filtered_times)]\n",
    "\n",
    "        # Group by TIME (rounded to 3-second intervals)\n",
    "        filtered_data_ = filtered_data.copy()\n",
    "        filtered_data_['TIME'] = filtered_data_['TIME'].dt.floor('3S')\n",
    "        grouped = filtered_data_.groupby('TIME')[col_name].apply(list).reset_index()\n",
    "\n",
    "        # Split the list into individual columns\n",
    "        result_df = pd.DataFrame(grouped[col_name].tolist())\n",
    "\n",
    "        # Add result to the list\n",
    "        result_dfs.append(result_df)\n",
    "        \n",
    "    # Initialize the list to store results\n",
    "    data_list = []\n",
    "    k = 0\n",
    "\n",
    "    for k in result_dfs:\n",
    "        \n",
    "        # Convert to array format\n",
    "        data = k.values\n",
    "        data_list.append(data)\n",
    "\n",
    "    # Convert the list to a NumPy array\n",
    "    data_array = np.array(data_list)\n",
    "\n",
    "    # Reshape to the required format\n",
    "    # Transform to the shape (number of data, 64, 3000)\n",
    "    reshaped_array = np.transpose(data_array, (1, 0, 2)) \n",
    "    \n",
    "    # Modify 'train-s-answer'\n",
    "    df_label.loc[df_label['train-s-answer'] == -1.0, 'train-s-answer'] = 0\n",
    "    df_label['train-s-answer'] = df_label['train-s-answer'].astype(int)\n",
    "\n",
    "    return reshaped_array, df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data time loading function\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'TIME'\n",
    "    \n",
    "    # Load the data \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # Create a dummy value column for resampling\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # Perform resampling\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'])\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    # Determine resampling units based on the data and perform resampling\n",
    "    df = df.resample('3S').mean()\n",
    "    \n",
    "    # Remove missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove the dummy value column\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time update function\n",
    "# Update start and end times based on batch size\n",
    "def update_time(time_df, start_time, batch_size):\n",
    "    \n",
    "    # Calculate how many data points need to be loaded\n",
    "    time = batch_size\n",
    "    \n",
    "    # Check the index number of the current time\n",
    "    # If not found, set to the first index as there is no data for the current time\n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # Set the end time for the batch data based on the current time \n",
    "    end_time_ = str(time_df.index[index_now + time] + timedelta(seconds=1))\n",
    "    \n",
    "    # Set the index number for the next start time\n",
    "    index_next = index_now + time\n",
    "    \n",
    "    # Set the next start time\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL encoding\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "   * 1 Hanning Window\n",
    "   * 2 FFT \n",
    "   * 3 PCA -> Include in the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hanning window function setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanning window function setup \n",
    "def hanning_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FFT (Fast Fourier Transform) function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT transformation function\n",
    "def change_fft(sample_rate, data):\n",
    "    # Total number of samples in the signal\n",
    "    N = sample_rate\n",
    "    \n",
    "    # Initialize an array to store FFT results for each channel\n",
    "    fft_results = np.zeros((data.shape[0], data.shape[1], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # Apply FFT to the entire dataset\n",
    "    for i in range(data.shape[0]):  # For each sample\n",
    "        for j in range(data.shape[1]):  # For each channel\n",
    "            yf = fft(data[i, j], n=N)  # Calculate FFT\n",
    "            # Compute the absolute value of the FFT result and normalize (only the meaningful part)\n",
    "            fft_results[i, j] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    return fft_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using ResNet1d model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet 1D Model Setup \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Identity mapping\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64 \n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(64, 64, kernel_size=7, stride=2, padding=3) \n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=2).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "* Proceed by loading only the necessary batch size of data for training.\n",
    "* Save the model with the highest F1 score based on the validation data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train(table, tag_name, name, timeformat, model, batch_size_train, batch_size_valid, epochs, time_df_train, time_df_valid, pca, sample_rate):\n",
    "    \n",
    "    # Initialize training loss & accuracy\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    # Initialize best F1 Score value\n",
    "    best_f1= 0\n",
    "\n",
    "    # Start model training\n",
    "    for epoch in epochs:\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        correct = 0\n",
    "        total=0\n",
    "        \n",
    "        # Set initial start time\n",
    "        start_time_ = str(time_df_train.index[0])\n",
    "        \n",
    "        # Set end time\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "        \n",
    "        # Use a while loop to call data\n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # Set the time for loading data based on the batch size \n",
    "            start_time_, end_time_, next_start_time_, index_next= update_time(time_df_train, start_time_, batch_size_train)\n",
    "        \n",
    "            # Load batch data \n",
    "            data, label = data_load(table, tag_name, name, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # Apply Hanning window\n",
    "            data = data * hanning_window(sample_rate)\n",
    "            \n",
    "            # Apply FFT\n",
    "            data = change_fft(sample_rate, data)\n",
    "            \n",
    "            # Apply PCA\n",
    "            # Reshape to 2D\n",
    "            data_ = data.reshape(-1, data.shape[2])\n",
    "            data_ = pca.fit_transform(data_)\n",
    "            # Reshape back to original 3D format\n",
    "            data = data_.reshape(data.shape[0], data.shape[1], -1)\n",
    "            \n",
    "            # Print if the loaded data is empty \n",
    "            if len(data) == 0:\n",
    "                print(\"No data available.\")\n",
    "            \n",
    "            # Input the data into the model when it accumulates to the batch size\n",
    "            if len(data) == batch_size_train:\n",
    "                \n",
    "                # Check total batch count\n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # Convert data to numpy arrays\n",
    "                input_data = np.array(data)\n",
    "                input_target = np.array(label)[:-1]\n",
    "\n",
    "                # Convert data to Tensor\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "                input_target = torch.tensor(input_target, dtype=torch.float32).to(device).long().squeeze()\n",
    "                \n",
    "                # Optimize the optimizer\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Input to the model\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, input_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "                # Set label predictions\n",
    "                _,pred = torch.max(outputs, dim=1)\n",
    "                correct += torch.sum(pred==input_target).item()\n",
    "                total += input_target.size(0)\n",
    "\n",
    "                # Reset batch data\n",
    "                data = []\n",
    "                \n",
    "            # Set the next start time   \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "            \n",
    "            # Prevent fetching beyond the last time\n",
    "            if index_next + batch_size_train >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "        train_acc.append(100 * correct / total)\n",
    "        train_loss.append(running_loss/total_step)\n",
    "        print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "        \n",
    "        # Perform validation at the end of each epoch and save the model with the best performance\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            preds_v = []\n",
    "            targets_v = []\n",
    "                \n",
    "            # Set initial start time\n",
    "            start_time_v = str(time_df_valid.index[0])\n",
    "            \n",
    "            # Set end time\n",
    "            end_time_valid = str(time_df_valid.index[-1])\n",
    "            \n",
    "            # Use a while loop to call data \n",
    "            while start_time_v < end_time_valid:\n",
    "                \n",
    "                # Set the time for loading data based on the batch size\n",
    "                start_time_v, end_time_v, next_start_time_v, index_next_v = update_time(time_df_valid, start_time_v, batch_size_valid)\n",
    "                \n",
    "                # Load batch data \n",
    "                data_v, label_v = data_load(table, tag_name, name, start_time_v, end_time_v, timeformat)\n",
    "                \n",
    "                # Apply Hanning window\n",
    "                data_v = data_v * hanning_window(sample_rate)\n",
    "                \n",
    "                # Apply FFT\n",
    "                data_v = change_fft(sample_rate, data_v)\n",
    "                \n",
    "                # Apply PCA\n",
    "                # Reshape to 2D\n",
    "                data_ = data_v.reshape(-1, data_v.shape[2])\n",
    "                data_ = pca.fit_transform(data_)\n",
    "                # Reshape back to original 3D format\n",
    "                data_v = data_.reshape(data_v.shape[0], data_v.shape[1], -1)\n",
    "                \n",
    "                # Print if the loaded data is empty\n",
    "                if len(data_v) == 0:\n",
    "                    print(\"No data available.\")\n",
    "                \n",
    "                # Input the data into the model when it accumulates to the batch size\n",
    "                if len(data_v) == batch_size_valid:\n",
    "                    \n",
    "                    # Convert data to numpy arrays\n",
    "                    input_data_v = np.array(data_v)\n",
    "                    input_target_v = np.array(label_v)[:-1]\n",
    "\n",
    "                    # Convert data to Tensor\n",
    "                    input_data_v = torch.tensor(input_data_v, dtype=torch.float32).to(device).float()\n",
    "                    input_target_v = torch.tensor(input_target_v, dtype=torch.float32).to(device).long().squeeze()\n",
    "                    \n",
    "                    # Input to the model\n",
    "                    outputs_v = model(input_data_v)\n",
    "                    \n",
    "                    # Set label predictions \n",
    "                    _,pred_v = torch.max(outputs_v, dim=1)\n",
    "                    target_v = input_target_v.view_as(pred_v)\n",
    "    \n",
    "                    preds_v.append(pred_v)\n",
    "                    targets_v.append(target_v)\n",
    "                    \n",
    "                    # Reset batch data\n",
    "                    data_v = []\n",
    "                    \n",
    "                # Set the next start time    \n",
    "                start_time_v = unquote(next_start_time_v)\n",
    "                \n",
    "                # Prevent fetching beyond the last time\n",
    "                if index_next_v + batch_size_valid >= len(time_df_valid):\n",
    "                    break\n",
    "                    \n",
    "            # Combine predictions and labels collected from all batches\n",
    "            preds_v = torch.cat(preds_v).detach().cpu().numpy()\n",
    "            targets_v = torch.cat(targets_v).detach().cpu().numpy()\n",
    "            \n",
    "            f1score = f1_score(targets_v, preds_v,  average='macro')\n",
    "            if best_f1 < f1score:\n",
    "                best_f1 = f1score\n",
    "                # Save the best model \n",
    "                with open(\"./result/BCI1_ResN et1d_New_Batch.txt\", \"a\") as text_file:\n",
    "                    print('epoch=====',epoch, file=text_file)\n",
    "                    print(classification_report(targets_v, preds_v, digits=4), file=text_file)\n",
    "                torch.save(model, f'./result/BCI1_ResNet1d_New_Batch.pt') \n",
    "            epochs.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62039d48f29a471cbd9e8496b3954dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.2080353131661048, train acc: 47.5962\n",
      "\n",
      "train loss: 1.1204177118264713, train acc: 49.0385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.0725246285780883, train acc: 53.3654\n",
      "\n",
      "train loss: 0.9887269127827425, train acc: 50.4808\n",
      "\n",
      "train loss: 0.949872727577503, train acc: 50.0000\n",
      "\n",
      "train loss: 0.9121195398844207, train acc: 51.9231\n",
      "\n",
      "train loss: 0.8853872693501987, train acc: 53.3654\n",
      "\n",
      "train loss: 0.8643282795181642, train acc: 52.8846\n",
      "\n",
      "train loss: 0.8463760217030843, train acc: 51.9231\n",
      "\n",
      "train loss: 0.8322952550191145, train acc: 57.2115\n",
      "\n",
      "train loss: 0.8204017089797067, train acc: 55.2885\n",
      "\n",
      "train loss: 0.8087945209863858, train acc: 62.0192\n",
      "\n",
      "train loss: 0.7943512659806472, train acc: 66.3462\n",
      "\n",
      "train loss: 0.7745996657963637, train acc: 76.9231\n",
      "\n",
      "train loss: 0.7561487672802729, train acc: 83.1731\n",
      "\n",
      "train loss: 0.7251674725602453, train acc: 89.9038\n",
      "\n",
      "train loss: 0.6985537067257981, train acc: 91.3462\n",
      "\n",
      "train loss: 0.669987006733815, train acc: 93.2692\n",
      "\n",
      "train loss: 0.6403646057826065, train acc: 97.1154\n",
      "\n",
      "train loss: 0.6117753831777148, train acc: 97.5962\n",
      "\n",
      "train loss: 0.5850418515865019, train acc: 98.5577\n",
      "\n",
      "train loss: 0.5604482062268429, train acc: 98.5577\n",
      "\n",
      "train loss: 0.5377730489926519, train acc: 98.5577\n",
      "\n",
      "train loss: 0.5168935519938643, train acc: 98.5577\n",
      "\n",
      "train loss: 0.49746735017818317, train acc: 98.5577\n",
      "\n",
      "train loss: 0.47909938898137294, train acc: 99.0385\n",
      "\n",
      "train loss: 0.46313272613361717, train acc: 99.5192\n",
      "\n",
      "train loss: 0.4481612666113305, train acc: 98.0769\n",
      "\n",
      "train loss: 0.43353777228746815, train acc: 99.5192\n",
      "\n",
      "train loss: 0.4194104757174125, train acc: 99.5192\n",
      "\n",
      "train loss: 0.40597733807634195, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3933227802537238, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3814200048212036, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3702125302082658, train acc: 100.0000\n",
      "\n",
      "train loss: 0.35964334927631053, train acc: 100.0000\n",
      "\n",
      "train loss: 0.34965998744536403, train acc: 100.0000\n",
      "\n",
      "train loss: 0.34021535755864585, train acc: 100.0000\n",
      "\n",
      "train loss: 0.33126711573934026, train acc: 100.0000\n",
      "\n",
      "train loss: 0.32277721497566325, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3147113956634292, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3070386915517124, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2997310675202157, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2927630980623809, train acc: 100.0000\n",
      "\n",
      "train loss: 0.28611165182029114, train acc: 100.0000\n",
      "\n",
      "train loss: 0.27975565438797106, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2736758608596599, train acc: 100.0000\n",
      "\n",
      "train loss: 0.26785465513876117, train acc: 100.0000\n",
      "\n",
      "train loss: 0.26227588894402615, train acc: 100.0000\n",
      "\n",
      "train loss: 0.25692473032983487, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2517875346649984, train acc: 100.0000\n",
      "\n",
      "train loss: 0.24685172049206672, train acc: 100.0000\n",
      "\n",
      "train loss: 0.24210567870824845, train acc: 100.0000\n",
      "\n",
      "train loss: 0.237538672366774, train acc: 100.0000\n",
      "\n",
      "train loss: 0.23314076106622747, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2289027260119316, train acc: 100.0000\n",
      "\n",
      "train loss: 0.224816006190442, train acc: 100.0000\n",
      "\n",
      "train loss: 0.22087264091739586, train acc: 100.0000\n",
      "\n",
      "train loss: 0.21706521839188345, train acc: 100.0000\n",
      "\n",
      "train loss: 0.21338682850566476, train acc: 100.0000\n",
      "\n",
      "train loss: 0.20983102181320504, train acc: 100.0000\n",
      "\n",
      "train loss: 0.20639177172671072, train acc: 100.0000\n",
      "\n",
      "train loss: 0.20306344058107068, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19984074743197408, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19671874155410216, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1936927766268762, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19075848904528395, train acc: 100.0000\n",
      "\n",
      "train loss: 0.18791177476039836, train acc: 100.0000\n",
      "\n",
      "train loss: 0.18514877192853926, train acc: 100.0000\n",
      "\n",
      "train loss: 0.18246584166075364, train acc: 100.0000\n",
      "\n",
      "train loss: 0.17985955280781946, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1773266680004792, train acc: 100.0000\n",
      "\n",
      "train loss: 0.17486412952448457, train acc: 100.0000\n",
      "\n",
      "train loss: 0.17246904659207463, train acc: 100.0000\n",
      "\n",
      "train loss: 0.170138685416932, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1678704573885065, train acc: 100.0000\n",
      "\n",
      "train loss: 0.16566191057623278, train acc: 100.0000\n",
      "\n",
      "train loss: 0.16351072019925536, train acc: 100.0000\n",
      "\n",
      "train loss: 0.16141468063758352, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15937169823793643, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15737978346470438, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1554370445985571, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15354168387623357, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15169198789666352, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1498863265669757, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14812314618807104, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14640096484674342, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14471836882229572, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14307400894734817, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14146659644680284, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13989490007247965, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1383577423444797, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1368539974137492, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13538258745423642, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1339424806145482, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13253268870097312, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1311522641511136, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12980029909764537, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1284759221503926, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12717829752597948, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12590662289538018, train acc: 100.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### Training Parameter Settings ################################################\n",
    "# Set tag table name\n",
    "table = 'bci1'\n",
    "# Set tag name\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set time format\n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Set the start time for the train data\n",
    "start_time_train = '2024-01-01 00:00:00'\n",
    "# Set the end time for the train data\n",
    "end_time_train = '2024-01-01 03:41:00'\n",
    "# Set train batch size\n",
    "batch_size_train = 16\n",
    "# Set number of epochs\n",
    "epochs = trange(100, desc='training')\n",
    "# Set sample rate\n",
    "sample_rate = 3000\n",
    "# Set PCA\n",
    "# Select principal components that explain 95% of the variance\n",
    "pca = PCA(n_components=14)\n",
    "# Load training time list \n",
    "time_df_train = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "########################################### validation Parameter Settings ################################################\n",
    "# Set the start time for the validation data\n",
    "start_time_valid = '2024-01-01 03:42:00'\n",
    "# Set the end time for the validation data\n",
    "end_time_valid = '2024-01-01 04:09:00'\n",
    "# Set vaild batch size\n",
    "batch_size_valid = 16\n",
    "# Load validation time list\n",
    "time_df_valid = time_data_load(table, name, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "\n",
    "########################################### Proceed with training ################################################\n",
    "train(table, tag_name, name, timeformat, model, batch_size_train, batch_size_valid, epochs, time_df_train, time_df_valid, pca, sample_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing function  \n",
    "def test(table, tag_name, name, timeformat, model, batch_size, time_df_test, pca, sample_rate):\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                model.eval()\n",
    "                \n",
    "                # Initial settings \n",
    "                preds_t = []\n",
    "                targets_t = []\n",
    "                    \n",
    "                # Set the initial start time\n",
    "                start_time_t = str(time_df_test.index[0])\n",
    "                \n",
    "                # Set the end time\n",
    "                end_time_test = str(time_df_test.index[-1])\n",
    "                \n",
    "                # Use a while loop to call data  \n",
    "                while start_time_t < end_time_test:\n",
    "                    \n",
    "                    # Set the time for loading data based on the batch size\n",
    "                    start_time_t, end_time_t, next_start_time_t, index_next_t = update_time(time_df_test, start_time_t, batch_size)\n",
    "                    \n",
    "                    # Load batch data\n",
    "                    data_t, label_t = data_load(table, tag_name, name, start_time_t, end_time_t, timeformat)\n",
    "                    \n",
    "                    # Apply Hanning window\n",
    "                    data_t = data_t * hanning_window(sample_rate)\n",
    "                    \n",
    "                    # Apply FFT\n",
    "                    data_t = change_fft(sample_rate, data_t)\n",
    "                    \n",
    "                    # Apply PCA\n",
    "                    # Reshape to 2D\n",
    "                    data_ = data_t.reshape(-1, data_t.shape[2])\n",
    "                    data_ = pca.fit_transform(data_)\n",
    "                    # Reshape back to original 3D format\n",
    "                    data_t = data_.reshape(data_t.shape[0], data_t.shape[1], -1)\n",
    "                    \n",
    "                    # Print if the loaded data is empty\n",
    "                    if len(data_t) == 0:\n",
    "                        print(\"No data available.\")\n",
    "                    \n",
    "                    # Input the data into the model when it accumulates to the batch size\n",
    "                    if len(data_t) == batch_size:\n",
    "                        \n",
    "                        # Convert data to numpy arrays\n",
    "                        input_data_t = np.array(data_t)\n",
    "                        input_target_t = np.array(label_t)[:-1]\n",
    "\n",
    "                        # Convert data to Tensor\n",
    "                        input_data_t = torch.tensor(input_data_t, dtype=torch.float32).to(device).float()\n",
    "                        input_target_t = torch.tensor(input_target_t, dtype=torch.float32).to(device).long().squeeze()\n",
    "                        \n",
    "                        # Input to the model\n",
    "                        outputs_t = model(input_data_t)\n",
    "                        \n",
    "                        # Set label predictions\n",
    "                        _,pred_t = torch.max(outputs_t, dim=1)\n",
    "                        target_t = input_target_t.view_as(pred_t)\n",
    "        \n",
    "                        preds_t.append(pred_t)\n",
    "                        targets_t.append(target_t)\n",
    "                        \n",
    "                        # Reset batch data\n",
    "                        data_t = []\n",
    "                        \n",
    "                    # Set the next start time   \n",
    "                    start_time_t = unquote(next_start_time_t)\n",
    "                    \n",
    "                    # Prevent fetching beyond the last time\n",
    "                    if index_next_t + batch_size + 1 >= len(time_df_test):\n",
    "                        break\n",
    "                        \n",
    "                # Combine predictions and labels collected from all batches\n",
    "                preds_t = torch.cat(preds_t).detach().cpu().numpy()\n",
    "                targets_t = torch.cat(targets_t).detach().cpu().numpy()\n",
    "                    \n",
    "            return targets_t, preds_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Test Parameter Settings ################################################\n",
    "# Load the best model\n",
    "model_ = torch.load(f'./result/BCI1_ResNet1d_New_Batch.pt') \n",
    "# Set the start time for the test data\n",
    "start_time_test = '2024-01-01 04:10:00'\n",
    "# Set the end time for the test data\n",
    "end_time_test = '2024-01-01 04:37:01'\n",
    "# Set the test batch size\n",
    "batch_size_test = 4\n",
    "# Load the test time list\n",
    "time_df_test = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "######################################## Proceed with testing #############################################\n",
    "targets_t, preds_t = test(table, tag_name, name, timeformat, model_, batch_size_test, time_df_test, pca, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78        14\n",
      "           1       0.67      1.00      0.80        10\n",
      "\n",
      "    accuracy                           0.79        24\n",
      "   macro avg       0.83      0.82      0.79        24\n",
      "weighted avg       0.86      0.79      0.79        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets_t, preds_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
