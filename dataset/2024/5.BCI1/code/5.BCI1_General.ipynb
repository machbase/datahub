{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## 사용 라이브러리 호출\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.fft import fft\n",
    "\n",
    "## 모델 사용 라이브러리 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## 모델 학습 결과 경로 설정 \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Cuda 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## 랜덤 시드 설정\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 시드 설정 \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 데이터 컬럼 선택\n",
    "\n",
    "* tag name 이 많은 경우 tag name을 지정하는 것에 있어서 변수 설정이 다소 유연해짐\n",
    "* tag name 은 순서대로 불러와짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag name 출력 함수 \n",
    "def show_column(URL):\n",
    "    \n",
    "    # Tag name 데이터 로드\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # List 형식으로 변환\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tag name 출력 파라미터 설정\n",
    "table = 'bci1'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## tag name list 생성 \n",
    "tag_name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test-s-0',\n",
       " 'test-s-1',\n",
       " 'test-s-10',\n",
       " 'test-s-11',\n",
       " 'test-s-12',\n",
       " 'test-s-13',\n",
       " 'test-s-14',\n",
       " 'test-s-15',\n",
       " 'test-s-16',\n",
       " 'test-s-17',\n",
       " 'test-s-18',\n",
       " 'test-s-19',\n",
       " 'test-s-2',\n",
       " 'test-s-20',\n",
       " 'test-s-21',\n",
       " 'test-s-22',\n",
       " 'test-s-23',\n",
       " 'test-s-24',\n",
       " 'test-s-25',\n",
       " 'test-s-26',\n",
       " 'test-s-27',\n",
       " 'test-s-28',\n",
       " 'test-s-29',\n",
       " 'test-s-3',\n",
       " 'test-s-30',\n",
       " 'test-s-31',\n",
       " 'test-s-32',\n",
       " 'test-s-33',\n",
       " 'test-s-34',\n",
       " 'test-s-35',\n",
       " 'test-s-36',\n",
       " 'test-s-37',\n",
       " 'test-s-38',\n",
       " 'test-s-39',\n",
       " 'test-s-4',\n",
       " 'test-s-40',\n",
       " 'test-s-41',\n",
       " 'test-s-42',\n",
       " 'test-s-43',\n",
       " 'test-s-44',\n",
       " 'test-s-45',\n",
       " 'test-s-46',\n",
       " 'test-s-47',\n",
       " 'test-s-48',\n",
       " 'test-s-49',\n",
       " 'test-s-5',\n",
       " 'test-s-50',\n",
       " 'test-s-51',\n",
       " 'test-s-52',\n",
       " 'test-s-53',\n",
       " 'test-s-54',\n",
       " 'test-s-55',\n",
       " 'test-s-56',\n",
       " 'test-s-57',\n",
       " 'test-s-58',\n",
       " 'test-s-59',\n",
       " 'test-s-6',\n",
       " 'test-s-60',\n",
       " 'test-s-61',\n",
       " 'test-s-62',\n",
       " 'test-s-63',\n",
       " 'test-s-7',\n",
       " 'test-s-8',\n",
       " 'test-s-9',\n",
       " 'train-s-0',\n",
       " 'train-s-1',\n",
       " 'train-s-10',\n",
       " 'train-s-11',\n",
       " 'train-s-12',\n",
       " 'train-s-13',\n",
       " 'train-s-14',\n",
       " 'train-s-15',\n",
       " 'train-s-16',\n",
       " 'train-s-17',\n",
       " 'train-s-18',\n",
       " 'train-s-19',\n",
       " 'train-s-2',\n",
       " 'train-s-20',\n",
       " 'train-s-21',\n",
       " 'train-s-22',\n",
       " 'train-s-23',\n",
       " 'train-s-24',\n",
       " 'train-s-25',\n",
       " 'train-s-26',\n",
       " 'train-s-27',\n",
       " 'train-s-28',\n",
       " 'train-s-29',\n",
       " 'train-s-3',\n",
       " 'train-s-30',\n",
       " 'train-s-31',\n",
       " 'train-s-32',\n",
       " 'train-s-33',\n",
       " 'train-s-34',\n",
       " 'train-s-35',\n",
       " 'train-s-36',\n",
       " 'train-s-37',\n",
       " 'train-s-38',\n",
       " 'train-s-39',\n",
       " 'train-s-4',\n",
       " 'train-s-40',\n",
       " 'train-s-41',\n",
       " 'train-s-42',\n",
       " 'train-s-43',\n",
       " 'train-s-44',\n",
       " 'train-s-45',\n",
       " 'train-s-46',\n",
       " 'train-s-47',\n",
       " 'train-s-48',\n",
       " 'train-s-49',\n",
       " 'train-s-5',\n",
       " 'train-s-50',\n",
       " 'train-s-51',\n",
       " 'train-s-52',\n",
       " 'train-s-53',\n",
       " 'train-s-54',\n",
       " 'train-s-55',\n",
       " 'train-s-56',\n",
       " 'train-s-57',\n",
       " 'train-s-58',\n",
       " 'train-s-59',\n",
       " 'train-s-6',\n",
       " 'train-s-60',\n",
       " 'train-s-61',\n",
       " 'train-s-62',\n",
       " 'train-s-63',\n",
       " 'train-s-7',\n",
       " 'train-s-8',\n",
       " 'train-s-9',\n",
       " 'train-s-answer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAG Name format 변환 \n",
    "\n",
    "* 위의 과정에서 BCI1 dataset의 모든 Tag Name 을 확인후 사용할 컬럼만 뽑아서 입력할 파라미터 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'train-s-0','train-s-1','train-s-10','train-s-11','train-s-12','train-s-13','train-s-14','train-s-15','train-s-16','train-s-17','train-s-18','train-s-19','train-s-2','train-s-20','train-s-21','train-s-22','train-s-23','train-s-24','train-s-25','train-s-26','train-s-27','train-s-28','train-s-29','train-s-3','train-s-30','train-s-31','train-s-32','train-s-33','train-s-34','train-s-35','train-s-36','train-s-37','train-s-38','train-s-39','train-s-4','train-s-40','train-s-41','train-s-42','train-s-43','train-s-44','train-s-45','train-s-46','train-s-47','train-s-48','train-s-49','train-s-5','train-s-50','train-s-51','train-s-52','train-s-53','train-s-54','train-s-55','train-s-56','train-s-57','train-s-58','train-s-59','train-s-6','train-s-60','train-s-61','train-s-62','train-s-63','train-s-7','train-s-8','train-s-9','train-s-answer'\n"
     ]
    }
   ],
   "source": [
    "# 원하는 tag name 설정\n",
    "# 여기서 tag name 은 컬럼을 의미\n",
    "tags = tag_name[64:]\n",
    "\n",
    "# 리스트의 각 항목을 작은따옴표로 감싸고, 쉼표로 구분\n",
    "tags_= \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# 사용 tag name 확인\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BCI1 Dataset 로드\n",
    "\n",
    "* Tag Name 들을 사용하여 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 파라미터 설정\n",
    "\n",
    "# tag table 이름 설정\n",
    "table = 'bci1'\n",
    "# tag name 설정\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# 시간 포멧 설정 \n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Train , validation , test 데이터 셋 설정\n",
    "# 학습 데이터 시작 시간 설정\n",
    "start_time_train = quote('2024-01-01 00:00:00')\n",
    "# 학습 데이터  끝 시간 설정\n",
    "end_time_train = quote('2024-01-01 04:37:03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load_train(table, tag_name, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # 결과를 저장할 리스트\n",
    "    result_dfs = []\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "\n",
    "    # 같은 시간대 별 데이터로 전환\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # train-s-answer 값 분리\n",
    "    df_label = df.iloc[:, -1:].dropna()\n",
    "\n",
    "    # train-s-answer 컬럼 제거\n",
    "    df = df.iloc[:, :-1]\n",
    "\n",
    "    for col_name in tag_name[64:-1]:\n",
    "\n",
    "        # TIME 설정\n",
    "        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "        # 3초 단위로 그룹화하여 데이터 개수 세기\n",
    "        df_counts = df.groupby(df['TIME'].dt.floor('3S')).size().reset_index(name='count')\n",
    "\n",
    "        # 데이터 개수가 동일한 그룹만 필터링\n",
    "        most_common_count = df_counts['count'].mode()[0]\n",
    "        filtered_df_counts = df_counts[df_counts['count'] == most_common_count]\n",
    "\n",
    "        # 필터링된 시간값들을 리스트로 변환\n",
    "        filtered_times = filtered_df_counts['TIME'].tolist()\n",
    "\n",
    "        # 원본 데이터프레임에서 필터링된 시간값들만 선택\n",
    "        filtered_data = df[df['TIME'].dt.floor('3S').isin(filtered_times)]\n",
    "\n",
    "        # TIME을 기준으로 그룹화 (3초 단위로 반올림)\n",
    "        filtered_data_ = filtered_data.copy()\n",
    "        filtered_data_['TIME'] = filtered_data_['TIME'].dt.floor('3S')\n",
    "        grouped = filtered_data_.groupby('TIME')[col_name].apply(list).reset_index()\n",
    "\n",
    "        # 리스트를 개별 열로 나누기\n",
    "        result_df = pd.DataFrame(grouped[col_name].tolist())\n",
    "\n",
    "        result_dfs.append(result_df)  # 결과 추가\n",
    "        \n",
    "    # 결과를 저장할 리스트 초기화\n",
    "    data_list = []\n",
    "    k = 0\n",
    "\n",
    "    for k in result_dfs:\n",
    "        \n",
    "        # array 형태로 변환\n",
    "        data = k.values\n",
    "        data_list.append(data)\n",
    "\n",
    "    # 리스트를 NumPy 배열로 변환\n",
    "    data_array = np.array(data_list)\n",
    "\n",
    "    # 필요한 형태로 reshape\n",
    "    # (데이터 개수, 64, 3000) 형태로 변환\n",
    "    reshaped_array = np.transpose(data_array, (1, 0, 2)) \n",
    "    \n",
    "    # train-s-answer 변경\n",
    "    df_label.loc[df_label['train-s-answer'] == -1.0, 'train-s-answer'] = 0\n",
    "    df_label['train-s-answer'] = df_label['train-s-answer'].astype(int)    \n",
    "\n",
    "    return reshaped_array, df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "\n",
    "# 학습 데이터 로드\n",
    "train, train_label = data_load_train(table, tag_name, name, start_time_train, end_time_train, timeformat)\n",
    "\n",
    "# 데이터 분리\n",
    "train, test, train_label, test_label = train_test_split(train, train_label, test_size=0.2, random_state=77)\n",
    "test, valid, test_label, valid_label = train_test_split(test, test_label, test_size=0.5, random_state=77)\n",
    "\n",
    "train_label = train_label.reset_index(drop=True)\n",
    "valid_label = valid_label.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222, 64, 3000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 64, 3000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 64, 3000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "   * 1 Hanning Window\n",
    "   * 2 FFT \n",
    "   * 3 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hanning Window 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanning window 함수 설정 \n",
    "def hanning_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "window_length = 3000\n",
    "\n",
    "train_ = train * hanning_window(3000)\n",
    "valid_ = valid * hanning_window(3000)\n",
    "test_ = test * hanning_window(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT 변환 함수\n",
    "def change_fft(sample_rate, data):\n",
    "    # 신호의 총 샘플 수\n",
    "    N = sample_rate\n",
    "    \n",
    "    # 각 채널에 대해 FFT 결과를 저장할 배열 초기화\n",
    "    fft_results = np.zeros((data.shape[0], data.shape[1], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # 전체 데이터에 대해 FFT 적용\n",
    "    for i in range(data.shape[0]):  # 각 샘플에 대해\n",
    "        for j in range(data.shape[1]):  # 각 채널에 대해\n",
    "            yf = fft(data[i, j], n=N)  # FFT 계산\n",
    "            # FFT 결과의 절댓값을 계산하고 정규화 (유의미한 부분만)\n",
    "            fft_results[i, j] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    return fft_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링 주기 -> 초당 데이터 개수 \n",
    "sampling_rate = 3000\n",
    "\n",
    "# FFT 변환\n",
    "train_ = change_fft(sampling_rate, train_)\n",
    "valid_ = change_fft(sampling_rate, valid_)\n",
    "test_ = change_fft(sampling_rate, test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 적용 함수 \n",
    "def apply_pca(train_data, valid_data, test_data, n_components=0.95):\n",
    "    \n",
    "    # 2차원으로 변환\n",
    "    train_reshaped = train_data.reshape(-1, train_data.shape[2])\n",
    "    valid_reshaped = valid_data.reshape(-1, valid_data.shape[2])\n",
    "    test_reshaped = test_data.reshape(-1, test_data.shape[2]) \n",
    "\n",
    "    # 훈련 데이터로 PCA 학습\n",
    "    pca = PCA(n_components=n_components)\n",
    "    train_pca = pca.fit_transform(train_reshaped)\n",
    "    \n",
    "    # 검증 및 테스트 데이터에 PCA 적용\n",
    "    valid_pca = pca.transform(valid_reshaped)\n",
    "    test_pca = pca.transform(test_reshaped)\n",
    "\n",
    "    # 다시 원래의 3차원 형태로 변환\n",
    "    train_pca_reshaped = train_pca.reshape(train_data.shape[0], train_data.shape[1], -1)\n",
    "    valid_pca_reshaped = valid_pca.reshape(valid_data.shape[0], valid_data.shape[1], -1)\n",
    "    test_pca_reshaped = test_pca.reshape(test_data.shape[0], test_data.shape[1], -1)\n",
    "    \n",
    "    return train_pca_reshaped, valid_pca_reshaped, test_pca_reshaped, pca\n",
    "\n",
    "# 데이터셋에 PCA 적용\n",
    "train_pca, valid_pca, test_pca, pca_model = apply_pca(train_, valid_, test_, n_components=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 & 로더 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 설정\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \n",
    "        self.data = torch.Tensor(data)\n",
    "        self.labels = torch.Tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_time_data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return input_time_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 설정 \n",
    "train_ = EEGDataset(train_pca, train_label.values)\n",
    "valid_ = EEGDataset(valid_pca, valid_label.values)\n",
    "test_ = EEGDataset(test_pca, test_label.values)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_dataloader = DataLoader(train_, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 14])\n"
     ]
    }
   ],
   "source": [
    "print(list(train_dataloader)[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 모델 설정 \n",
    "\n",
    "* ResNet 1d 기본 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet 1d 모델 설정 \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Identity mapping\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64 \n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(64, 64, kernel_size=7, stride=2, padding=3) \n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정 파라미터\n",
    "# 학습률 \n",
    "learning_rate = 0.01\n",
    "\n",
    "# 모델 초기화\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=2).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 구조 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 설정\n",
    "\n",
    "* 학습 중 validation 데이터 기준 F1 score 값이 제일 높은 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbb5bc268854ba8975095d86596b284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.4139056674071722, train acc: 49.0991\n",
      "\n",
      "train loss: 1.1966735869646072, train acc: 51.3514\n",
      "\n",
      "train loss: 1.1239286561806996, train acc: 50.0000\n",
      "\n",
      "train loss: 1.0237462978277887, train acc: 54.0541\n",
      "\n",
      "train loss: 0.9544157181467329, train acc: 55.4054\n",
      "\n",
      "train loss: 0.9004832179773422, train acc: 66.2162\n",
      "\n",
      "train loss: 0.8366896386961548, train acc: 83.3333\n",
      "\n",
      "train loss: 0.7724695115217141, train acc: 88.2883\n",
      "\n",
      "train loss: 0.7162908397851483, train acc: 88.2883\n",
      "\n",
      "train loss: 0.6693504722722408, train acc: 90.5405\n",
      "\n",
      "train loss: 0.6212176911460309, train acc: 94.1441\n",
      "\n",
      "train loss: 0.5784983822959475, train acc: 96.3964\n",
      "\n",
      "train loss: 0.547223576266513, train acc: 95.0450\n",
      "\n",
      "train loss: 0.5140178799284717, train acc: 97.2973\n",
      "\n",
      "train loss: 0.4824921410557137, train acc: 97.7477\n",
      "\n",
      "train loss: 0.47120989181517614, train acc: 94.1441\n",
      "\n",
      "train loss: 0.4532639800597563, train acc: 95.0450\n",
      "\n",
      "train loss: 0.43208300089013174, train acc: 97.2973\n",
      "\n",
      "train loss: 0.411094871548436, train acc: 98.6486\n",
      "\n",
      "train loss: 0.3910890764343744, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3725619237243553, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3563012914657355, train acc: 99.5495\n",
      "\n",
      "train loss: 0.34137498884303563, train acc: 99.0991\n",
      "\n",
      "train loss: 0.32743568754660507, train acc: 99.5495\n",
      "\n",
      "train loss: 0.3145712887440615, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3025404250961176, train acc: 100.0000\n",
      "\n",
      "train loss: 0.29217146761007473, train acc: 99.5495\n",
      "\n",
      "train loss: 0.2843638341467164, train acc: 97.7477\n",
      "\n",
      "train loss: 0.27920444114874843, train acc: 95.9459\n",
      "\n",
      "train loss: 0.27250048819004885, train acc: 97.2973\n",
      "\n",
      "train loss: 0.26524714858638315, train acc: 97.2973\n",
      "\n",
      "train loss: 0.25876365744847313, train acc: 98.1982\n",
      "\n",
      "train loss: 0.2525518297169272, train acc: 98.1982\n",
      "\n",
      "train loss: 0.24825441749437935, train acc: 96.3964\n",
      "\n",
      "train loss: 0.24455984266516342, train acc: 95.9459\n",
      "\n",
      "train loss: 0.2432530432666385, train acc: 91.8919\n",
      "\n",
      "train loss: 0.2376411296086874, train acc: 99.0991\n",
      "\n",
      "train loss: 0.23161629943382292, train acc: 99.5495\n",
      "\n",
      "train loss: 0.22882116113831236, train acc: 98.6486\n",
      "\n",
      "train loss: 0.22594614460788956, train acc: 96.3964\n",
      "\n",
      "train loss: 0.2216026193534926, train acc: 97.7477\n",
      "\n",
      "train loss: 0.2172523203854483, train acc: 99.0991\n",
      "\n",
      "train loss: 0.21244997796038448, train acc: 100.0000\n",
      "\n",
      "train loss: 0.20842269641963293, train acc: 99.0991\n",
      "\n",
      "train loss: 0.20431619600121015, train acc: 99.5495\n",
      "\n",
      "train loss: 0.199956412731807, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19578570662893338, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1917115921181445, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1882074622242274, train acc: 99.0991\n",
      "\n",
      "train loss: 0.18737684066463023, train acc: 97.2973\n",
      "\n",
      "train loss: 0.18698244002970948, train acc: 96.3964\n",
      "\n",
      "train loss: 0.18387324705256614, train acc: 99.5495\n",
      "\n",
      "train loss: 0.1805758680410058, train acc: 99.5495\n",
      "\n",
      "train loss: 0.17745691215966936, train acc: 99.5495\n",
      "\n",
      "train loss: 0.17483093013975207, train acc: 98.6486\n",
      "\n",
      "train loss: 0.17217540913795526, train acc: 99.5495\n",
      "\n",
      "train loss: 0.1708582701035987, train acc: 95.9459\n",
      "\n",
      "train loss: 0.16865175559030904, train acc: 98.6486\n",
      "\n",
      "train loss: 0.1661991934737322, train acc: 98.6486\n",
      "\n",
      "train loss: 0.16390576957509556, train acc: 99.5495\n",
      "\n",
      "train loss: 0.16138139417465802, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15901772465176156, train acc: 99.5495\n",
      "\n",
      "train loss: 0.15655055574207177, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1549519763278666, train acc: 98.1982\n",
      "\n",
      "train loss: 0.15285393277684306, train acc: 99.5495\n",
      "\n",
      "train loss: 0.15070467086719275, train acc: 99.5495\n",
      "\n",
      "train loss: 0.14853868270740922, train acc: 99.5495\n",
      "\n",
      "train loss: 0.14637754326832664, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14448057413096863, train acc: 99.5495\n",
      "\n",
      "train loss: 0.14242545016065, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14052042280071175, train acc: 99.5495\n",
      "\n",
      "train loss: 0.13857328138544292, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13669600142654703, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1348491533236199, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13305159291824004, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1313014556666658, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12961252099712842, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12795112300378708, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12633523053694462, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12475632107141654, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12321644117135516, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12171425645278064, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12024858989616516, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11881786275866899, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11742095357007398, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11605578075628191, train acc: 100.0000\n",
      "\n",
      "train loss: 0.114723159042207, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11342158793207738, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11214755327895486, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1109017876903111, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10968586548945981, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10849823498833105, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10733202792618318, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10619020576952809, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1050725081980311, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10397803156202012, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10290614645081476, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10185825559955144, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10082944254237955, train acc: 100.0000\n",
      "\n",
      "train loss: 0.09982235513781566, train acc: 100.0000\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_dataloader)\n",
    "epoch_in = trange(100, desc='training')\n",
    "best_f1= 0\n",
    "\n",
    "for epoch in epoch_in:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "\n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    # 모델 학습 \n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = train_data[0].to(device).float()\n",
    "        labels = train_data[1].to(device).long().squeeze()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # label 예측 값 설정 \n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==labels).item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "    \n",
    "    # Epoch 마다 validation을 진행해서 가장 좋은 성능을 보이는 모델을 저장 \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "            inputs_v = valid_data[0].to(device).float()\n",
    "            labels_v = valid_data[1].to(device).long().squeeze() \n",
    "            \n",
    "            outputs_v = model(inputs_v)\n",
    "            \n",
    "            # label 예측 값 설정 \n",
    "            _,pred_v = torch.max(outputs_v, dim=1)\n",
    "            target_v = labels_v.view_as(pred_v)\n",
    "            \n",
    "            preds_.append(pred_v)\n",
    "            targets_.append(target_v)\n",
    "            \n",
    "        # 모든 배치에서 수집된 예측과 라벨을 합침\n",
    "        preds_ = torch.cat(preds_).detach().cpu().numpy()\n",
    "        targets_ = torch.cat(targets_).detach().cpu().numpy()\n",
    "        \n",
    "        f1score = f1_score(targets_, preds_,  average='macro')\n",
    "        if best_f1 < f1score:\n",
    "            best_f1 = f1score\n",
    "            # 베스트 모델 저장 \n",
    "            with open(\"./result/BCI1_ResNet1d_General.txt\", \"a\") as text_file:\n",
    "                print('epoch=====',epoch, file=text_file)\n",
    "                print(classification_report(targets_, preds_, digits=4), file=text_file)\n",
    "            torch.save(model, f'./result/BCI1_ResNet1d_General.pt') \n",
    "        epoch_in.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "model_ = torch.load(f'./result/BCI1_ResNet1d_General.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 테스트 \n",
    "preds_test = []\n",
    "target_test = []\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    for batch_idx, test_data in enumerate(test_dataloader):\n",
    "        inputs_t = test_data[0].to(device).float()\n",
    "        labels_t = test_data[1].to(device).long().squeeze()\n",
    "        \n",
    "        outputs_t = model_(inputs_t)\n",
    "        \n",
    "        _,pred_t = torch.max(outputs_t, dim=1)\n",
    "\n",
    "        preds_test.append(pred_t)\n",
    "        target_test.append(labels_t.item())\n",
    "        \n",
    "    # 모든 배치에서 수집된 예측과 라벨을 합침\n",
    "    preds_test = torch.cat(preds_test).detach().cpu().numpy()\n",
    "\n",
    "# 테스트 결과 데이터 프레임 생성 \n",
    "final = pd.DataFrame(target_test, columns=['label'])\n",
    "final['pred'] = preds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        16\n",
      "           1       0.82      0.75      0.78        12\n",
      "\n",
      "    accuracy                           0.82        28\n",
      "   macro avg       0.82      0.81      0.82        28\n",
      "weighted avg       0.82      0.82      0.82        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(final['label'].values, final['pred'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
