{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.fft import fft\n",
    "\n",
    "## Import libraries for the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## Set path for saving model training results  \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed\n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'bci1'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list\n",
    "tag_name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test-s-0',\n",
       " 'test-s-1',\n",
       " 'test-s-10',\n",
       " 'test-s-11',\n",
       " 'test-s-12',\n",
       " 'test-s-13',\n",
       " 'test-s-14',\n",
       " 'test-s-15',\n",
       " 'test-s-16',\n",
       " 'test-s-17',\n",
       " 'test-s-18',\n",
       " 'test-s-19',\n",
       " 'test-s-2',\n",
       " 'test-s-20',\n",
       " 'test-s-21',\n",
       " 'test-s-22',\n",
       " 'test-s-23',\n",
       " 'test-s-24',\n",
       " 'test-s-25',\n",
       " 'test-s-26',\n",
       " 'test-s-27',\n",
       " 'test-s-28',\n",
       " 'test-s-29',\n",
       " 'test-s-3',\n",
       " 'test-s-30',\n",
       " 'test-s-31',\n",
       " 'test-s-32',\n",
       " 'test-s-33',\n",
       " 'test-s-34',\n",
       " 'test-s-35',\n",
       " 'test-s-36',\n",
       " 'test-s-37',\n",
       " 'test-s-38',\n",
       " 'test-s-39',\n",
       " 'test-s-4',\n",
       " 'test-s-40',\n",
       " 'test-s-41',\n",
       " 'test-s-42',\n",
       " 'test-s-43',\n",
       " 'test-s-44',\n",
       " 'test-s-45',\n",
       " 'test-s-46',\n",
       " 'test-s-47',\n",
       " 'test-s-48',\n",
       " 'test-s-49',\n",
       " 'test-s-5',\n",
       " 'test-s-50',\n",
       " 'test-s-51',\n",
       " 'test-s-52',\n",
       " 'test-s-53',\n",
       " 'test-s-54',\n",
       " 'test-s-55',\n",
       " 'test-s-56',\n",
       " 'test-s-57',\n",
       " 'test-s-58',\n",
       " 'test-s-59',\n",
       " 'test-s-6',\n",
       " 'test-s-60',\n",
       " 'test-s-61',\n",
       " 'test-s-62',\n",
       " 'test-s-63',\n",
       " 'test-s-7',\n",
       " 'test-s-8',\n",
       " 'test-s-9',\n",
       " 'train-s-0',\n",
       " 'train-s-1',\n",
       " 'train-s-10',\n",
       " 'train-s-11',\n",
       " 'train-s-12',\n",
       " 'train-s-13',\n",
       " 'train-s-14',\n",
       " 'train-s-15',\n",
       " 'train-s-16',\n",
       " 'train-s-17',\n",
       " 'train-s-18',\n",
       " 'train-s-19',\n",
       " 'train-s-2',\n",
       " 'train-s-20',\n",
       " 'train-s-21',\n",
       " 'train-s-22',\n",
       " 'train-s-23',\n",
       " 'train-s-24',\n",
       " 'train-s-25',\n",
       " 'train-s-26',\n",
       " 'train-s-27',\n",
       " 'train-s-28',\n",
       " 'train-s-29',\n",
       " 'train-s-3',\n",
       " 'train-s-30',\n",
       " 'train-s-31',\n",
       " 'train-s-32',\n",
       " 'train-s-33',\n",
       " 'train-s-34',\n",
       " 'train-s-35',\n",
       " 'train-s-36',\n",
       " 'train-s-37',\n",
       " 'train-s-38',\n",
       " 'train-s-39',\n",
       " 'train-s-4',\n",
       " 'train-s-40',\n",
       " 'train-s-41',\n",
       " 'train-s-42',\n",
       " 'train-s-43',\n",
       " 'train-s-44',\n",
       " 'train-s-45',\n",
       " 'train-s-46',\n",
       " 'train-s-47',\n",
       " 'train-s-48',\n",
       " 'train-s-49',\n",
       " 'train-s-5',\n",
       " 'train-s-50',\n",
       " 'train-s-51',\n",
       " 'train-s-52',\n",
       " 'train-s-53',\n",
       " 'train-s-54',\n",
       " 'train-s-55',\n",
       " 'train-s-56',\n",
       " 'train-s-57',\n",
       " 'train-s-58',\n",
       " 'train-s-59',\n",
       " 'train-s-6',\n",
       " 'train-s-60',\n",
       " 'train-s-61',\n",
       " 'train-s-62',\n",
       " 'train-s-63',\n",
       " 'train-s-7',\n",
       " 'train-s-8',\n",
       " 'train-s-9',\n",
       " 'train-s-answer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the BCI1 dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Select all training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'train-s-0','train-s-1','train-s-10','train-s-11','train-s-12','train-s-13','train-s-14','train-s-15','train-s-16','train-s-17','train-s-18','train-s-19','train-s-2','train-s-20','train-s-21','train-s-22','train-s-23','train-s-24','train-s-25','train-s-26','train-s-27','train-s-28','train-s-29','train-s-3','train-s-30','train-s-31','train-s-32','train-s-33','train-s-34','train-s-35','train-s-36','train-s-37','train-s-38','train-s-39','train-s-4','train-s-40','train-s-41','train-s-42','train-s-43','train-s-44','train-s-45','train-s-46','train-s-47','train-s-48','train-s-49','train-s-5','train-s-50','train-s-51','train-s-52','train-s-53','train-s-54','train-s-55','train-s-56','train-s-57','train-s-58','train-s-59','train-s-6','train-s-60','train-s-61','train-s-62','train-s-63','train-s-7','train-s-8','train-s-9','train-s-answer'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = tag_name[64:]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_= \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BCI1 Dataset\n",
    "* Load the data using the Tag Names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading parameter settings\n",
    "\n",
    "# Set the tag table name\n",
    "table = 'bci1'\n",
    "# Set the tag names\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set the time format\n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Set the data start time\n",
    "start_time = quote('2024-01-01 00:00:00')\n",
    "# Set the data end time\n",
    "end_time = quote('2024-01-01 04:37:03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def data_load(table, tag_name, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # List to store the results\n",
    "    result_dfs = []\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "\n",
    "    # Convert to data grouped by the time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "\n",
    "    # Separate target values\n",
    "    df_label = df.iloc[:, -1:].dropna()\n",
    "\n",
    "    # Remove target column\n",
    "    df = df.iloc[:, :-1]\n",
    "\n",
    "    for col_name in tag_name[64:-1]:\n",
    "\n",
    "        # Set TIME column\n",
    "        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "        # Group by 3-second intervals and count the number of records\n",
    "        df_counts = df.groupby(df['TIME'].dt.floor('3S')).size().reset_index(name='count')\n",
    "\n",
    "        # Filter only groups with the most common count\n",
    "        most_common_count = df_counts['count'].mode()[0]\n",
    "        filtered_df_counts = df_counts[df_counts['count'] == most_common_count]\n",
    "\n",
    "        # Convert filtered time values to a list\n",
    "        filtered_times = filtered_df_counts['TIME'].tolist()\n",
    "\n",
    "        # Select only the filtered time values from the original DataFrame\n",
    "        filtered_data = df[df['TIME'].dt.floor('3S').isin(filtered_times)]\n",
    "\n",
    "        # Group by TIME (rounded to 3-second intervals)\n",
    "        filtered_data_ = filtered_data.copy()\n",
    "        filtered_data_['TIME'] = filtered_data_['TIME'].dt.floor('3S')\n",
    "        grouped = filtered_data_.groupby('TIME')[col_name].apply(list).reset_index()\n",
    "\n",
    "        # Split the list into individual columns\n",
    "        result_df = pd.DataFrame(grouped[col_name].tolist())\n",
    "\n",
    "        # Add result to the list\n",
    "        result_dfs.append(result_df)\n",
    "        \n",
    "    # Initialize the list to store results\n",
    "    data_list = []\n",
    "    k = 0\n",
    "\n",
    "    for k in result_dfs:\n",
    "        \n",
    "        # Convert to array format\n",
    "        data = k.values\n",
    "        data_list.append(data)\n",
    "\n",
    "    # Convert the list to a NumPy array\n",
    "    data_array = np.array(data_list)\n",
    "\n",
    "    # Reshape to the required format\n",
    "    # Transform to the shape (number of data, 64, 3000)\n",
    "    reshaped_array = np.transpose(data_array, (1, 0, 2)) \n",
    "    \n",
    "    # Modify 'train-s-answer'\n",
    "    df_label.loc[df_label['train-s-answer'] == -1.0, 'train-s-answer'] = 0\n",
    "    df_label['train-s-answer'] = df_label['train-s-answer'].astype(int)\n",
    "\n",
    "    return reshaped_array, df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "# Load training data\n",
    "train, train_label = data_load(table, tag_name, name, start_time, end_time, timeformat)\n",
    "\n",
    "# Split data -> train, validation, test\n",
    "train, test, train_label, test_label = train_test_split(train, train_label, test_size=0.2, random_state=77)\n",
    "test, valid, test_label, valid_label = train_test_split(test, test_label, test_size=0.5, random_state=77)\n",
    "\n",
    "train_label = train_label.reset_index(drop=True)\n",
    "valid_label = valid_label.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "   * 1 Hanning Window\n",
    "   * 2 FFT \n",
    "   * 3 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying Hanning Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanning window function setup \n",
    "def hanning_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Setup\n",
    "window_length = 3000\n",
    "\n",
    "# Applying Hanning Window\n",
    "train_ = train * hanning_window(3000)\n",
    "valid_ = valid * hanning_window(3000)\n",
    "test_ = test * hanning_window(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying FFT (Fast Fourier Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT transformation function\n",
    "def change_fft(sample_rate, data):\n",
    "    # Total number of samples in the signal\n",
    "    N = sample_rate\n",
    "    \n",
    "    # Initialize an array to store FFT results for each channel\n",
    "    fft_results = np.zeros((data.shape[0], data.shape[1], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # Apply FFT to the entire dataset\n",
    "    for i in range(data.shape[0]):  # For each sample\n",
    "        for j in range(data.shape[1]):  # For each channel\n",
    "            yf = fft(data[i, j], n=N)  # Calculate FFT\n",
    "            # Compute the absolute value of the FFT result and normalize (only the meaningful part)\n",
    "            fft_results[i, j] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    return fft_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling period -> Number of data points per second\n",
    "sampling_rate = 3000\n",
    "\n",
    "# Apply FFT transformation\n",
    "train_ = change_fft(sampling_rate, train_)\n",
    "valid_ = change_fft(sampling_rate, valid_)\n",
    "test_ = change_fft(sampling_rate, test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Applying PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA application function\n",
    "def apply_pca(train_data, valid_data, test_data, n_components=0.95):\n",
    "    \n",
    "    # Reshape to 2D\n",
    "    train_reshaped = train_data.reshape(-1, train_data.shape[2])\n",
    "    valid_reshaped = valid_data.reshape(-1, valid_data.shape[2])\n",
    "    test_reshaped = test_data.reshape(-1, test_data.shape[2]) \n",
    "\n",
    "    # Fit PCA on training data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    train_pca = pca.fit_transform(train_reshaped)\n",
    "    \n",
    "    # Apply PCA to validation and test data\n",
    "    valid_pca = pca.transform(valid_reshaped)\n",
    "    test_pca = pca.transform(test_reshaped)\n",
    "\n",
    "    # Reshape back to original 3D format\n",
    "    train_pca_reshaped = train_pca.reshape(train_data.shape[0], train_data.shape[1], -1)\n",
    "    valid_pca_reshaped = valid_pca.reshape(valid_data.shape[0], valid_data.shape[1], -1)\n",
    "    test_pca_reshaped = test_pca.reshape(test_data.shape[0], test_data.shape[1], -1)\n",
    "    \n",
    "    return train_pca_reshaped, valid_pca_reshaped, test_pca_reshaped, pca\n",
    "\n",
    "# Apply PCA to the datasets\n",
    "train_pca, valid_pca, test_pca, pca_model = apply_pca(train_, valid_, test_, n_components=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Loader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Setup\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \n",
    "        self.data = torch.Tensor(data)\n",
    "        self.labels = torch.Tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_time_data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return input_time_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup \n",
    "train_ = EEGDataset(train_pca, train_label.values)\n",
    "valid_ = EEGDataset(valid_pca, valid_label.values)\n",
    "test_ = EEGDataset(test_pca, test_label.values)\n",
    "\n",
    "# Data loader setup\n",
    "train_dataloader = DataLoader(train_, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 14])\n"
     ]
    }
   ],
   "source": [
    "# Verify DataLoader application and check the shape of the input data\n",
    "print(list(train_dataloader)[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using ResNet1d model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet 1D Model Setup \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Identity mapping\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64 \n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(64, 64, kernel_size=7, stride=2, padding=3) \n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=2).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "* Save the model with the highest F1 score based on the validation data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508e3fb49e4e4e6fb7a54d3fd1bd700d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.4153182591710771, train acc: 49.0991\n",
      "\n",
      "train loss: 1.1911047611917769, train acc: 51.3514\n",
      "\n",
      "train loss: 1.1260899901390076, train acc: 49.0991\n",
      "\n",
      "train loss: 1.0170632281473704, train acc: 56.3063\n",
      "\n",
      "train loss: 0.9465377867221832, train acc: 58.1081\n",
      "\n",
      "train loss: 0.8920519987032528, train acc: 63.9640\n",
      "\n",
      "train loss: 0.8320432107667534, train acc: 78.8288\n",
      "\n",
      "train loss: 0.7660457894339094, train acc: 90.0901\n",
      "\n",
      "train loss: 0.712477548668782, train acc: 89.1892\n",
      "\n",
      "train loss: 0.6751021089098816, train acc: 88.2883\n",
      "\n",
      "train loss: 0.6252017873339356, train acc: 96.3964\n",
      "\n",
      "train loss: 0.5772606652428208, train acc: 98.6486\n",
      "\n",
      "train loss: 0.5492561949963253, train acc: 90.5405\n",
      "\n",
      "train loss: 0.5180750247465248, train acc: 95.4955\n",
      "\n",
      "train loss: 0.4896357618787859, train acc: 94.5946\n",
      "\n",
      "train loss: 0.4717200311637758, train acc: 92.7928\n",
      "\n",
      "train loss: 0.4527906350847216, train acc: 94.1441\n",
      "\n",
      "train loss: 0.4353300654685073, train acc: 93.2432\n",
      "\n",
      "train loss: 0.41912484790515364, train acc: 95.0450\n",
      "\n",
      "train loss: 0.40011601843829603, train acc: 99.5495\n",
      "\n",
      "train loss: 0.3818832254221387, train acc: 99.5495\n",
      "\n",
      "train loss: 0.36562198475638236, train acc: 99.5495\n",
      "\n",
      "train loss: 0.3513296034081057, train acc: 98.1982\n",
      "\n",
      "train loss: 0.3383513167413444, train acc: 98.6486\n",
      "\n",
      "train loss: 0.3271746166051681, train acc: 98.6486\n",
      "\n",
      "train loss: 0.31650763396636655, train acc: 99.0991\n",
      "\n",
      "train loss: 0.3059105173385226, train acc: 99.0991\n",
      "\n",
      "train loss: 0.29571137777647705, train acc: 99.0991\n",
      "\n",
      "train loss: 0.2859915081398912, train acc: 99.5495\n",
      "\n",
      "train loss: 0.27682030671248437, train acc: 100.0000\n",
      "\n",
      "train loss: 0.268058520291322, train acc: 100.0000\n",
      "\n",
      "train loss: 0.25980886197643016, train acc: 100.0000\n",
      "\n",
      "train loss: 0.25231953588821326, train acc: 99.5495\n",
      "\n",
      "train loss: 0.24523277316226252, train acc: 99.5495\n",
      "\n",
      "train loss: 0.2384451879546296, train acc: 99.5495\n",
      "\n",
      "train loss: 0.23812169488389687, train acc: 97.2973\n",
      "\n",
      "train loss: 0.23303219816623716, train acc: 98.6486\n",
      "\n",
      "train loss: 0.2271577232633899, train acc: 99.5495\n",
      "\n",
      "train loss: 0.22221720971164538, train acc: 97.7477\n",
      "\n",
      "train loss: 0.21819301052431034, train acc: 97.2973\n",
      "\n",
      "train loss: 0.21362389288047046, train acc: 99.0991\n",
      "\n",
      "train loss: 0.20919521521736928, train acc: 98.6486\n",
      "\n",
      "train loss: 0.20449845202066352, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2001542081611557, train acc: 99.5495\n",
      "\n",
      "train loss: 0.19572636281444836, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19164290274586399, train acc: 99.5495\n",
      "\n",
      "train loss: 0.18790232889367306, train acc: 99.5495\n",
      "\n",
      "train loss: 0.1843887677357066, train acc: 99.5495\n",
      "\n",
      "train loss: 0.1814766126816169, train acc: 98.6486\n",
      "\n",
      "train loss: 0.17847905746385645, train acc: 99.0991\n",
      "\n",
      "train loss: 0.17561114182965679, train acc: 98.1982\n",
      "\n",
      "train loss: 0.1724083604239266, train acc: 99.5495\n",
      "\n",
      "train loss: 0.16955747403188473, train acc: 99.5495\n",
      "\n",
      "train loss: 0.1665613724884569, train acc: 100.0000\n",
      "\n",
      "train loss: 0.16357288738616577, train acc: 100.0000\n",
      "\n",
      "train loss: 0.16065796977074584, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15784669078612848, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15514374895397592, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15307927526099127, train acc: 99.0991\n",
      "\n",
      "train loss: 0.1526122202933049, train acc: 99.0991\n",
      "\n",
      "train loss: 0.15118324274252842, train acc: 98.6486\n",
      "\n",
      "train loss: 0.15034979144743246, train acc: 98.1982\n",
      "\n",
      "train loss: 0.14810162996530352, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1459697979641429, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14373524519594985, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1415882496654244, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1394846200108432, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13744314071540978, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1354591564659122, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13352570585856513, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13168585679975728, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13002247657804775, train acc: 99.5495\n",
      "\n",
      "train loss: 0.12849612319854695, train acc: 99.0991\n",
      "\n",
      "train loss: 0.12717308514741504, train acc: 99.5495\n",
      "\n",
      "train loss: 0.1258568025893921, train acc: 98.6486\n",
      "\n",
      "train loss: 0.12457873234436055, train acc: 99.5495\n",
      "\n",
      "train loss: 0.12358883086400485, train acc: 98.6486\n",
      "\n",
      "train loss: 0.12293914415534804, train acc: 97.7477\n",
      "\n",
      "train loss: 0.12319705281471495, train acc: 92.7928\n",
      "\n",
      "train loss: 0.12215877951891593, train acc: 98.6486\n",
      "\n",
      "train loss: 0.12077148800235582, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11930652500956582, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11791228558187161, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11651729917377056, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11515389656836404, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11382083215233307, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11260778645920709, train acc: 99.5495\n",
      "\n",
      "train loss: 0.1114760526993282, train acc: 99.5495\n",
      "\n",
      "train loss: 0.11025240465174459, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10909623999732979, train acc: 99.5495\n",
      "\n",
      "train loss: 0.10816535056279644, train acc: 99.5495\n",
      "\n",
      "train loss: 0.1070145021979747, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10628109619786694, train acc: 99.0991\n",
      "\n",
      "train loss: 0.1052289685196136, train acc: 99.5495\n",
      "\n",
      "train loss: 0.10412449197271713, train acc: 100.0000\n",
      "\n",
      "train loss: 0.10337667059983485, train acc: 99.0991\n",
      "\n",
      "train loss: 0.10260204747672644, train acc: 99.0991\n",
      "\n",
      "train loss: 0.10352844293902014, train acc: 98.1982\n",
      "\n",
      "train loss: 0.10506456897240476, train acc: 90.5405\n",
      "\n",
      "train loss: 0.10580171061527936, train acc: 94.1441\n"
     ]
    }
   ],
   "source": [
    "# Initialize training loss\n",
    "train_loss = []\n",
    "# Initialize training accuracy\n",
    "train_acc = []\n",
    "# Initialize total step\n",
    "total_step = len(train_dataloader)\n",
    "# Set number of epochs\n",
    "epoch_in = trange(100, desc='training')\n",
    "# Initialize best F1 Score value\n",
    "best_f1= 0\n",
    "\n",
    "# Start model training\n",
    "for epoch in epoch_in:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "\n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = train_data[0].to(device).float()\n",
    "        labels = train_data[1].to(device).long().squeeze()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Input to the model\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Set label predictions\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred == labels).item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss / total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "    \n",
    "    # Perform validation at the end of each epoch and save the model with the best performance\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "            inputs_v = valid_data[0].to(device).float()\n",
    "            labels_v = valid_data[1].to(device).long().squeeze() \n",
    "            \n",
    "            outputs_v = model(inputs_v)\n",
    "            \n",
    "            # Set label predictions\n",
    "            _, pred_v = torch.max(outputs_v, dim=1)\n",
    "            target_v = labels_v.view_as(pred_v)\n",
    "            \n",
    "            preds_.append(pred_v)\n",
    "            targets_.append(target_v)\n",
    "            \n",
    "        # Combine predictions and labels collected from all batches\n",
    "        preds_ = torch.cat(preds_).detach().cpu().numpy()\n",
    "        targets_ = torch.cat(targets_).detach().cpu().numpy()\n",
    "        \n",
    "        f1score = f1_score(targets_, preds_, average='macro')\n",
    "        if best_f1 < f1score:\n",
    "            best_f1 = f1score\n",
    "            # Save the best model\n",
    "            with open(\"./result/BCI1_ResNet1d_General.txt\", \"a\") as text_file:\n",
    "                print('epoch=====',epoch, file=text_file)\n",
    "                print(classification_report(targets_, preds_, digits=4), file=text_file)\n",
    "            torch.save(model, f'./result/BCI1_ResNet1d_General.pt') \n",
    "        epoch_in.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_ = torch.load(f'./result/BCI1_ResNet1d_General.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing\n",
    "preds_test = []\n",
    "target_test = []\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    for batch_idx, test_data in enumerate(test_dataloader):\n",
    "        inputs_t = test_data[0].to(device).float()\n",
    "        labels_t = test_data[1].to(device).long().squeeze()\n",
    "        \n",
    "        outputs_t = model_(inputs_t)\n",
    "        \n",
    "        _, pred_t = torch.max(outputs_t, dim=1)\n",
    "\n",
    "        preds_test.append(pred_t)\n",
    "        target_test.append(labels_t.item())\n",
    "        \n",
    "    # Combine predictions and labels collected from all batches\n",
    "    preds_test = torch.cat(preds_test).detach().cpu().numpy()\n",
    "\n",
    "# Create a DataFrame for test results\n",
    "final = pd.DataFrame(target_test, columns=['label'])\n",
    "final['pred'] = preds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84        16\n",
      "           1       0.77      0.83      0.80        12\n",
      "\n",
      "    accuracy                           0.82        28\n",
      "   macro avg       0.82      0.82      0.82        28\n",
      "weighted avg       0.82      0.82      0.82        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(final['label'].values, final['pred'].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
