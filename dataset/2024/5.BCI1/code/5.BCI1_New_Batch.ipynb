{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## 사용 라이브러리 호출\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.fft import fft\n",
    "\n",
    "## 모델 사용 라이브러리 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## 모델 학습 결과 경로 설정 \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Cuda 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## 랜덤 시드 설정\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 시드 설정 \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 데이터 컬럼 선택\n",
    "\n",
    "* tag name 이 많은 경우 tag name을 지정하는 것에 있어서 변수 설정이 다소 유연해짐\n",
    "* tag name 은 순서대로 불러와짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag name 출력 함수 \n",
    "def show_column(URL):\n",
    "    \n",
    "    # Tag name 데이터 로드\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # List 형식으로 변환\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tag name 출력 파라미터 설정\n",
    "table = 'bci1'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## tag name list 생성 \n",
    "tag_name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test-s-0',\n",
       " 'test-s-1',\n",
       " 'test-s-10',\n",
       " 'test-s-11',\n",
       " 'test-s-12',\n",
       " 'test-s-13',\n",
       " 'test-s-14',\n",
       " 'test-s-15',\n",
       " 'test-s-16',\n",
       " 'test-s-17',\n",
       " 'test-s-18',\n",
       " 'test-s-19',\n",
       " 'test-s-2',\n",
       " 'test-s-20',\n",
       " 'test-s-21',\n",
       " 'test-s-22',\n",
       " 'test-s-23',\n",
       " 'test-s-24',\n",
       " 'test-s-25',\n",
       " 'test-s-26',\n",
       " 'test-s-27',\n",
       " 'test-s-28',\n",
       " 'test-s-29',\n",
       " 'test-s-3',\n",
       " 'test-s-30',\n",
       " 'test-s-31',\n",
       " 'test-s-32',\n",
       " 'test-s-33',\n",
       " 'test-s-34',\n",
       " 'test-s-35',\n",
       " 'test-s-36',\n",
       " 'test-s-37',\n",
       " 'test-s-38',\n",
       " 'test-s-39',\n",
       " 'test-s-4',\n",
       " 'test-s-40',\n",
       " 'test-s-41',\n",
       " 'test-s-42',\n",
       " 'test-s-43',\n",
       " 'test-s-44',\n",
       " 'test-s-45',\n",
       " 'test-s-46',\n",
       " 'test-s-47',\n",
       " 'test-s-48',\n",
       " 'test-s-49',\n",
       " 'test-s-5',\n",
       " 'test-s-50',\n",
       " 'test-s-51',\n",
       " 'test-s-52',\n",
       " 'test-s-53',\n",
       " 'test-s-54',\n",
       " 'test-s-55',\n",
       " 'test-s-56',\n",
       " 'test-s-57',\n",
       " 'test-s-58',\n",
       " 'test-s-59',\n",
       " 'test-s-6',\n",
       " 'test-s-60',\n",
       " 'test-s-61',\n",
       " 'test-s-62',\n",
       " 'test-s-63',\n",
       " 'test-s-7',\n",
       " 'test-s-8',\n",
       " 'test-s-9',\n",
       " 'train-s-0',\n",
       " 'train-s-1',\n",
       " 'train-s-10',\n",
       " 'train-s-11',\n",
       " 'train-s-12',\n",
       " 'train-s-13',\n",
       " 'train-s-14',\n",
       " 'train-s-15',\n",
       " 'train-s-16',\n",
       " 'train-s-17',\n",
       " 'train-s-18',\n",
       " 'train-s-19',\n",
       " 'train-s-2',\n",
       " 'train-s-20',\n",
       " 'train-s-21',\n",
       " 'train-s-22',\n",
       " 'train-s-23',\n",
       " 'train-s-24',\n",
       " 'train-s-25',\n",
       " 'train-s-26',\n",
       " 'train-s-27',\n",
       " 'train-s-28',\n",
       " 'train-s-29',\n",
       " 'train-s-3',\n",
       " 'train-s-30',\n",
       " 'train-s-31',\n",
       " 'train-s-32',\n",
       " 'train-s-33',\n",
       " 'train-s-34',\n",
       " 'train-s-35',\n",
       " 'train-s-36',\n",
       " 'train-s-37',\n",
       " 'train-s-38',\n",
       " 'train-s-39',\n",
       " 'train-s-4',\n",
       " 'train-s-40',\n",
       " 'train-s-41',\n",
       " 'train-s-42',\n",
       " 'train-s-43',\n",
       " 'train-s-44',\n",
       " 'train-s-45',\n",
       " 'train-s-46',\n",
       " 'train-s-47',\n",
       " 'train-s-48',\n",
       " 'train-s-49',\n",
       " 'train-s-5',\n",
       " 'train-s-50',\n",
       " 'train-s-51',\n",
       " 'train-s-52',\n",
       " 'train-s-53',\n",
       " 'train-s-54',\n",
       " 'train-s-55',\n",
       " 'train-s-56',\n",
       " 'train-s-57',\n",
       " 'train-s-58',\n",
       " 'train-s-59',\n",
       " 'train-s-6',\n",
       " 'train-s-60',\n",
       " 'train-s-61',\n",
       " 'train-s-62',\n",
       " 'train-s-63',\n",
       " 'train-s-7',\n",
       " 'train-s-8',\n",
       " 'train-s-9',\n",
       " 'train-s-answer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAG Name format 변환 \n",
    "\n",
    "* 위의 과정에서 BCI1 dataset의 모든 Tag Name 을 확인후 사용할 컬럼만 뽑아서 입력할 파라미터 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'train-s-0','train-s-1','train-s-10','train-s-11','train-s-12','train-s-13','train-s-14','train-s-15','train-s-16','train-s-17','train-s-18','train-s-19','train-s-2','train-s-20','train-s-21','train-s-22','train-s-23','train-s-24','train-s-25','train-s-26','train-s-27','train-s-28','train-s-29','train-s-3','train-s-30','train-s-31','train-s-32','train-s-33','train-s-34','train-s-35','train-s-36','train-s-37','train-s-38','train-s-39','train-s-4','train-s-40','train-s-41','train-s-42','train-s-43','train-s-44','train-s-45','train-s-46','train-s-47','train-s-48','train-s-49','train-s-5','train-s-50','train-s-51','train-s-52','train-s-53','train-s-54','train-s-55','train-s-56','train-s-57','train-s-58','train-s-59','train-s-6','train-s-60','train-s-61','train-s-62','train-s-63','train-s-7','train-s-8','train-s-9','train-s-answer'\n"
     ]
    }
   ],
   "source": [
    "# 원하는 tag name 설정\n",
    "# 여기서 tag name 은 컬럼을 의미\n",
    "tags = tag_name[64:]\n",
    "\n",
    "# 리스트의 각 항목을 작은따옴표로 감싸고, 쉼표로 구분\n",
    "tags_= \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# 사용 tag name 확인\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BCI1 Dataset 로드\n",
    "\n",
    "* Tag Name 들을 사용하여 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(table, tag_name, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # 결과를 저장할 리스트\n",
    "    result_dfs = []\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "\n",
    "    # 같은 시간대 별 데이터로 전환\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # train-s-answer 값 분리\n",
    "    df_label = df.iloc[:, -1:].dropna()\n",
    "\n",
    "    # train-s-answer 컬럼 제거\n",
    "    df = df.iloc[:, :-1]\n",
    "\n",
    "    for col_name in tag_name[64:-1]:\n",
    "\n",
    "        # TIME 설정\n",
    "        df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "        # 3초 단위로 그룹화하여 데이터 개수 세기\n",
    "        df_counts = df.groupby(df['TIME'].dt.floor('3S')).size().reset_index(name='count')\n",
    "\n",
    "        # 데이터 개수가 동일한 그룹만 필터링\n",
    "        most_common_count = df_counts['count'].mode()[0]\n",
    "        filtered_df_counts = df_counts[df_counts['count'] == most_common_count]\n",
    "\n",
    "        # 필터링된 시간값들을 리스트로 변환\n",
    "        filtered_times = filtered_df_counts['TIME'].tolist()\n",
    "\n",
    "        # 원본 데이터프레임에서 필터링된 시간값들만 선택\n",
    "        filtered_data = df[df['TIME'].dt.floor('3S').isin(filtered_times)]\n",
    "\n",
    "        # TIME을 기준으로 그룹화 (3초 단위로 반올림)\n",
    "        filtered_data_ = filtered_data.copy()\n",
    "        filtered_data_['TIME'] = filtered_data_['TIME'].dt.floor('3S')\n",
    "        grouped = filtered_data_.groupby('TIME')[col_name].apply(list).reset_index()\n",
    "\n",
    "        # 리스트를 개별 열로 나누기\n",
    "        result_df = pd.DataFrame(grouped[col_name].tolist())\n",
    "\n",
    "        result_dfs.append(result_df)  # 결과 추가\n",
    "        \n",
    "    # 결과를 저장할 리스트 초기화\n",
    "    data_list = []\n",
    "    k = 0\n",
    "\n",
    "    for k in result_dfs:\n",
    "        \n",
    "        # array 형태로 변환\n",
    "        data = k.values\n",
    "        data_list.append(data)\n",
    "\n",
    "    # 리스트를 NumPy 배열로 변환\n",
    "    data_array = np.array(data_list)\n",
    "\n",
    "    # 필요한 형태로 reshape\n",
    "    # (데이터 개수, 64, 3000) 형태로 변환\n",
    "    reshaped_array = np.transpose(data_array, (1, 0, 2)) \n",
    "    \n",
    "    # train-s-answer 변경\n",
    "    df_label.loc[df_label['train-s-answer'] == -1.0, 'train-s-answer'] = 0\n",
    "    df_label['train-s-answer'] = df_label['train-s-answer'].astype(int)    \n",
    "\n",
    "    return reshaped_array, df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시간 로드 함수\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'time'\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # resample을 위해 임의의 value 컬럼 생성\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # resample 진행\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df = df.resample('3S').mean()\n",
    "    \n",
    "    # 결측값 제거\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # 임의의 value 컬럼 제거\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 변환 함수\n",
    "# 시간 추가하려면 해당 과정 필요\n",
    "# window_size : 데이터를 묶어서 수집되는 주기 \n",
    "# step_size : 데이터의 간격 \n",
    "def add_time(time_df, start_time, batch_size):\n",
    "    \n",
    "    # 몇개의 데이터를 로드해야 되는지 계산\n",
    "    time = batch_size\n",
    "    \n",
    "    # 현재 시간의 인덱스 번호를 확인\n",
    "    # 없는 경우는 맨처음 시간이 없는 경우이기 때문에 맨처음 인덱스로 지정함 \n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # 현재 시간 기준 배치 데이터의 마지막 시간 설정 \n",
    "    end_time_ = str(time_df.index[index_now + time] + timedelta(seconds=1))\n",
    "    \n",
    "    # 다음 시작 시간의 인덱스 번호 설정\n",
    "    index_next = index_now + time\n",
    "    \n",
    "    # 다음 시작 시간 설정\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL 인코딩\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanning window 함수 설정 \n",
    "def hanning_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT 변환 함수\n",
    "def change_fft(sample_rate, data):\n",
    "    # 신호의 총 샘플 수\n",
    "    N = sample_rate\n",
    "    \n",
    "    # 각 채널에 대해 FFT 결과를 저장할 배열 초기화\n",
    "    fft_results = np.zeros((data.shape[0], data.shape[1], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    # 전체 데이터에 대해 FFT 적용\n",
    "    for i in range(data.shape[0]):  # 각 샘플에 대해\n",
    "        for j in range(data.shape[1]):  # 각 채널에 대해\n",
    "            yf = fft(data[i, j], n=N)  # FFT 계산\n",
    "            # FFT 결과의 절댓값을 계산하고 정규화 (유의미한 부분만)\n",
    "            fft_results[i, j] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    return fft_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 모델 설정 \n",
    "\n",
    "* ResNet 1d 기본 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ResNet 1d 모델 설정 \n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Identity mapping\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64 \n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(64, 64, kernel_size=7, stride=2, padding=3) \n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정 파라미터\n",
    "# 학습률 \n",
    "learning_rate = 0.01\n",
    "\n",
    "# 모델 초기화\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=2).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 구조 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "\n",
    "* 필요한 배치 크기의 데이터만 불러와서 학습하는 방법으로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수 설정\n",
    "def train(table, tag_name, name, timeformat, model, start_time_train, end_time_train, start_time_valid, end_time_valid, batch_size_train, batch_size_valid, epochs, time_df_train, time_df_valid, pca, sample_rate):\n",
    "    \n",
    "    # 초기 train loss 설정\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    # 베스트 f1 값 초기화\n",
    "    best_f1= 0\n",
    "\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        # 학습 모드 설정 \n",
    "        model.train()\n",
    "        \n",
    "        # 초기 loss 초기화\n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        correct = 0\n",
    "        total=0\n",
    "        \n",
    "        # 초기 시작 시간 설정\n",
    "        start_time_ = start_time_train\n",
    "        \n",
    "        # 끝 시간 설정\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "        \n",
    "        # while 문을 통해 데이터 호출 \n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # 배치 크기에 따라 데이터 로드 \n",
    "            start_time_, end_time_, next_start_time_, index_next= add_time(time_df_train, start_time_, batch_size_train)\n",
    "        \n",
    "            # 데이터 로드 \n",
    "            data, label = data_load(table, tag_name, name, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # Hanning window 적용\n",
    "            data = data * hanning_window(sample_rate)\n",
    "            \n",
    "            # FFT 변환\n",
    "            data = change_fft(sample_rate, data)\n",
    "            \n",
    "            # PCA 적용\n",
    "            # 2차원으로 변환\n",
    "            data_ = data.reshape(-1, data.shape[2])\n",
    "            data_ = pca.fit_transform(data_)\n",
    "            # 다시 원래의 3차원 형태로 변환\n",
    "            data = data_.reshape(data.shape[0], data.shape[1], -1)\n",
    "            \n",
    "            # 로드한 데이터가 비어 있을 경우 출력 \n",
    "            if len(data) == 0:\n",
    "                print(\"데이터가 없습니다.\")\n",
    "            \n",
    "            # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "            if len(data) == batch_size_train:\n",
    "                \n",
    "                # 총 배치수 체크용  \n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # 데이터를 numpy 배열로 변환\n",
    "                input_data = np.array(data)\n",
    "                input_target = np.array(label)[:-1]\n",
    "\n",
    "                # 데이터를 Tensor로 변환\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "                input_target = torch.tensor(input_target, dtype=torch.float32).to(device).long().squeeze()\n",
    "                \n",
    "                # 옵티마이저 최적화 \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 모델 입력\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # loss 계산\n",
    "                loss = criterion(outputs, input_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "                # label 예측 값 설정 \n",
    "                _,pred = torch.max(outputs, dim=1)\n",
    "                correct += torch.sum(pred==input_target).item()\n",
    "                total += input_target.size(0)\n",
    "\n",
    "                # 배치 리셋\n",
    "                data = []\n",
    "                \n",
    "                # 다음 시작 시간 설정    \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "            \n",
    "            # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "            if index_next + batch_size_train >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "        train_acc.append(100 * correct / total)\n",
    "        train_loss.append(running_loss/total_step)\n",
    "        print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "        \n",
    "        # Epoch 마다 validation을 진행해서 가장 좋은 성능을 보이는 모델을 저장 \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            # 초기화\n",
    "            preds_v = []\n",
    "            targets_v = []\n",
    "                \n",
    "            # 초기 시작 시간 설정\n",
    "            start_time_v = start_time_valid\n",
    "            \n",
    "            # 끝 시간 설정\n",
    "            end_time_valid = str(time_df_valid.index[-1])\n",
    "            \n",
    "            # while 문을 통해 데이터 호출 \n",
    "            while start_time_v < end_time_valid:\n",
    "                \n",
    "                # 배치 크기에 따라 데이터 로드 \n",
    "                start_time_v, end_time_v, next_start_time_v, index_next_v = add_time(time_df_valid, start_time_v, batch_size_valid)\n",
    "                \n",
    "                # 데이터 로드 \n",
    "                data_v, label_v = data_load(table, tag_name, name, start_time_v, end_time_v, timeformat)\n",
    "                \n",
    "                # Hanning window 적용\n",
    "                data_v = data_v * hanning_window(sample_rate)\n",
    "                \n",
    "                # FFT 변환\n",
    "                data_v = change_fft(sample_rate, data_v)\n",
    "                \n",
    "                # PCA 적용\n",
    "                # 2차원으로 변환\n",
    "                data_ = data_v.reshape(-1, data_v.shape[2])\n",
    "                data_ = pca.fit_transform(data_)\n",
    "                # 다시 원래의 3차원 형태로 변환\n",
    "                data_v = data_.reshape(data_v.shape[0], data_v.shape[1], -1)\n",
    "\n",
    "                \n",
    "                # 로드한 데이터가 비어 있을 경우 출력 \n",
    "                if len(data_v) == 0:\n",
    "                    print(\"데이터가 없습니다.\")\n",
    "                \n",
    "                # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "                if len(data_v) == batch_size_valid:\n",
    "                    \n",
    "                    # 데이터를 numpy 배열로 변환\n",
    "                    input_data_v = np.array(data_v)\n",
    "                    input_target_v = np.array(label_v)[:-1]\n",
    "\n",
    "                    # 데이터를 Tensor로 변환\n",
    "                    input_data_v = torch.tensor(input_data_v, dtype=torch.float32).to(device).float()\n",
    "                    input_target_v = torch.tensor(input_target_v, dtype=torch.float32).to(device).long().squeeze()\n",
    "                    \n",
    "                    # 모델 입력\n",
    "                    outputs_v = model(input_data_v)\n",
    "                    \n",
    "                    # label 예측 값 설정 \n",
    "                    _,pred_v = torch.max(outputs_v, dim=1)\n",
    "                    target_v = input_target_v.view_as(pred_v)\n",
    "    \n",
    "                    preds_v.append(pred_v)\n",
    "                    targets_v.append(target_v)\n",
    "                    \n",
    "                    # 배치 리셋\n",
    "                    data_v = []\n",
    "                    \n",
    "                # 다음 시작 시간 설정    \n",
    "                start_time_v = unquote(next_start_time_v)\n",
    "                \n",
    "                # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "                if index_next_v + batch_size_valid > len(time_df_valid):\n",
    "                    break\n",
    "                    \n",
    "            # 모든 배치에서 수집된 예측과 라벨을 합침\n",
    "            preds_v = torch.cat(preds_v).detach().cpu().numpy()\n",
    "            targets_v = torch.cat(targets_v).detach().cpu().numpy()\n",
    "            \n",
    "            f1score = f1_score(targets_v, preds_v,  average='macro')\n",
    "            if best_f1 < f1score:\n",
    "                best_f1 = f1score\n",
    "                # 베스트 모델 저장 \n",
    "                with open(\"./result/BCI1_ResN et1d_New_Batch.txt\", \"a\") as text_file:\n",
    "                    print('epoch=====',epoch, file=text_file)\n",
    "                    print(classification_report(targets_v, preds_v, digits=4), file=text_file)\n",
    "                torch.save(model, f'./result/BCI1_ResNet1d_New_Batch.pt') \n",
    "            epochs.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62039d48f29a471cbd9e8496b3954dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.2080353131661048, train acc: 47.5962\n",
      "\n",
      "train loss: 1.1204177118264713, train acc: 49.0385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.0725246285780883, train acc: 53.3654\n",
      "\n",
      "train loss: 0.9887269127827425, train acc: 50.4808\n",
      "\n",
      "train loss: 0.949872727577503, train acc: 50.0000\n",
      "\n",
      "train loss: 0.9121195398844207, train acc: 51.9231\n",
      "\n",
      "train loss: 0.8853872693501987, train acc: 53.3654\n",
      "\n",
      "train loss: 0.8643282795181642, train acc: 52.8846\n",
      "\n",
      "train loss: 0.8463760217030843, train acc: 51.9231\n",
      "\n",
      "train loss: 0.8322952550191145, train acc: 57.2115\n",
      "\n",
      "train loss: 0.8204017089797067, train acc: 55.2885\n",
      "\n",
      "train loss: 0.8087945209863858, train acc: 62.0192\n",
      "\n",
      "train loss: 0.7943512659806472, train acc: 66.3462\n",
      "\n",
      "train loss: 0.7745996657963637, train acc: 76.9231\n",
      "\n",
      "train loss: 0.7561487672802729, train acc: 83.1731\n",
      "\n",
      "train loss: 0.7251674725602453, train acc: 89.9038\n",
      "\n",
      "train loss: 0.6985537067257981, train acc: 91.3462\n",
      "\n",
      "train loss: 0.669987006733815, train acc: 93.2692\n",
      "\n",
      "train loss: 0.6403646057826065, train acc: 97.1154\n",
      "\n",
      "train loss: 0.6117753831777148, train acc: 97.5962\n",
      "\n",
      "train loss: 0.5850418515865019, train acc: 98.5577\n",
      "\n",
      "train loss: 0.5604482062268429, train acc: 98.5577\n",
      "\n",
      "train loss: 0.5377730489926519, train acc: 98.5577\n",
      "\n",
      "train loss: 0.5168935519938643, train acc: 98.5577\n",
      "\n",
      "train loss: 0.49746735017818317, train acc: 98.5577\n",
      "\n",
      "train loss: 0.47909938898137294, train acc: 99.0385\n",
      "\n",
      "train loss: 0.46313272613361717, train acc: 99.5192\n",
      "\n",
      "train loss: 0.4481612666113305, train acc: 98.0769\n",
      "\n",
      "train loss: 0.43353777228746815, train acc: 99.5192\n",
      "\n",
      "train loss: 0.4194104757174125, train acc: 99.5192\n",
      "\n",
      "train loss: 0.40597733807634195, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3933227802537238, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3814200048212036, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3702125302082658, train acc: 100.0000\n",
      "\n",
      "train loss: 0.35964334927631053, train acc: 100.0000\n",
      "\n",
      "train loss: 0.34965998744536403, train acc: 100.0000\n",
      "\n",
      "train loss: 0.34021535755864585, train acc: 100.0000\n",
      "\n",
      "train loss: 0.33126711573934026, train acc: 100.0000\n",
      "\n",
      "train loss: 0.32277721497566325, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3147113956634292, train acc: 100.0000\n",
      "\n",
      "train loss: 0.3070386915517124, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2997310675202157, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2927630980623809, train acc: 100.0000\n",
      "\n",
      "train loss: 0.28611165182029114, train acc: 100.0000\n",
      "\n",
      "train loss: 0.27975565438797106, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2736758608596599, train acc: 100.0000\n",
      "\n",
      "train loss: 0.26785465513876117, train acc: 100.0000\n",
      "\n",
      "train loss: 0.26227588894402615, train acc: 100.0000\n",
      "\n",
      "train loss: 0.25692473032983487, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2517875346649984, train acc: 100.0000\n",
      "\n",
      "train loss: 0.24685172049206672, train acc: 100.0000\n",
      "\n",
      "train loss: 0.24210567870824845, train acc: 100.0000\n",
      "\n",
      "train loss: 0.237538672366774, train acc: 100.0000\n",
      "\n",
      "train loss: 0.23314076106622747, train acc: 100.0000\n",
      "\n",
      "train loss: 0.2289027260119316, train acc: 100.0000\n",
      "\n",
      "train loss: 0.224816006190442, train acc: 100.0000\n",
      "\n",
      "train loss: 0.22087264091739586, train acc: 100.0000\n",
      "\n",
      "train loss: 0.21706521839188345, train acc: 100.0000\n",
      "\n",
      "train loss: 0.21338682850566476, train acc: 100.0000\n",
      "\n",
      "train loss: 0.20983102181320504, train acc: 100.0000\n",
      "\n",
      "train loss: 0.20639177172671072, train acc: 100.0000\n",
      "\n",
      "train loss: 0.20306344058107068, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19984074743197408, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19671874155410216, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1936927766268762, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19075848904528395, train acc: 100.0000\n",
      "\n",
      "train loss: 0.18791177476039836, train acc: 100.0000\n",
      "\n",
      "train loss: 0.18514877192853926, train acc: 100.0000\n",
      "\n",
      "train loss: 0.18246584166075364, train acc: 100.0000\n",
      "\n",
      "train loss: 0.17985955280781946, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1773266680004792, train acc: 100.0000\n",
      "\n",
      "train loss: 0.17486412952448457, train acc: 100.0000\n",
      "\n",
      "train loss: 0.17246904659207463, train acc: 100.0000\n",
      "\n",
      "train loss: 0.170138685416932, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1678704573885065, train acc: 100.0000\n",
      "\n",
      "train loss: 0.16566191057623278, train acc: 100.0000\n",
      "\n",
      "train loss: 0.16351072019925536, train acc: 100.0000\n",
      "\n",
      "train loss: 0.16141468063758352, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15937169823793643, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15737978346470438, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1554370445985571, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15354168387623357, train acc: 100.0000\n",
      "\n",
      "train loss: 0.15169198789666352, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1498863265669757, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14812314618807104, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14640096484674342, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14471836882229572, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14307400894734817, train acc: 100.0000\n",
      "\n",
      "train loss: 0.14146659644680284, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13989490007247965, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1383577423444797, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1368539974137492, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13538258745423642, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1339424806145482, train acc: 100.0000\n",
      "\n",
      "train loss: 0.13253268870097312, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1311522641511136, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12980029909764537, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1284759221503926, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12717829752597948, train acc: 100.0000\n",
      "\n",
      "train loss: 0.12590662289538018, train acc: 100.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### 학습 파라미터 설정 ################################################\n",
    "# tag table 이름 설정\n",
    "table = 'bci1'\n",
    "# tag name 설정\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# 시간 포멧 설정 \n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# 학습 데이터 시작 시간 설정\n",
    "start_time_train = '2024-01-01 00:00:00'\n",
    "# 학습 데이터  끝 시간 설정\n",
    "end_time_train = '2024-01-01 03:41:00'\n",
    "# 배치 사이즈 설정\n",
    "batch_size_train = 16\n",
    "batch_size_valid = 16\n",
    "# epoch 설정\n",
    "epochs = trange(100, desc='training')\n",
    "# sample rate 설정\n",
    "sample_rate = 3000\n",
    "# PCA 설정\n",
    "# 95%의 분산을 설명하는 주성분 선택\n",
    "pca = PCA(n_components=14)\n",
    "# 학습 시간 리스트 로드 \n",
    "time_df_train = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "########################################### 검증 파라미터 설정 ################################################\n",
    "# 검증 데이터 시작 시간 설정\n",
    "start_time_valid = '2024-01-01 03:42:00'\n",
    "# 검증 데이터  끝 시간 설정\n",
    "end_time_valid = '2024-01-01 04:09:00'\n",
    "# 검증 시간 리스트 로드 \n",
    "time_df_valid = time_data_load(table, name, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "\n",
    "######################################## 학습 진행 #############################################\n",
    "train(table, tag_name, name, timeformat, model, start_time_train, end_time_train, start_time_valid, end_time_valid, batch_size_train, batch_size_valid, epochs, time_df_train, time_df_valid, pca, sample_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "model_ = torch.load(f'./result/BCI1_ResNet1d_New_Batch.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 테스트 함수 설정\n",
    "def test(table, tag_name, name, timeformat, model, start_time_test, end_time_test, batch_size, time_df_test, pca, sample_rate):\n",
    "    \n",
    "            # Epoch 마다 testation을 진행해서 가장 좋은 성능을 보이는 모델을 저장 \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                model.eval()\n",
    "                \n",
    "                # 초기화\n",
    "                preds_t = []\n",
    "                targets_t = []\n",
    "                    \n",
    "                # 초기 시작 시간 설정\n",
    "                start_time_t = start_time_test\n",
    "                \n",
    "                # 끝 시간 설정\n",
    "                end_time_test = str(time_df_test.index[-1])\n",
    "                \n",
    "                # while 문을 통해 데이터 호출 \n",
    "                while start_time_t < end_time_test:\n",
    "                    \n",
    "                    # 배치 크기에 따라 데이터 로드 \n",
    "                    start_time_t, end_time_t, next_start_time_t, index_next_t = add_time(time_df_test, start_time_t, batch_size)\n",
    "                    \n",
    "                    # 데이터 로드 \n",
    "                    data_t, label_t = data_load(table, tag_name, name, start_time_t, end_time_t, timeformat)\n",
    "                    \n",
    "                    # Hanning window 적용\n",
    "                    data_t = data_t * hanning_window(sample_rate)\n",
    "                    \n",
    "                    # FFT 변환\n",
    "                    data_t = change_fft(sample_rate, data_t)\n",
    "                    \n",
    "                    # PCA 적용\n",
    "                    # 2차원으로 변환\n",
    "                    data_ = data_t.reshape(-1, data_t.shape[2])\n",
    "                    data_ = pca.fit_transform(data_)\n",
    "                    # 다시 원래의 3차원 형태로 변환\n",
    "                    data_t = data_.reshape(data_t.shape[0], data_t.shape[1], -1)\n",
    "\n",
    "                    \n",
    "                    # 로드한 데이터가 비어 있을 경우 출력 \n",
    "                    if len(data_t) == 0:\n",
    "                        print(\"데이터가 없습니다.\")\n",
    "                    \n",
    "                    # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "                    if len(data_t) == batch_size:\n",
    "                        \n",
    "                        # 데이터를 numpy 배열로 변환\n",
    "                        input_data_t = np.array(data_t)\n",
    "                        input_target_t = np.array(label_t)[:-1]\n",
    "\n",
    "                        # 데이터를 Tensor로 변환\n",
    "                        input_data_t = torch.tensor(input_data_t, dtype=torch.float32).to(device).float()\n",
    "                        input_target_t = torch.tensor(input_target_t, dtype=torch.float32).to(device).long().squeeze()\n",
    "                        \n",
    "                        # 모델 입력\n",
    "                        outputs_t = model(input_data_t)\n",
    "                        \n",
    "                        # label 예측 값 설정 \n",
    "                        _,pred_t = torch.max(outputs_t, dim=1)\n",
    "                        target_t = input_target_t.view_as(pred_t)\n",
    "        \n",
    "                        preds_t.append(pred_t)\n",
    "                        targets_t.append(target_t)\n",
    "                        \n",
    "                        # 배치 리셋\n",
    "                        data_t = []\n",
    "                        \n",
    "                    # 다음 시작 시간 설정    \n",
    "                    start_time_t = unquote(next_start_time_t)\n",
    "                    \n",
    "                    # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "                    if index_next_t + batch_size + 1 > len(time_df_test):\n",
    "                        break\n",
    "                        \n",
    "                # 모든 배치에서 수집된 예측과 라벨을 합침\n",
    "                preds_t = torch.cat(preds_t).detach().cpu().numpy()\n",
    "                targets_t = torch.cat(targets_t).detach().cpu().numpy()\n",
    "                    \n",
    "            return targets_t, preds_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## 테스트 파라미터 설정 #############################################\n",
    "\n",
    "# 테스트 데이터 시작 시간 설정\n",
    "start_time_test = '2024-01-01 04:10:00'\n",
    "# 테스트 데이터  끝 시간 설정\n",
    "end_time_test = '2024-01-01 04:37:01'\n",
    "# 배치 사이즈 설정\n",
    "batch_size_test = 4\n",
    "\n",
    "# 검증 시간 리스트 로드\n",
    "time_df_test = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "######################################## 테스트 진행 #############################################\n",
    "targets_t, preds_t = test(table, tag_name, name, timeformat, model_, start_time_test, end_time_test, batch_size_test, time_df_test, pca, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78        14\n",
      "           1       0.67      1.00      0.80        10\n",
      "\n",
      "    accuracy                           0.79        24\n",
      "   macro avg       0.83      0.82      0.79        24\n",
      "weighted avg       0.86      0.79      0.79        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(targets_t, preds_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
