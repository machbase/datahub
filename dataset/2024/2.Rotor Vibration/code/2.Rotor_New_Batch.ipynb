{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## 사용 라이브러리 호출\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from datetime import timedelta\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## 모델 사용 라이브러리 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## 모델 학습 결과 경로 설정 \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Cuda 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## 랜덤 시드 설정\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 시드 설정 \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 데이터 컬럼 선택\n",
    "\n",
    "* tag name 이 많은 경우 tag name을 지정하는 것에 있어서 변수 설정이 다소 유연해짐\n",
    "* tag name 은 순서대로 불러와짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag name 출력 함수 \n",
    "def show_column(URL):\n",
    "    \n",
    "    # Tag name 데이터 로드\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # List 형식으로 변환\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tag name 출력 파라미터 설정\n",
    "table = 'rotor'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## tag name list 생성 \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g1_sensor1_normal',\n",
       " 'g1_sensor1_type1',\n",
       " 'g1_sensor1_type2',\n",
       " 'g1_sensor1_type3',\n",
       " 'g1_sensor2_normal',\n",
       " 'g1_sensor2_type1',\n",
       " 'g1_sensor2_type2',\n",
       " 'g1_sensor2_type3',\n",
       " 'g1_sensor3_normal',\n",
       " 'g1_sensor3_type1',\n",
       " 'g1_sensor3_type2',\n",
       " 'g1_sensor3_type3',\n",
       " 'g1_sensor4_normal',\n",
       " 'g1_sensor4_type1',\n",
       " 'g1_sensor4_type2',\n",
       " 'g1_sensor4_type3',\n",
       " 'g2_sensor1_normal',\n",
       " 'g2_sensor1_type1',\n",
       " 'g2_sensor1_type2',\n",
       " 'g2_sensor1_type3',\n",
       " 'g2_sensor2_normal',\n",
       " 'g2_sensor2_type1',\n",
       " 'g2_sensor2_type2',\n",
       " 'g2_sensor2_type3',\n",
       " 'g2_sensor3_normal',\n",
       " 'g2_sensor3_type1',\n",
       " 'g2_sensor3_type2',\n",
       " 'g2_sensor3_type3',\n",
       " 'g2_sensor4_normal',\n",
       " 'g2_sensor4_type1',\n",
       " 'g2_sensor4_type2',\n",
       " 'g2_sensor4_type3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAG Name format 변환 \n",
    "\n",
    "* 위의 과정에서 rotor dataset의 모든 Tag Name 을 확인후 사용할 컬럼만 뽑아서 입력할 파라미터 형태로 변환\n",
    "\n",
    "* g1 Tag Name 사용하여 예제 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'g1_sensor1_normal','g1_sensor1_type1','g1_sensor1_type2','g1_sensor1_type3','g1_sensor2_normal','g1_sensor2_type1','g1_sensor2_type2','g1_sensor2_type3','g1_sensor3_normal','g1_sensor3_type1','g1_sensor3_type2','g1_sensor3_type3','g1_sensor4_normal','g1_sensor4_type1','g1_sensor4_type2','g1_sensor4_type3'\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# 원하는 tag name 설정\n",
    "# 여기서 tag name 은 컬럼을 의미\n",
    "tags = name[:16]\n",
    "\n",
    "# 리스트의 각 항목을 작은따옴표로 감싸고, 쉼표로 구분\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# 사용 tag name 확인\n",
    "print(tags_)\n",
    "\n",
    "# 해당 값을 모델의 input shape로 설정 \n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotor Dataset 로드\n",
    "\n",
    "* 데이터 로드시 전체 데이터 셋을 각각 Load\n",
    "\n",
    "* label 설명\n",
    "    * normal : 정상\n",
    "    * type1 : Disk 2에 회전 불균형 (270도 위치에 볼트, 너트 부착)\n",
    "    * type2 : Support 4에 지지 불균형\n",
    "    * type3 : Type 1 + Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시간 로드 함수\n",
    "def time_data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    target = 'time'\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f\"http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?target={target}&table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}\")\n",
    "    \n",
    "    # resample을 위해 임의의 value 컬럼 생성\n",
    "    df['value'] = 0\n",
    "    \n",
    "    # resample 진행\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df = df.resample('1s').mean()\n",
    "    \n",
    "    # 결측값 제거\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # 임의의 value 컬럼 제거\n",
    "    df = df.drop(['value'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 이름에서 레이블 추출 함수\n",
    "def get_label(column_name):\n",
    "    \n",
    "    # 레이블 매핑\n",
    "    label_mapping = {\n",
    "        'normal': 0,\n",
    "        'type1': 1,\n",
    "        'type2': 2,\n",
    "        'type3': 3\n",
    "    }\n",
    "    \n",
    "    column_name = str(column_name)\n",
    "    for key in label_mapping.keys():\n",
    "        if key in column_name:\n",
    "            return label_mapping[key]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 함수\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # 같은 시간대 별 데이터로 전환\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # time 설정\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # 빈 데이터 프레임 생성\n",
    "    data_result = pd.DataFrame()\n",
    "\n",
    "    # 각 tag name 별 데이터 보간\n",
    "\n",
    "    for i in range(len(df.columns[1:])):\n",
    "        \n",
    "        # 시간 설정\n",
    "        start = pd.to_datetime(unquote(start_time))\n",
    "        end = pd.to_datetime(unquote(end_time))\n",
    "        \n",
    "        data_ = df.iloc[ : , [0] + list(range(i+1, i+2))].dropna()\n",
    "        \n",
    "        # 보간할 새로운 시간 구간 생성 (1초마다 1000개의 포인트)\n",
    "        # 이 경우, 원본 데이터는 1ms 간격으로 측정되었으므로 1초 간격으로 1000개의 포인트를 생성\n",
    "        # 0에서 140 구간을 생성 -> 140,000\n",
    "        new_time_range = pd.date_range(start=start, end=end, freq='1ms')[:-1]\n",
    "        new_time_range_ = pd.date_range(start=start, end=end, freq='1s')[:-1]\n",
    "\n",
    "        # 선형 보간을 사용하여 데이터 보간\n",
    "        # datetime을 숫자형으로 변환 (epoch time in seconds)\n",
    "        time_numeric = pd.to_numeric(data_['TIME'])\n",
    "        new_time_numeric = pd.to_numeric(new_time_range)\n",
    "\n",
    "        value = data_[data_.columns[1:].values]\n",
    "\n",
    "        # 선형 보간 객체 생성\n",
    "        interpolator = interp1d(time_numeric, value.values.reshape(-1), kind='linear', fill_value='extrapolate')\n",
    "        interpolated_values = interpolator(new_time_numeric)\n",
    "        interpolated_values = np.clip(interpolated_values, min(value.values), max(value.values))\n",
    "\n",
    "        # 데이터 프레임 생성\n",
    "        data_remake = pd.DataFrame(interpolated_values.reshape(-1,1000))\n",
    "        data_remake['time'] = new_time_range_\n",
    "        data_remake['sensor'] = f'{data_.columns[1:].item()}'\n",
    "\n",
    "        # 이동할 컬럼과 새로운 순서 지정\n",
    "        cols = data_remake.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('time')))\n",
    "        cols.insert(1, cols.pop(cols.index('sensor')))\n",
    "        data_remake = data_remake[cols]\n",
    "\n",
    "        # 빈 데이터 프레임에 추가\n",
    "        data_result = pd.concat([data_result, data_remake], ignore_index=True)\n",
    "        \n",
    "    # 시간순 정렬\n",
    "    data_result = data_result.sort_values(by='time').reset_index(drop=True)\n",
    "    \n",
    "    # label 설정 \n",
    "    # 각 컬럼에 대해 레이블을 적용하여 새로운 시리즈 생성\n",
    "    labels = pd.Series(data_result['sensor']).map(get_label)\n",
    "\n",
    "    # 데이터 프레임에 label 추가\n",
    "    data_result['label'] = labels.values\n",
    "\n",
    "    # 시간, 센서 이름 컬럼 제거\n",
    "    data_result = data_result.iloc[:,2:]\n",
    "        \n",
    "    return data_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 변환 함수\n",
    "# 시간 추가하려면 해당 과정 필요\n",
    "def add_time(time_df, start_time, batch_size):\n",
    "    \n",
    "    # 몇개의 데이터를 로드해야 되는지 계산\n",
    "    time = int(batch_size / 16)\n",
    "    \n",
    "    # 현재 시간의 인덱스 번호를 확인\n",
    "    # 없는 경우는 맨처음 시간이 없는 경우이기 때문에 맨처음 인덱스로 지정함 \n",
    "    try:\n",
    "        index_now = time_df.index.get_loc(start_time)\n",
    "    except KeyError:\n",
    "        index_now = 0\n",
    "    \n",
    "    # 현재 시간 기준 배치 데이터의 마지막 시간 설정 \n",
    "    end_time_ = str(time_df.index[index_now + time])\n",
    "    \n",
    "    # 다음 시작 시간의 인덱스 번호 설정\n",
    "    index_next = index_now + time\n",
    "    \n",
    "    # 다음 시작 시간 설정\n",
    "    next_start_time_ = str(time_df.index[index_next])\n",
    "    \n",
    "    # URL 인코딩\n",
    "    start_time_ = quote(start_time)\n",
    "    end_time_ = quote(end_time_)\n",
    "    next_start_time_ = quote(next_start_time_)\n",
    "    \n",
    "    return start_time_, end_time_, next_start_time_, index_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "   * hanning window, FFT, MinMaxScaling 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hanning window 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanning window 함수 설정 \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Hanning 윈도우 생성\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # 각 행에 Hanning 윈도우 적용\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT 변환 함수\n",
    "def change_fft(sample_rate, df):\n",
    "    # 신호의 총 샘플 수\n",
    "    N = sample_rate\n",
    "    \n",
    "    # 각 행에 대해 FFT 적용\n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        # 각 행의 FFT 계산\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # FFT 결과의 절댓값을 계산하고 정규화 (유의미한 부분만)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # FFT 결과를 데이터 프레임으로 변환\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 모델 설정 \n",
    "\n",
    "* 1D ResNet 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D ResNet 모델 사용 \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=4):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet1D(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정 파라미터\n",
    "# 학습률 \n",
    "learning_rate = 0.01\n",
    "\n",
    "# 모델 초기화\n",
    "model = ResNet1D(ResidualBlock, [2, 2, 2, 2], num_classes=4).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 구조 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 설정\n",
    "\n",
    "* 학습 중 validation 데이터 기준 F1 score 값이 제일 높은 모델을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수 설정\n",
    "def train(table, name, timeformat, model, start_time_train, end_time_train, start_time_valid, end_time_valid, batch_size, epochs, scaler, time_df_train, time_df_valid, sample_rate):\n",
    "    \n",
    "    # 초기 train loss 설정\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    # 베스트 f1 값 초기화\n",
    "    best_f1= -np.inf\n",
    "\n",
    "    for epoch in epochs:\n",
    "        \n",
    "        # 학습 모드 설정 \n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_step = 0\n",
    "        correct = 0\n",
    "        total=0\n",
    "\n",
    "        # 초기 시작 시간 설정\n",
    "        start_time_ = start_time_train\n",
    "\n",
    "        # 끝 시간 설정\n",
    "        end_time_train = str(time_df_train.index[-1])\n",
    "\n",
    "        # while 문을 통해 데이터 호출 \n",
    "        while start_time_ < end_time_train:\n",
    "            \n",
    "            # 배치 크기에 따라 데이터 로드 \n",
    "            start_time_, end_time_, next_start_time_, index_next= add_time(time_df_train, start_time_, batch_size)\n",
    "            \n",
    "            # 데이터 로드 \n",
    "            data = data_load(table, name, start_time_, end_time_, timeformat)\n",
    "            \n",
    "            # hanning window 적용\n",
    "            data_ = set_hanning_window(sample_rate, data.iloc[:,:-1])\n",
    "            \n",
    "            # FFT 적용 \n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # MinMax scaler 적용 \n",
    "            data_ = scaler.fit_transform(data_)\n",
    "            \n",
    "            # 데이터 프레임 + label 설정\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data['label'].values\n",
    "            \n",
    "            # 데이터 랜덤 셔플\n",
    "            data_ = data_.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            # 로드한 데이터가 비어 있을 경우 출력 \n",
    "            if len(data_) == 0:\n",
    "                print(\"데이터가 없습니다.\")\n",
    "            \n",
    "            # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "            if len(data_) == batch_size:\n",
    "                \n",
    "                # 총 배치수 체크용  \n",
    "                total_step = total_step + 1\n",
    "                \n",
    "                # 데이터를 numpy 배열로 변환\n",
    "                input_data = np.array(data_.iloc[:,:-1]).reshape(batch_size, 1 , -1)\n",
    "                label = np.array(data_.iloc[:,-1:])\n",
    "\n",
    "                # 데이터를 Tensor로 변환\n",
    "                input_data = torch.tensor(input_data, dtype=torch.float32).to(device).float()\n",
    "                label = torch.tensor(label).to(device).long().squeeze()\n",
    "\n",
    "                # 옵티마이저 최적화 \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 모델 입력\n",
    "                outputs = model(input_data)\n",
    "                \n",
    "                # loss 계산\n",
    "                loss = criterion(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # label 예측 값 설정 \n",
    "                _,pred = torch.max(outputs, dim=1)\n",
    "                correct += torch.sum(pred==label).item()\n",
    "                total += label.size(0)\n",
    "                \n",
    "                # 배치 리셋\n",
    "                data_ = 0\n",
    "                \n",
    "            # 다음 시작 시간 설정    \n",
    "            start_time_ = unquote(next_start_time_)\n",
    "                \n",
    "            # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "            if index_next + (int(batch_size /16)) >= len(time_df_train):\n",
    "                break\n",
    "            \n",
    "        train_acc.append(100 * correct / total)\n",
    "        train_loss.append(running_loss/total_step)\n",
    "        print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "\n",
    "        # Epoch 마다 validation을 진행해서 가장 좋은 성능을 보이는 모델을 저장 \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            # 초기화\n",
    "            preds_ = []\n",
    "            targets_ = []\n",
    "                \n",
    "            # 초기 시작 시간 설정\n",
    "            start_time_v = start_time_valid\n",
    "            \n",
    "            # 끝 시간 설정\n",
    "            end_time_valid = str(time_df_valid.index[-1])\n",
    "            \n",
    "            # while 문을 통해 데이터 호출 \n",
    "            while start_time_v < end_time_valid:\n",
    "                \n",
    "                # 배치 크기에 따라 데이터 로드 \n",
    "                start_time_v, end_time_v, next_start_time_v, index_next_v = add_time(time_df_valid, start_time_v, batch_size)\n",
    "                \n",
    "                # 데이터 로드 \n",
    "                data_v = data_load(table, name, start_time_v, end_time_v, timeformat)\n",
    "                \n",
    "                # hanning window 적용\n",
    "                data_ = set_hanning_window(sample_rate, data_v.iloc[:,:-1])\n",
    "                \n",
    "                # FFT 적용 \n",
    "                data_  = change_fft(sample_rate, data_ )\n",
    "                \n",
    "                # MinMax scaler 적용 \n",
    "                data_ = scaler.fit_transform(data_)\n",
    "                \n",
    "                # 데이터 프레임 + label 설정\n",
    "                data_ = pd.DataFrame(data_)\n",
    "                data_['label'] = data_v['label'].values\n",
    "                \n",
    "                # 데이터 랜덤 셔플\n",
    "                data_ = data_.sample(frac=1).reset_index(drop=True)\n",
    "                \n",
    "                # 로드한 데이터가 비어 있을 경우 출력 \n",
    "                if len(data_) == 0:\n",
    "                    print(\"데이터가 없습니다.\")\n",
    "                \n",
    "                # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "                if len(data_) == batch_size:\n",
    "                    \n",
    "                    # 데이터를 numpy 배열로 변환\n",
    "                    input_data_v = np.array(data_.iloc[:,:-1]).reshape(batch_size, 1 , -1)\n",
    "                    label_v = np.array(data_.iloc[:,-1:])\n",
    "\n",
    "                    # 데이터를 Tensor로 변환\n",
    "                    input_data_v = torch.tensor(input_data_v, dtype=torch.float32).to(device).float()\n",
    "                    label_v = torch.tensor(label_v).to(device).long().squeeze()\n",
    "                    \n",
    "                    # 모델 입력\n",
    "                    outputs_v = model(input_data_v)\n",
    "                    \n",
    "                    # label 예측 값 설정 \n",
    "                    _,pred_v = torch.max(outputs_v, dim=1)\n",
    "                    target_v = label_v.view_as(pred_v)\n",
    "      \n",
    "                    preds_.append(pred_v)\n",
    "                    targets_.append(target_v)\n",
    "                    \n",
    "                    # 배치 리셋\n",
    "                    data_ = 0\n",
    "                    \n",
    "                # 다음 시작 시간 설정    \n",
    "                start_time_v = unquote(next_start_time_v)\n",
    "                \n",
    "                # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "                if index_next_v + (int(batch_size /16)) >= len(time_df_valid):\n",
    "                    break\n",
    "                    \n",
    "            # 모든 배치에서 수집된 예측과 라벨을 합침\n",
    "            preds_v = torch.cat(preds_).detach().cpu().numpy()\n",
    "            targets_v = torch.cat(targets_).detach().cpu().numpy()\n",
    "            \n",
    "            f1score = f1_score(targets_v, preds_v,  average='macro')\n",
    "            if best_f1 < f1score:\n",
    "                best_f1 = f1score\n",
    "                # 베스트 모델 저장 \n",
    "                with open(\"./result/rotor_1d_ResNet_New_Batch.txt\", \"a\") as text_file:\n",
    "                    print('epoch=====',epoch, file=text_file)\n",
    "                    print(classification_report(targets_v, preds_v, digits=4), file=text_file)\n",
    "                torch.save(model, f'./result/rotor_1d_ResNet_New_Batch.pt') \n",
    "            epochs.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")\n",
    "               \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fa7c8610884fda971062cec94e2580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.1887357015998996, train acc: 49.1071\n",
      "\n",
      "train loss: 1.0850530114709114, train acc: 53.9541\n",
      "\n",
      "train loss: 0.9714551280144931, train acc: 66.5179\n",
      "\n",
      "train loss: 0.8701754094918771, train acc: 76.5306\n",
      "\n",
      "train loss: 0.7768268527349038, train acc: 83.8648\n",
      "\n",
      "train loss: 0.698825776279226, train acc: 88.4566\n",
      "\n",
      "train loss: 0.6311774903788903, train acc: 92.5383\n",
      "\n",
      "train loss: 0.5699560408342668, train acc: 94.7704\n",
      "\n",
      "train loss: 0.524442224559013, train acc: 94.0689\n",
      "\n",
      "train loss: 0.4860002517110991, train acc: 94.8980\n",
      "\n",
      "train loss: 0.45642082132347245, train acc: 94.1964\n",
      "\n",
      "train loss: 0.42380084072716445, train acc: 97.5765\n",
      "\n",
      "train loss: 0.39790750064295344, train acc: 97.5765\n",
      "\n",
      "train loss: 0.3710970530060219, train acc: 99.1709\n",
      "\n",
      "train loss: 0.3482742648154377, train acc: 98.9796\n",
      "\n",
      "train loss: 0.3296967620554657, train acc: 97.8316\n",
      "\n",
      "train loss: 0.31148497769011296, train acc: 99.3622\n",
      "\n",
      "train loss: 0.2973461289206616, train acc: 98.2781\n",
      "\n",
      "train loss: 0.28358842209458396, train acc: 98.5969\n",
      "\n",
      "train loss: 0.27152728677774207, train acc: 98.3418\n",
      "\n",
      "train loss: 0.2633562314355221, train acc: 96.8112\n",
      "\n",
      "train loss: 0.2552190486838397, train acc: 97.0026\n",
      "\n",
      "train loss: 0.24600047294154642, train acc: 98.2781\n",
      "\n",
      "train loss: 0.23684274839115615, train acc: 98.7245\n",
      "\n",
      "train loss: 0.22813682625344178, train acc: 99.2985\n",
      "\n",
      "train loss: 0.22009997571283357, train acc: 99.6173\n",
      "\n",
      "train loss: 0.2122686017950667, train acc: 99.6173\n",
      "\n",
      "train loss: 0.20474764972681947, train acc: 100.0000\n",
      "\n",
      "train loss: 0.1976956365901394, train acc: 100.0000\n",
      "\n",
      "train loss: 0.19111090096416064, train acc: 100.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet1D(\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### 학습 파라미터 설정 ################################################\n",
    "# tag table 이름 설정\n",
    "table = 'rotor'\n",
    "# tag name 설정\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# 학습 시작 시간 설정\n",
    "start_time_train = '2024-01-01 00:00:00'\n",
    "# 학습 끝 시간 설정\n",
    "end_time_train = '2024-01-01 00:01:39'\n",
    "# 시간 포멧 설정 \n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# 배치 사이즈 설정\n",
    "batch_size = 16\n",
    "# epoch 설정\n",
    "epochs = trange(30, desc='training')\n",
    "# sample rate 설정\n",
    "sample_rate = 1000\n",
    "# Min-Max scaler 설정 \n",
    "scaler = MinMaxScaler()\n",
    "# 학습 시간 리스트 로드 \n",
    "time_df_train = time_data_load(table, name, quote(start_time_train), quote(end_time_train), timeformat)\n",
    "\n",
    "######################################## validation 파라미터 설정 #############################################\n",
    "\n",
    "# 검증 시작 시간 설정\n",
    "start_time_valid = '2024-01-01 00:01:39'\n",
    "# 검증 끝 시간 설정\n",
    "end_time_valid = '2024-01-01 00:02:00'\n",
    "\n",
    "# 검증 시간 리스트 로드\n",
    "time_df_valid = time_data_load(table, name, quote(start_time_valid), quote(end_time_valid), timeformat)\n",
    "\n",
    "################################################ 학습 진행 ####################################################\n",
    "train(table, name, timeformat, model, start_time_train, end_time_train, start_time_valid, end_time_valid, batch_size, epochs, scaler, time_df_train, time_df_valid, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "model_ = torch.load(f'./result/rotor_1d_ResNet_New_Batch.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 테스트 함수 \n",
    "## 테스트 함수 \n",
    "def test(table, name, timeformat, model, start_time_test, end_time_test, batch_size, sample_rate, scaler, time_df_test):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # 초기 시작 시간 설정\n",
    "        start_time_t = start_time_test\n",
    "        \n",
    "        # 끝 시간 설정\n",
    "        end_time_test = str(time_df_test.index[-1])\n",
    "        \n",
    "        # label 리스트 설정\n",
    "        preds_test = []\n",
    "        target_test = []\n",
    "        \n",
    "        # while 문을 통해 데이터 호출 \n",
    "        while start_time_t < end_time_test:\n",
    "            \n",
    "            # 배치 크기에 따라 데이터 로드 \n",
    "            start_time_t, end_time_t, next_start_time_t, index_next_t = add_time(time_df_test, start_time_t, batch_size)\n",
    "            \n",
    "            # 데이터 로드 \n",
    "            data_v = data_load(table, name, start_time_t, end_time_t, timeformat)\n",
    "            \n",
    "            # hanning window 적용\n",
    "            data_ = set_hanning_window(sample_rate, data_v.iloc[:,:-1])\n",
    "            \n",
    "            # FFT 적용 \n",
    "            data_  = change_fft(sample_rate, data_ )\n",
    "            \n",
    "            # MinMax scaler 적용 \n",
    "            data_ = scaler.fit_transform(data_)\n",
    "            \n",
    "            # 데이터 프레임 + label 설정\n",
    "            data_ = pd.DataFrame(data_)\n",
    "            data_['label'] = data_v['label'].values\n",
    "            \n",
    "            # 데이터 랜덤 셔플\n",
    "            data_ = data_.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            # 로드한 데이터가 비어 있을 경우 출력 \n",
    "            if len(data_) == 0:\n",
    "                print(\"데이터가 없습니다.\")\n",
    "            \n",
    "            # 배치 사이즈 만큼 데이터가 쌓이면 다음 배치로 이동\n",
    "            if len(data_) == batch_size:\n",
    "                \n",
    "                # 데이터를 numpy 배열로 변환\n",
    "                input_data_test = np.array(data_.iloc[:,:-1]).reshape(batch_size, 1 , -1)\n",
    "                input_data_label = np.array(data_.iloc[:,-1:])\n",
    "                \n",
    "                # 데이터를 Tensor로 변환\n",
    "                input_data_test = torch.tensor(input_data_test, dtype=torch.float32).to(device).float()\n",
    "                input_data_label = torch.tensor(input_data_label, dtype=torch.float32).to(device).long()\n",
    "                \n",
    "                # DataLoader 생성\n",
    "                dataset = TensorDataset(input_data_test, input_data_label)\n",
    "                data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "                \n",
    "                for batch_input, batch_label in data_loader:\n",
    "\n",
    "                    # 모델 입력\n",
    "                    outputs_t = model(batch_input)\n",
    "                    \n",
    "                    # 예측 label 확인 \n",
    "                    _,pred_t = torch.max(outputs_t, dim=1)\n",
    "                    targets_t = batch_label.view_as(pred_t).to(device)\n",
    "                    \n",
    "                    preds_test.append(pred_t)\n",
    "                    target_test.append(targets_t)\n",
    "                \n",
    "                # 배치 리셋\n",
    "                data_ = []\n",
    "                \n",
    "            # 다음 시작 시간 설정    \n",
    "            start_time_t = unquote(next_start_time_t) \n",
    "            \n",
    "            # 마지막 시간을 넘어 가져오는 것을 방지\n",
    "            if index_next_t + (int(batch_size /16)) >= len(time_df_test):\n",
    "                break\n",
    "            \n",
    "    # 모든 배치에서 수집된 예측과 라벨을 합침\n",
    "    preds_test = torch.cat(preds_test).detach().cpu().numpy()\n",
    "    target_test = torch.cat(target_test).detach().cpu().numpy()\n",
    "\n",
    "    # 결과 데이터 프레임 생성\n",
    "    \n",
    "    final_df = pd.DataFrame(target_test, columns=['label'])\n",
    "    final_df['pred'] = target_test\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## 테스트 파라미터 설정 #############################################\n",
    "\n",
    "# 검증 시작 시간 설정\n",
    "start_time_test = '2024-01-01 00:01:59'\n",
    "# 검증 끝 시간 설정\n",
    "end_time_test = '2024-01-01 00:02:20'\n",
    "# 검증 시간 리스트 로드\n",
    "time_df_test = time_data_load(table, name, quote(start_time_test), quote(end_time_test), timeformat)\n",
    "\n",
    "######################################## 테스트 진행 #############################################\n",
    "final_df = test(table, name, timeformat, model_, start_time_test, end_time_test, batch_size, sample_rate, scaler, time_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        80\n",
      "           1       1.00      1.00      1.00        80\n",
      "           2       1.00      1.00      1.00        80\n",
      "           3       1.00      1.00      1.00        80\n",
      "\n",
      "    accuracy                           1.00       320\n",
      "   macro avg       1.00      1.00      1.00       320\n",
      "weighted avg       1.00      1.00      1.00       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(final_df['label'].values, final_df['pred'].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
