{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## 사용 라이브러리 호출\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote, unquote\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## 모델 사용 라이브러리 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "## 모델 학습 결과 경로 설정 \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Cuda 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## 랜덤 시드 설정\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 시드 설정 \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 데이터 컬럼 선택\n",
    "\n",
    "* tag name 이 많은 경우 tag name을 지정하는 것에 있어서 변수 설정이 다소 유연해짐\n",
    "* tag name 은 순서대로 불러와짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag name 출력 함수 \n",
    "def show_column(URL):\n",
    "    \n",
    "    # Tag name 데이터 로드\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # List 형식으로 변환\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tag name 출력 파라미터 설정\n",
    "table = 'rotor'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## tag name list 생성 \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g1_sensor1_normal',\n",
       " 'g1_sensor1_type1',\n",
       " 'g1_sensor1_type2',\n",
       " 'g1_sensor1_type3',\n",
       " 'g1_sensor2_normal',\n",
       " 'g1_sensor2_type1',\n",
       " 'g1_sensor2_type2',\n",
       " 'g1_sensor2_type3',\n",
       " 'g1_sensor3_normal',\n",
       " 'g1_sensor3_type1',\n",
       " 'g1_sensor3_type2',\n",
       " 'g1_sensor3_type3',\n",
       " 'g1_sensor4_normal',\n",
       " 'g1_sensor4_type1',\n",
       " 'g1_sensor4_type2',\n",
       " 'g1_sensor4_type3',\n",
       " 'g2_sensor1_normal',\n",
       " 'g2_sensor1_type1',\n",
       " 'g2_sensor1_type2',\n",
       " 'g2_sensor1_type3',\n",
       " 'g2_sensor2_normal',\n",
       " 'g2_sensor2_type1',\n",
       " 'g2_sensor2_type2',\n",
       " 'g2_sensor2_type3',\n",
       " 'g2_sensor3_normal',\n",
       " 'g2_sensor3_type1',\n",
       " 'g2_sensor3_type2',\n",
       " 'g2_sensor3_type3',\n",
       " 'g2_sensor4_normal',\n",
       " 'g2_sensor4_type1',\n",
       " 'g2_sensor4_type2',\n",
       " 'g2_sensor4_type3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAG Name format 변환 \n",
    "\n",
    "* 위의 과정에서 rotor dataset의 모든 Tag Name 을 확인후 사용할 컬럼만 뽑아서 입력할 파라미터 형태로 변환\n",
    "\n",
    "* g1 Tag Name 사용하여 예제 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'g1_sensor1_normal','g1_sensor1_type1','g1_sensor1_type2','g1_sensor1_type3','g1_sensor2_normal','g1_sensor2_type1','g1_sensor2_type2','g1_sensor2_type3','g1_sensor3_normal','g1_sensor3_type1','g1_sensor3_type2','g1_sensor3_type3','g1_sensor4_normal','g1_sensor4_type1','g1_sensor4_type2','g1_sensor4_type3'\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# 원하는 tag name 설정\n",
    "# 여기서 tag name 은 컬럼을 의미\n",
    "tags = name[:16]\n",
    "\n",
    "# 리스트의 각 항목을 작은따옴표로 감싸고, 쉼표로 구분\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# 사용 tag name 확인\n",
    "print(tags_)\n",
    "\n",
    "# 해당 값을 모델의 input shape로 설정 \n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotor Dataset 로드\n",
    "\n",
    "* 데이터 로드시 전체 데이터 셋을 각각 Load\n",
    "\n",
    "* label 설명\n",
    "    * normal : 정상\n",
    "    * type1 : Disk 2에 회전 불균형 (270도 위치에 볼트, 너트 부착)\n",
    "    * type2 : Support 4에 지지 불균형\n",
    "    * type3 : Type 1 + Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 파라미터 설정\n",
    "\n",
    "# tag table 이름 설정\n",
    "table = 'rotor'\n",
    "# tag name 설정\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# 시간 포멧 설정 \n",
    "timeformat = quote('2006-01-02 15:04:05.000000')\n",
    "# Train , validation , test 데이터 셋 설정\n",
    "# 학습 데이터 시작 시간 설정\n",
    "start_time = quote('2024-01-01 00:00:00')\n",
    "# 학습 데이터  끝 시간 설정\n",
    "end_time = quote('2024-01-01 00:02:19.999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 함수\n",
    "def data_load(table, name, start_time, end_time, timeformat):\n",
    "    \n",
    "    # 데이터 로드 \n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # 같은 시간대 별 데이터로 전환\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # time 설정\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # 빈 데이터 프레임 생성\n",
    "    df_result = pd.DataFrame()\n",
    "    \n",
    "    # 각 tag name 별 데이터 보간\n",
    "    \n",
    "    for i in range(len(df.columns[1:])):\n",
    "        \n",
    "        # 시간 설정\n",
    "        start = pd.to_datetime(unquote(start_time))\n",
    "        end = pd.to_datetime(unquote(end_time))\n",
    "        \n",
    "        df_ = df.iloc[ : , [0] + list(range(i+1, i+2))].dropna()\n",
    "        \n",
    "        # 보간할 새로운 시간 구간 생성 (1초마다 1000개의 포인트)\n",
    "        # 이 경우, 원본 데이터는 1ms 간격으로 측정되었으므로 1초 간격으로 1000개의 포인트를 생성\n",
    "        # 0에서 140 구간을 생성 -> 140,000\n",
    "        new_time_range = pd.date_range(start=start, end=end, freq='1ms')\n",
    "        new_time_range_ = pd.date_range(start=start, end=end, freq='1s')\n",
    "\n",
    "        # 선형 보간을 사용하여 데이터 보간\n",
    "        # datetime을 숫자형으로 변환 (epoch time in seconds)\n",
    "        time_numeric = pd.to_numeric(df_['TIME'])\n",
    "        new_time_numeric = pd.to_numeric(new_time_range)\n",
    "\n",
    "        value = df_[df_.columns[1:].values]\n",
    "\n",
    "        # 선형 보간 객체 생성\n",
    "        interpolator = interp1d(time_numeric, value.values.reshape(-1), kind='linear', fill_value='extrapolate')\n",
    "        interpolated_values = interpolator(new_time_numeric)\n",
    "        interpolated_values = np.clip(interpolated_values, min(value.values), max(value.values))\n",
    "\n",
    "        # 데이터 프레임 생성\n",
    "        df_remake = pd.DataFrame(interpolated_values.reshape(-1,1000))\n",
    "        df_remake['time'] = new_time_range_\n",
    "        df_remake['sensor'] = f'{df_.columns[1:].item()}'\n",
    "\n",
    "        # 이동할 컬럼과 새로운 순서 지정\n",
    "        cols = df_remake.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('time')))\n",
    "        cols.insert(1, cols.pop(cols.index('sensor')))\n",
    "        df_remake = df_remake[cols]\n",
    "\n",
    "        # 빈 데이터 프레임에 추가\n",
    "        df_result = pd.concat([df_result, df_remake], ignore_index=True)\n",
    "        \n",
    "    # 시간순 정렬\n",
    "    df_result = df_result.sort_values(by='time').reset_index(drop=True)\n",
    "        \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>sensor</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor1_normal</td>\n",
       "      <td>-0.853307</td>\n",
       "      <td>-0.524641</td>\n",
       "      <td>-0.003741</td>\n",
       "      <td>-0.297684</td>\n",
       "      <td>-0.091203</td>\n",
       "      <td>-0.045372</td>\n",
       "      <td>-0.060902</td>\n",
       "      <td>0.508235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139246</td>\n",
       "      <td>0.696438</td>\n",
       "      <td>-0.508470</td>\n",
       "      <td>0.264728</td>\n",
       "      <td>-0.399669</td>\n",
       "      <td>-0.316289</td>\n",
       "      <td>-0.763595</td>\n",
       "      <td>-1.010909</td>\n",
       "      <td>-0.718536</td>\n",
       "      <td>-0.720669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor1_type2</td>\n",
       "      <td>0.555219</td>\n",
       "      <td>-0.153753</td>\n",
       "      <td>-0.197844</td>\n",
       "      <td>-0.972652</td>\n",
       "      <td>-0.913384</td>\n",
       "      <td>-1.390481</td>\n",
       "      <td>-1.414697</td>\n",
       "      <td>-1.586338</td>\n",
       "      <td>...</td>\n",
       "      <td>1.346492</td>\n",
       "      <td>1.246483</td>\n",
       "      <td>1.169959</td>\n",
       "      <td>1.310134</td>\n",
       "      <td>1.402345</td>\n",
       "      <td>1.329445</td>\n",
       "      <td>1.220657</td>\n",
       "      <td>1.077944</td>\n",
       "      <td>0.662889</td>\n",
       "      <td>0.673957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor1_type3</td>\n",
       "      <td>3.919664</td>\n",
       "      <td>3.713706</td>\n",
       "      <td>2.698885</td>\n",
       "      <td>1.338952</td>\n",
       "      <td>0.701167</td>\n",
       "      <td>-0.333580</td>\n",
       "      <td>-0.488003</td>\n",
       "      <td>-2.033326</td>\n",
       "      <td>...</td>\n",
       "      <td>6.453300</td>\n",
       "      <td>6.381317</td>\n",
       "      <td>6.292205</td>\n",
       "      <td>6.023239</td>\n",
       "      <td>5.512099</td>\n",
       "      <td>5.003446</td>\n",
       "      <td>3.997718</td>\n",
       "      <td>3.273269</td>\n",
       "      <td>1.813393</td>\n",
       "      <td>1.391992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor2_normal</td>\n",
       "      <td>0.048823</td>\n",
       "      <td>-0.029477</td>\n",
       "      <td>-0.004731</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.096184</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>...</td>\n",
       "      <td>1.224365</td>\n",
       "      <td>0.973323</td>\n",
       "      <td>0.398562</td>\n",
       "      <td>0.589916</td>\n",
       "      <td>0.520218</td>\n",
       "      <td>-0.442281</td>\n",
       "      <td>-0.729798</td>\n",
       "      <td>-1.159330</td>\n",
       "      <td>-1.270378</td>\n",
       "      <td>-1.268479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>g1_sensor2_type1</td>\n",
       "      <td>-1.054255</td>\n",
       "      <td>-1.173785</td>\n",
       "      <td>-1.142896</td>\n",
       "      <td>-1.357203</td>\n",
       "      <td>-0.848193</td>\n",
       "      <td>-0.319767</td>\n",
       "      <td>-1.745371</td>\n",
       "      <td>0.480831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252693</td>\n",
       "      <td>-1.117656</td>\n",
       "      <td>0.199578</td>\n",
       "      <td>-0.209522</td>\n",
       "      <td>-0.181900</td>\n",
       "      <td>-0.995153</td>\n",
       "      <td>-0.854758</td>\n",
       "      <td>0.305171</td>\n",
       "      <td>-1.650853</td>\n",
       "      <td>-0.284521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor1_type2</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>0.586425</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.480463</td>\n",
       "      <td>0.595780</td>\n",
       "      <td>0.716434</td>\n",
       "      <td>0.976936</td>\n",
       "      <td>0.963343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073815</td>\n",
       "      <td>0.539034</td>\n",
       "      <td>0.110189</td>\n",
       "      <td>-0.139597</td>\n",
       "      <td>0.531263</td>\n",
       "      <td>0.664176</td>\n",
       "      <td>1.050553</td>\n",
       "      <td>0.868198</td>\n",
       "      <td>0.792942</td>\n",
       "      <td>1.197780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor1_type1</td>\n",
       "      <td>1.904225</td>\n",
       "      <td>2.580567</td>\n",
       "      <td>2.227736</td>\n",
       "      <td>2.198483</td>\n",
       "      <td>2.072673</td>\n",
       "      <td>2.332377</td>\n",
       "      <td>2.170863</td>\n",
       "      <td>1.553462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112187</td>\n",
       "      <td>-0.015991</td>\n",
       "      <td>-0.595242</td>\n",
       "      <td>-0.885469</td>\n",
       "      <td>-0.792080</td>\n",
       "      <td>-1.389177</td>\n",
       "      <td>-1.368793</td>\n",
       "      <td>-2.281878</td>\n",
       "      <td>-1.887351</td>\n",
       "      <td>-2.443287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor1_normal</td>\n",
       "      <td>0.917832</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>0.723296</td>\n",
       "      <td>0.487312</td>\n",
       "      <td>1.017593</td>\n",
       "      <td>0.800933</td>\n",
       "      <td>0.449253</td>\n",
       "      <td>0.847483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261695</td>\n",
       "      <td>-0.025523</td>\n",
       "      <td>-0.376261</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>-0.685341</td>\n",
       "      <td>-1.057457</td>\n",
       "      <td>-0.513635</td>\n",
       "      <td>-1.085233</td>\n",
       "      <td>-0.677111</td>\n",
       "      <td>-1.447941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor4_type2</td>\n",
       "      <td>-0.734751</td>\n",
       "      <td>-1.679474</td>\n",
       "      <td>-0.205754</td>\n",
       "      <td>-0.770393</td>\n",
       "      <td>0.273960</td>\n",
       "      <td>0.291936</td>\n",
       "      <td>0.355024</td>\n",
       "      <td>0.864662</td>\n",
       "      <td>...</td>\n",
       "      <td>2.318283</td>\n",
       "      <td>1.554217</td>\n",
       "      <td>1.711223</td>\n",
       "      <td>0.320252</td>\n",
       "      <td>0.483345</td>\n",
       "      <td>-0.423499</td>\n",
       "      <td>-1.127343</td>\n",
       "      <td>-0.805819</td>\n",
       "      <td>-1.208654</td>\n",
       "      <td>-1.151504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>2024-01-01 00:02:19</td>\n",
       "      <td>g1_sensor4_type3</td>\n",
       "      <td>1.105399</td>\n",
       "      <td>0.855382</td>\n",
       "      <td>-0.168068</td>\n",
       "      <td>-0.113933</td>\n",
       "      <td>-0.138020</td>\n",
       "      <td>-1.075471</td>\n",
       "      <td>-0.289690</td>\n",
       "      <td>-0.019060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.325286</td>\n",
       "      <td>0.390614</td>\n",
       "      <td>0.928405</td>\n",
       "      <td>1.126668</td>\n",
       "      <td>1.435119</td>\n",
       "      <td>1.799479</td>\n",
       "      <td>1.283169</td>\n",
       "      <td>0.758537</td>\n",
       "      <td>0.934503</td>\n",
       "      <td>0.412345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time             sensor         0         1         2  \\\n",
       "0    2024-01-01 00:00:00  g1_sensor1_normal -0.853307 -0.524641 -0.003741   \n",
       "1    2024-01-01 00:00:00   g1_sensor1_type2  0.555219 -0.153753 -0.197844   \n",
       "2    2024-01-01 00:00:00   g1_sensor1_type3  3.919664  3.713706  2.698885   \n",
       "3    2024-01-01 00:00:00  g1_sensor2_normal  0.048823 -0.029477 -0.004731   \n",
       "4    2024-01-01 00:00:00   g1_sensor2_type1 -1.054255 -1.173785 -1.142896   \n",
       "...                  ...                ...       ...       ...       ...   \n",
       "2235 2024-01-01 00:02:19   g1_sensor1_type2  0.101884  0.586425  0.301300   \n",
       "2236 2024-01-01 00:02:19   g1_sensor1_type1  1.904225  2.580567  2.227736   \n",
       "2237 2024-01-01 00:02:19  g1_sensor1_normal  0.917832  0.091463  0.723296   \n",
       "2238 2024-01-01 00:02:19   g1_sensor4_type2 -0.734751 -1.679474 -0.205754   \n",
       "2239 2024-01-01 00:02:19   g1_sensor4_type3  1.105399  0.855382 -0.168068   \n",
       "\n",
       "             3         4         5         6         7  ...       990  \\\n",
       "0    -0.297684 -0.091203 -0.045372 -0.060902  0.508235  ...  0.139246   \n",
       "1    -0.972652 -0.913384 -1.390481 -1.414697 -1.586338  ...  1.346492   \n",
       "2     1.338952  0.701167 -0.333580 -0.488003 -2.033326  ...  6.453300   \n",
       "3     0.009673  0.096184  0.009673  0.009673  0.024096  ...  1.224365   \n",
       "4    -1.357203 -0.848193 -0.319767 -1.745371  0.480831  ...  0.252693   \n",
       "...        ...       ...       ...       ...       ...  ...       ...   \n",
       "2235  0.480463  0.595780  0.716434  0.976936  0.963343  ... -0.073815   \n",
       "2236  2.198483  2.072673  2.332377  2.170863  1.553462  ...  0.112187   \n",
       "2237  0.487312  1.017593  0.800933  0.449253  0.847483  ...  0.261695   \n",
       "2238 -0.770393  0.273960  0.291936  0.355024  0.864662  ...  2.318283   \n",
       "2239 -0.113933 -0.138020 -1.075471 -0.289690 -0.019060  ... -0.325286   \n",
       "\n",
       "           991       992       993       994       995       996       997  \\\n",
       "0     0.696438 -0.508470  0.264728 -0.399669 -0.316289 -0.763595 -1.010909   \n",
       "1     1.246483  1.169959  1.310134  1.402345  1.329445  1.220657  1.077944   \n",
       "2     6.381317  6.292205  6.023239  5.512099  5.003446  3.997718  3.273269   \n",
       "3     0.973323  0.398562  0.589916  0.520218 -0.442281 -0.729798 -1.159330   \n",
       "4    -1.117656  0.199578 -0.209522 -0.181900 -0.995153 -0.854758  0.305171   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2235  0.539034  0.110189 -0.139597  0.531263  0.664176  1.050553  0.868198   \n",
       "2236 -0.015991 -0.595242 -0.885469 -0.792080 -1.389177 -1.368793 -2.281878   \n",
       "2237 -0.025523 -0.376261 -0.542628 -0.685341 -1.057457 -0.513635 -1.085233   \n",
       "2238  1.554217  1.711223  0.320252  0.483345 -0.423499 -1.127343 -0.805819   \n",
       "2239  0.390614  0.928405  1.126668  1.435119  1.799479  1.283169  0.758537   \n",
       "\n",
       "           998       999  \n",
       "0    -0.718536 -0.720669  \n",
       "1     0.662889  0.673957  \n",
       "2     1.813393  1.391992  \n",
       "3    -1.270378 -1.268479  \n",
       "4    -1.650853 -0.284521  \n",
       "...        ...       ...  \n",
       "2235  0.792942  1.197780  \n",
       "2236 -1.887351 -2.443287  \n",
       "2237 -0.677111 -1.447941  \n",
       "2238 -1.208654 -1.151504  \n",
       "2239  0.934503  0.412345  \n",
       "\n",
       "[2240 rows x 1002 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "\n",
    "df = data_load(table, name, start_time, end_time, timeformat)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 매핑\n",
    "\n",
    "# 레이블 매핑\n",
    "label_mapping = {\n",
    "    'normal': 0,\n",
    "    'type1': 1,\n",
    "    'type2': 2,\n",
    "    'type3': 3\n",
    "}\n",
    "\n",
    "# 컬럼 이름에서 레이블 추출하기\n",
    "def get_label(column_name):\n",
    "    column_name = str(column_name)\n",
    "    for key in label_mapping.keys():\n",
    "        if key in column_name:\n",
    "            return label_mapping[key]\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    560\n",
      "2    560\n",
      "3    560\n",
      "1    560\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 각 컬럼에 대해 레이블을 적용하여 새로운 시리즈 생성\n",
    "labels = pd.Series(df['sensor']).map(get_label)\n",
    "\n",
    "# 데이터 프레임에 label 추가\n",
    "df['label'] = labels.values\n",
    "\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    400\n",
      "2    400\n",
      "3    400\n",
      "1    400\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    80\n",
      "2    80\n",
      "3    80\n",
      "1    80\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    80\n",
      "2    80\n",
      "3    80\n",
      "1    80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train, validation, tets 데이터 분리\n",
    "\n",
    "# 각 센서 별로 데이터를 뽑아서 설정\n",
    "# trian 각 100개\n",
    "# vaildation 각 20개\n",
    "# test 각 20개\n",
    "train = pd.DataFrame()\n",
    "valid = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df['sensor'].value_counts().index)):\n",
    "    \n",
    "    df_train = df[df['sensor'] == df['sensor'].value_counts().index[i]][:100].iloc[:,2:]\n",
    "    df_valid = df[df['sensor'] == df['sensor'].value_counts().index[i]][100:120].iloc[:,2:]\n",
    "    df_test = df[df['sensor'] == df['sensor'].value_counts().index[i]][120:].iloc[:,2:]\n",
    "    \n",
    "    train = pd.concat([train, df_train])\n",
    "    valid = pd.concat([valid, df_valid])\n",
    "    test = pd.concat([test, df_test])\n",
    "    \n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "print(train['label'].value_counts())\n",
    "print(valid['label'].value_counts())\n",
    "print(test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "* 각 데이터별로 hanning window, FFT, MinMax Scaling 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hanning window 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanning window 함수 설정 \n",
    "def set_hanning_window(sample_rate, df):\n",
    "    \n",
    "    # Hanning 윈도우 생성\n",
    "    hanning_window = np.hanning(sample_rate)\n",
    "\n",
    "    # 각 행에 Hanning 윈도우 적용\n",
    "    df_windowed = df.multiply(hanning_window, axis=1)\n",
    "    \n",
    "    return df_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "window_length = len(df.columns[2:-1])\n",
    "\n",
    "train_ = set_hanning_window(window_length, train.iloc[:,:-1])\n",
    "valid_ = set_hanning_window(window_length, valid.iloc[:,:-1])\n",
    "test_ = set_hanning_window(window_length, test.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT 변환 함수\n",
    "def change_fft(sample_rate, df):\n",
    "    # 신호의 총 샘플 수\n",
    "    N = sample_rate\n",
    "    \n",
    "    # 각 행에 대해 FFT 적용\n",
    "    fft_results = np.zeros((df.shape[0], N // 2 + 1), dtype=float)\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        # 각 행의 FFT 계산\n",
    "        yf = fft(df.iloc[i].values)\n",
    "        \n",
    "        # FFT 결과의 절댓값을 계산하고 정규화 (유의미한 부분만)\n",
    "        fft_results[i] = 2.0 / N * np.abs(yf[:N // 2 + 1])\n",
    "    \n",
    "    # FFT 결과를 데이터 프레임으로 변환\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    \n",
    "    return fft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링 주기 -> 초당 데이터 개수 \n",
    "sampling_rate = len(df.columns[2:-1])\n",
    "\n",
    "# FFT 변환\n",
    "train_ = change_fft(sampling_rate, train_)\n",
    "valid_ = change_fft(sampling_rate, valid_)\n",
    "test_ = change_fft(sampling_rate, test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    400\n",
      "2    400\n",
      "3    400\n",
      "1    400\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    80\n",
      "2    80\n",
      "3    80\n",
      "1    80\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    80\n",
      "2    80\n",
      "3    80\n",
      "1    80\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 스케일러 설정\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 스케일러 적용\n",
    "train_ = scaler.fit_transform(train_.values)\n",
    "valid_ = scaler.transform(valid_.values)\n",
    "test_ = scaler.transform(test_.values)\n",
    "\n",
    "# 데이터 프레임 설정\n",
    "train_scaled = pd.DataFrame(train_)\n",
    "valid_scaled = pd.DataFrame(valid_)\n",
    "test_scaled = pd.DataFrame(test_)\n",
    "\n",
    "# label 추가\n",
    "train_scaled['label'] = train['label'].values\n",
    "valid_scaled['label'] = valid['label'].values\n",
    "test_scaled['label'] = test['label'].values\n",
    "\n",
    "print(train_scaled['label'].value_counts())\n",
    "print(valid_scaled['label'].value_counts())\n",
    "print(test_scaled['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 및 로더 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotor_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.freq_data = df.iloc[:,:-1]\n",
    "        self.label = df.iloc[:,-1:].squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.freq_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        input_time_data = self.freq_data.iloc[index,:]\n",
    "        input_time_data = torch.Tensor(input_time_data).expand(1, input_time_data.shape[0])\n",
    "        label = self.label[index]\n",
    "\n",
    "        return input_time_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 설정 \n",
    "train_ = Rotor_Dataset(train_scaled)\n",
    "valid_ = Rotor_Dataset(valid_scaled)\n",
    "test_ = Rotor_Dataset(test_scaled)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_dataloader = DataLoader(train_, batch_size=8, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 501])\n"
     ]
    }
   ],
   "source": [
    "print(list(train_dataloader)[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 모델 설정 \n",
    "\n",
    "* 1D ResNet 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D ResNet 모델 사용 \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=4):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet1D(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정 파라미터\n",
    "# 학습률 \n",
    "learning_rate = 0.01\n",
    "\n",
    "# 모델 초기화\n",
    "model = ResNet1D(ResidualBlock, [2, 2, 2, 2], num_classes=4).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 구조 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 설정\n",
    "\n",
    "* 학습 중 validation 데이터 기준 F1 score 값이 제일 높은 모델을 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ab0362e0224d60ac1bd4a7f840260f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 1.323616251051426, train acc: 46.0000\n",
      "\n",
      "train loss: 0.9860608429275454, train acc: 72.5625\n",
      "\n",
      "train loss: 0.788141967896372, train acc: 83.3125\n",
      "\n",
      "train loss: 0.6710549941565841, train acc: 88.1250\n",
      "\n",
      "train loss: 0.5822780100503004, train acc: 91.3125\n",
      "\n",
      "train loss: 0.5170890546264126, train acc: 92.7500\n",
      "\n",
      "train loss: 0.4626576783857308, train acc: 95.2500\n",
      "\n",
      "train loss: 0.41828711269379715, train acc: 96.3750\n",
      "\n",
      "train loss: 0.38569233473555264, train acc: 95.6875\n",
      "\n",
      "train loss: 0.356374800868638, train acc: 97.1250\n",
      "\n",
      "train loss: 0.33595717485689836, train acc: 95.4375\n",
      "\n",
      "train loss: 0.3155956726285755, train acc: 97.6875\n",
      "\n",
      "train loss: 0.29604502652767295, train acc: 98.1250\n",
      "\n",
      "train loss: 0.2780919258827531, train acc: 98.6250\n",
      "\n",
      "train loss: 0.2646979931241561, train acc: 97.3750\n",
      "\n",
      "train loss: 0.25410173746935927, train acc: 96.5625\n",
      "\n",
      "train loss: 0.24196571283329618, train acc: 98.5000\n",
      "\n",
      "train loss: 0.23141979091860979, train acc: 97.8125\n",
      "\n",
      "train loss: 0.2203103987374418, train acc: 99.2500\n",
      "\n",
      "train loss: 0.2130020539189891, train acc: 97.1250\n",
      "\n",
      "train loss: 0.2066950386862395, train acc: 97.1250\n",
      "\n",
      "train loss: 0.20015441359845434, train acc: 97.8750\n",
      "\n",
      "train loss: 0.19228013941738661, train acc: 99.4375\n",
      "\n",
      "train loss: 0.1850538158677186, train acc: 99.3750\n",
      "\n",
      "train loss: 0.1811462240695939, train acc: 97.5000\n",
      "\n",
      "train loss: 0.17537410666952316, train acc: 99.1250\n",
      "\n",
      "train loss: 0.17130405093601375, train acc: 98.0000\n",
      "\n",
      "train loss: 0.1657447423794536, train acc: 99.4375\n",
      "\n",
      "train loss: 0.16130882485935188, train acc: 98.7500\n",
      "\n",
      "train loss: 0.15805327589198537, train acc: 98.1875\n",
      "\n",
      "train loss: 0.15392194977817902, train acc: 98.8750\n",
      "\n",
      "train loss: 0.15044419419250477, train acc: 98.5625\n",
      "\n",
      "train loss: 0.14738113234178732, train acc: 98.1250\n",
      "\n",
      "train loss: 0.1438187557863449, train acc: 99.1875\n",
      "\n",
      "train loss: 0.13990173326101507, train acc: 99.7500\n",
      "\n",
      "train loss: 0.136404464090292, train acc: 99.6250\n",
      "\n",
      "train loss: 0.13480809073132055, train acc: 98.3125\n",
      "\n",
      "train loss: 0.13155215751696533, train acc: 99.7500\n",
      "\n",
      "train loss: 0.12977117495120244, train acc: 97.8750\n",
      "\n",
      "train loss: 0.12728951274769829, train acc: 99.2500\n",
      "\n",
      "train loss: 0.12474674371888786, train acc: 99.1875\n",
      "\n",
      "train loss: 0.12287534352178933, train acc: 98.5625\n",
      "\n",
      "train loss: 0.12098251796220011, train acc: 98.7500\n",
      "\n",
      "train loss: 0.11849782964271766, train acc: 99.6875\n",
      "\n",
      "train loss: 0.11659240814612369, train acc: 99.2500\n",
      "\n",
      "train loss: 0.1141388981237847, train acc: 100.0000\n",
      "\n",
      "train loss: 0.11278880969015212, train acc: 98.2500\n",
      "\n",
      "train loss: 0.11054920364379413, train acc: 99.7500\n",
      "\n",
      "train loss: 0.1084520768612591, train acc: 99.7500\n",
      "\n",
      "train loss: 0.10717889793781525, train acc: 98.5625\n",
      "\n",
      "train loss: 0.1054827846996983, train acc: 99.3750\n",
      "\n",
      "train loss: 0.10366310399537579, train acc: 99.6875\n",
      "\n",
      "train loss: 0.10302456706470281, train acc: 97.8750\n",
      "\n",
      "train loss: 0.10227755333048363, train acc: 98.4375\n",
      "\n",
      "train loss: 0.10050805479886446, train acc: 99.8750\n",
      "\n",
      "train loss: 0.09880894179317615, train acc: 99.8125\n",
      "\n",
      "train loss: 0.09724102651520626, train acc: 99.7500\n",
      "\n",
      "train loss: 0.09602515145646431, train acc: 99.1875\n",
      "\n",
      "train loss: 0.09509854967977047, train acc: 98.8125\n",
      "\n",
      "train loss: 0.09365653166840505, train acc: 99.6250\n",
      "\n",
      "train loss: 0.09243942839731771, train acc: 99.5000\n",
      "\n",
      "train loss: 0.0910842699905282, train acc: 99.6875\n",
      "\n",
      "train loss: 0.08976639013712207, train acc: 99.8750\n",
      "\n",
      "train loss: 0.08838368851363508, train acc: 100.0000\n",
      "\n",
      "train loss: 0.0875377806105956, train acc: 99.1250\n",
      "\n",
      "train loss: 0.08659037071695512, train acc: 99.0000\n",
      "\n",
      "train loss: 0.08566562039294234, train acc: 99.0625\n",
      "\n",
      "train loss: 0.0851977050483959, train acc: 98.7500\n",
      "\n",
      "train loss: 0.08432917072679433, train acc: 98.9375\n",
      "\n",
      "train loss: 0.08315839317501794, train acc: 99.8750\n",
      "\n",
      "train loss: 0.08200570681676783, train acc: 99.9375\n",
      "\n",
      "train loss: 0.08123738603829472, train acc: 99.3125\n",
      "\n",
      "train loss: 0.080140597143835, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07922100499989206, train acc: 99.7500\n",
      "\n",
      "train loss: 0.07817355782743068, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07714832118905157, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07700640730920028, train acc: 98.6875\n",
      "\n",
      "train loss: 0.07640630151622675, train acc: 98.8125\n",
      "\n",
      "train loss: 0.07576507556205041, train acc: 98.8750\n",
      "\n",
      "train loss: 0.0749018142946257, train acc: 99.8125\n",
      "\n",
      "train loss: 0.07398976245725439, train acc: 99.9375\n",
      "\n",
      "train loss: 0.07309373631122926, train acc: 100.0000\n",
      "\n",
      "train loss: 0.07250863228526479, train acc: 99.0625\n",
      "\n",
      "train loss: 0.07178064151079888, train acc: 99.5625\n",
      "\n",
      "train loss: 0.0710922574939521, train acc: 99.5625\n",
      "\n",
      "train loss: 0.07090044472510831, train acc: 98.6875\n",
      "\n",
      "train loss: 0.0701752175886075, train acc: 99.6875\n",
      "\n",
      "train loss: 0.06950203933106786, train acc: 99.5625\n",
      "\n",
      "train loss: 0.06876795262334058, train acc: 99.8125\n",
      "\n",
      "train loss: 0.06800806599940945, train acc: 100.0000\n",
      "\n",
      "train loss: 0.0672624069251341, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06653360980256565, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06581913018923999, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06512029848615947, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06443610031147802, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06376587854320946, train acc: 100.0000\n",
      "\n",
      "train loss: 0.06399806082599911, train acc: 98.1250\n",
      "\n",
      "train loss: 0.06368929321060185, train acc: 99.0000\n",
      "\n",
      "train loss: 0.0631708095150949, train acc: 99.6875\n",
      "\n",
      "train loss: 0.0626127117389695, train acc: 99.8125\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_dataloader)\n",
    "epoch_in = trange(100, desc='training')\n",
    "best_f1= 0\n",
    "\n",
    "for epoch in epoch_in:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total=0\n",
    "\n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    # 모델 학습 \n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = train_data[0].to(device).float()\n",
    "        labels = train_data[1].to(device).long().squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # label 예측 값 설정 \n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==labels).item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss)}, train acc: {(100 * correct / total):.4f}')\n",
    "    \n",
    "    # Epoch 마다 validation을 진행해서 가장 좋은 성능을 보이는 모델을 저장 \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "            inputs_v = valid_data[0].to(device).float()\n",
    "            labels_v = valid_data[1].to(device).long().squeeze() \n",
    "            \n",
    "            outputs_v = model(inputs_v)\n",
    "            \n",
    "            # label 예측 값 설정 \n",
    "            _,pred_v = torch.max(outputs_v, dim=1)\n",
    "            target_v = labels_v.view_as(pred_v)\n",
    "            \n",
    "            preds_.append(pred_v)\n",
    "            targets_.append(target_v)\n",
    "            \n",
    "        # 모든 배치에서 수집된 예측과 라벨을 합침\n",
    "        preds_ = torch.cat(preds_).detach().cpu().numpy()\n",
    "        targets_ = torch.cat(targets_).detach().cpu().numpy()\n",
    "        \n",
    "        f1score = f1_score(targets_, preds_,  average='macro')\n",
    "        if best_f1 < f1score:\n",
    "            best_f1 = f1score\n",
    "            # 베스트 모델 저장 \n",
    "            with open(\"./result/Rotor_1d_ResNet_General.txt\", \"a\") as text_file:\n",
    "                print('epoch=====',epoch, file=text_file)\n",
    "                print(classification_report(targets_, preds_, digits=4), file=text_file)\n",
    "            torch.save(model, f'./result/Rotor_1d_ResNet_General.pt') \n",
    "        epoch_in.set_postfix_str(f\"epoch = {epoch},  f1_score = {f1score}, best_f1 = {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "model_ = torch.load(f'./result/Rotor_1d_ResNet_General.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MACH-DE-28\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "# 모델 테스트 \n",
    "preds_test = []\n",
    "target_test = []\n",
    "with torch.no_grad():\n",
    "    model_.eval()\n",
    "    for batch_idx, test_data in enumerate(test_dataloader):\n",
    "        inputs_t = test_data[0].to(device).float()\n",
    "        labels_t =  test_data[1].to(device).long().squeeze() \n",
    "        \n",
    "        outputs_t = model_(inputs_t)\n",
    "        \n",
    "        _,pred_t = torch.max(outputs_t, dim=1)\n",
    "        targets_t = labels_t.view_as(pred_t).to(device)\n",
    "\n",
    "        preds_test.append(pred_t)\n",
    "        target_test.append(targets_t)\n",
    "        \n",
    "    # 모든 배치에서 수집된 예측과 라벨을 합침\n",
    "    preds_test = torch.cat(preds_test).detach().cpu().numpy()\n",
    "    target_test = torch.cat(target_test).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97        80\n",
      "           1       0.97      0.96      0.97        80\n",
      "           2       1.00      1.00      1.00        80\n",
      "           3       1.00      1.00      1.00        80\n",
      "\n",
      "    accuracy                           0.98       320\n",
      "   macro avg       0.98      0.98      0.98       320\n",
      "weighted avg       0.98      0.98      0.98       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, preds_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
