{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from urllib.parse import quote\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Import libraries for the model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange\n",
    "import statistics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Set path for saving model training results  \n",
    "import os\n",
    "os.makedirs('./result', exist_ok=True)\n",
    "\n",
    "## Set Cuda for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "## Set random seed\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Set seed \n",
    "seed_val = 77\n",
    "set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Columns\n",
    "* Tag names are loaded in sequential order.\n",
    "* The process of selecting the required tag names from the tag name list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display tag names\n",
    "def show_column(URL):\n",
    "    \n",
    "    # Load tag name data\n",
    "    df = pd.read_csv(URL)\n",
    "    \n",
    "    # Convert to list format\n",
    "    df = df.values.reshape(-1)\n",
    "    \n",
    "    return df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters for displaying tag names\n",
    "table = 'home'\n",
    "\n",
    "NAME_URL = f'http://127.0.0.1:5654/db/tql/datahub/api/v1/get_tag_names.tql?table={table}'\n",
    "\n",
    "## Generate tag name list \n",
    "name = show_column(NAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TAG-Barn [kW]',\n",
       " 'TAG-Dishwasher [kW]',\n",
       " 'TAG-Fridge [kW]',\n",
       " 'TAG-Furnace 1 [kW]',\n",
       " 'TAG-Furnace 2 [kW]',\n",
       " 'TAG-Garage door [kW]',\n",
       " 'TAG-Home office [kW]',\n",
       " 'TAG-House overall [kW]',\n",
       " 'TAG-Kitchen 12 [kW]',\n",
       " 'TAG-Kitchen 14 [kW]',\n",
       " 'TAG-Kitchen 38 [kW]',\n",
       " 'TAG-Living room [kW]',\n",
       " 'TAG-Microwave [kW]',\n",
       " 'TAG-Solar [kW]',\n",
       " 'TAG-Well [kW]',\n",
       " 'TAG-Wine cellar [kW]',\n",
       " 'TAG-apparentTemperature',\n",
       " 'TAG-dewPoint',\n",
       " 'TAG-gen [kW]',\n",
       " 'TAG-humidity',\n",
       " 'TAG-precipIntensity',\n",
       " 'TAG-precipProbability',\n",
       " 'TAG-pressure',\n",
       " 'TAG-temperature',\n",
       " 'TAG-use [kW]',\n",
       " 'TAG-visibility',\n",
       " 'TAG-windBearing',\n",
       " 'TAG-windSpeed']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting TAG Name Format\n",
    "* After checking all the Tag Names from the Smart home dataset in the previous step, extract only the columns to be used and convert them into parameter format.\n",
    "* Use tag names TAG-windBearing, TAG-windSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TAG-windBearing','TAG-windSpeed'\n"
     ]
    }
   ],
   "source": [
    "# Set the desired tag names\n",
    "tags = name[-2:]\n",
    "\n",
    "# Wrap each item in the list with single quotes and separate with commas\n",
    "tags_ = \",\".join(f\"'{tag}'\" for tag in tags)\n",
    "\n",
    "# Check the selected tag names\n",
    "print(tags_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load Smart Home Dataset\n",
    "* When loading the dataset, load the train, validation, and test datasets separately.\n",
    "* As an example, use 1 hour of data for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading parameter settings\n",
    "\n",
    "# Set the tag table name\n",
    "table = 'home'\n",
    "# Set the tag names\n",
    "name = quote(tags_, safe=\":/\")\n",
    "# Set the time format  \n",
    "timeformat = 'Default'\n",
    "\n",
    "# Set the train data start time\n",
    "start_time_train = quote('2016-01-01 14:00:00')\n",
    "# Set the train data end time\n",
    "end_time_train = quote('2016-01-01 15:00:00')\n",
    "\n",
    "# Set the validation data start time\n",
    "start_time_val = quote('2016-01-01 15:00:00')\n",
    "# Set the validation data end time\n",
    "end_time_val = quote('2016-01-01 16:00:00')\n",
    "\n",
    "# Set the test data start time\n",
    "start_time_test = quote('2016-01-01 16:00:00')\n",
    "# Set the test data end time\n",
    "end_time_test = quote('2016-01-01 17:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load function\n",
    "# '1D': Daily interval (1 day)\n",
    "# '1H': Hourly interval (1 hour)\n",
    "# '1T' or 'min': Minute interval (1 minute)\n",
    "# '1S': Second interval (1 second)\n",
    "def data_load(table, name, start_time, end_time, timeformat, resample_time):\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(f'http://127.0.0.1:5654/db/tql/datahub/api/v1/select-rawdata.tql?table={table}&name={name}&start={start_time}&end={end_time}&timeformat={timeformat}')\n",
    "    \n",
    "    # Convert to data grouped by the same time\n",
    "    df = df.pivot_table(index='TIME', columns='NAME', values='VALUE', aggfunc='first').reset_index()\n",
    "    \n",
    "    # Set time index\n",
    "    df = df.set_index(pd.to_datetime(df['TIME']))\n",
    "    df = df.drop(['TIME'], axis=1)\n",
    "    \n",
    "    # Resampling with 1-second intervals\n",
    "    # Can be modified to desired intervals such as day, hour, minute, etc.\n",
    "    df = df.resample(f'{resample_time}').mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train = data_load(table, name, start_time_train, end_time_train, timeformat, \"1s\")\n",
    "# Load validation data\n",
    "valid = data_load(table, name, start_time_val, end_time_val, timeformat, \"1s\")\n",
    "# Load test data\n",
    "test = data_load(table, name, start_time_test, end_time_test, timeformat, \"1s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 TAG-windBearing  TAG-windSpeed\n",
      "TIME                                               \n",
      "2016-01-01 14:00:00            282.0           9.18\n",
      "2016-01-01 14:00:01            282.0           9.18\n",
      "2016-01-01 14:00:02            282.0           9.18\n",
      "2016-01-01 14:00:03            282.0           9.18\n",
      "2016-01-01 14:00:04            282.0           9.18\n",
      "...                              ...            ...\n",
      "2016-01-01 14:59:56            253.0          11.30\n",
      "2016-01-01 14:59:57            253.0          11.30\n",
      "2016-01-01 14:59:58            253.0          11.30\n",
      "2016-01-01 14:59:59            253.0          11.30\n",
      "2016-01-01 15:00:00            253.0          11.30\n",
      "\n",
      "[3601 rows x 2 columns]\n",
      "NAME                 TAG-windBearing  TAG-windSpeed\n",
      "TIME                                               \n",
      "2016-01-01 15:00:00            253.0          11.30\n",
      "2016-01-01 15:00:01            253.0          11.30\n",
      "2016-01-01 15:00:02            253.0          11.30\n",
      "2016-01-01 15:00:03            253.0          11.30\n",
      "2016-01-01 15:00:04            253.0          11.30\n",
      "...                              ...            ...\n",
      "2016-01-01 15:59:56            232.0           3.91\n",
      "2016-01-01 15:59:57            232.0           3.91\n",
      "2016-01-01 15:59:58            232.0           3.91\n",
      "2016-01-01 15:59:59            232.0           3.91\n",
      "2016-01-01 16:00:00            232.0           3.91\n",
      "\n",
      "[3601 rows x 2 columns]\n",
      "NAME                 TAG-windBearing  TAG-windSpeed\n",
      "TIME                                               \n",
      "2016-01-01 16:00:00            232.0           3.91\n",
      "2016-01-01 16:00:01            232.0           3.91\n",
      "2016-01-01 16:00:02            232.0           3.91\n",
      "2016-01-01 16:00:03            232.0           3.91\n",
      "2016-01-01 16:00:04            232.0           3.91\n",
      "...                              ...            ...\n",
      "2016-01-01 16:59:56             43.0           4.83\n",
      "2016-01-01 16:59:57             43.0           4.83\n",
      "2016-01-01 16:59:58             43.0           4.83\n",
      "2016-01-01 16:59:59             43.0           4.83\n",
      "2016-01-01 17:00:00             43.0           4.83\n",
      "\n",
      "[3601 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(valid)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* 1 MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler Setup\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Scaler\n",
    "train_ = scaler.fit_transform(train.values)\n",
    "valid_ = scaler.transform(valid.values)\n",
    "test_ = scaler.transform(test.values)\n",
    "\n",
    "# Set DataFrames\n",
    "train_scaled = pd.DataFrame(train_ , columns=train.columns)\n",
    "valid_scaled = pd.DataFrame(valid_ , columns=valid.columns)\n",
    "test_scaled = pd.DataFrame(test_ , columns=test.columns)\n",
    "\n",
    "# Reset time index\n",
    "train_scaled.index = train.index\n",
    "valid_scaled.index = valid.index\n",
    "test_scaled.index = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 TAG-windBearing  TAG-windSpeed\n",
      "TIME                                               \n",
      "2016-01-01 14:00:00         0.966667       0.563459\n",
      "2016-01-01 14:00:01         0.966667       0.563459\n",
      "2016-01-01 14:00:02         0.966667       0.563459\n",
      "2016-01-01 14:00:03         0.966667       0.563459\n",
      "2016-01-01 14:00:04         0.966667       0.563459\n",
      "...                              ...            ...\n",
      "2016-01-01 14:59:56         0.644444       0.859135\n",
      "2016-01-01 14:59:57         0.644444       0.859135\n",
      "2016-01-01 14:59:58         0.644444       0.859135\n",
      "2016-01-01 14:59:59         0.644444       0.859135\n",
      "2016-01-01 15:00:00         0.644444       0.859135\n",
      "\n",
      "[3601 rows x 2 columns]\n",
      "NAME                 TAG-windBearing  TAG-windSpeed\n",
      "TIME                                               \n",
      "2016-01-01 15:00:00         0.644444       0.859135\n",
      "2016-01-01 15:00:01         0.644444       0.859135\n",
      "2016-01-01 15:00:02         0.644444       0.859135\n",
      "2016-01-01 15:00:03         0.644444       0.859135\n",
      "2016-01-01 15:00:04         0.644444       0.859135\n",
      "...                              ...            ...\n",
      "2016-01-01 15:59:56         0.411111      -0.171548\n",
      "2016-01-01 15:59:57         0.411111      -0.171548\n",
      "2016-01-01 15:59:58         0.411111      -0.171548\n",
      "2016-01-01 15:59:59         0.411111      -0.171548\n",
      "2016-01-01 16:00:00         0.411111      -0.171548\n",
      "\n",
      "[3601 rows x 2 columns]\n",
      "NAME                 TAG-windBearing  TAG-windSpeed\n",
      "TIME                                               \n",
      "2016-01-01 16:00:00         0.411111      -0.171548\n",
      "2016-01-01 16:00:01         0.411111      -0.171548\n",
      "2016-01-01 16:00:02         0.411111      -0.171548\n",
      "2016-01-01 16:00:03         0.411111      -0.171548\n",
      "2016-01-01 16:00:04         0.411111      -0.171548\n",
      "...                              ...            ...\n",
      "2016-01-01 16:59:56        -1.688889      -0.043236\n",
      "2016-01-01 16:59:57        -1.688889      -0.043236\n",
      "2016-01-01 16:59:58        -1.688889      -0.043236\n",
      "2016-01-01 16:59:59        -1.688889      -0.043236\n",
      "2016-01-01 17:00:00        -1.688889      -0.043236\n",
      "\n",
      "[3601 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_scaled)\n",
    "print(valid_scaled)\n",
    "print(test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Loader Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Dataset Configuration\n",
    "* To train on time series data, you need to set the window size and the sliding step.\n",
    "\n",
    "* Window size: Determines how many time points to group together.\n",
    "* Step size: The time interval by which the window moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding Window Dataset setup \n",
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, data, window_size, step_size):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.windows = self._create_windows()\n",
    "    \n",
    "    # Set up sliding windows\n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, self.step_size):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            windows.append(torch.Tensor(window.values))\n",
    "        return windows\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.windows[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding window configuration\n",
    "window_size = 3\n",
    "step_size = 1 \n",
    "\n",
    "# Set up datasets  \n",
    "train_ = SlidingWindowDataset(train_scaled, window_size, step_size)\n",
    "valid_ = SlidingWindowDataset(valid_scaled, window_size, step_size)\n",
    "test_ = SlidingWindowDataset(test_scaled, window_size, step_size)\n",
    "\n",
    "# Set up data loaders\n",
    "train_dataloader = DataLoader(train_, batch_size=32, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Verify DataLoader application and check the shape of the input data\n",
    "print(list(train_dataloader)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "* Using LSTM AE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoder class definition\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.encoder_fc = nn.Linear(hidden_dim, 2*hidden_dim)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.decoder_fc = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder part\n",
    "        _, (h, _) = self.encoder_lstm(x)\n",
    "        latent = self.encoder_fc(h[-1])\n",
    "        \n",
    "        # Decoder part\n",
    "        hidden = self.decoder_fc(latent).unsqueeze(0).repeat(x.size(1), 1, 1).permute(1, 0, 2)\n",
    "        output, _ = self.decoder_lstm(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAutoencoder(\n",
      "  (encoder_lstm): LSTM(2, 4, num_layers=3, batch_first=True)\n",
      "  (encoder_fc): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (decoder_fc): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (decoder_lstm): LSTM(4, 2, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration parameters\n",
    "\n",
    "# number of input data columns\n",
    "# last number in print(list(train_dataloader)[0][0].shape)\n",
    "input_dim = 2\n",
    "\n",
    "# LSMT hidden state size\n",
    "hidden_dim = 4\n",
    "\n",
    "# layer size\n",
    "num_layers = 3\n",
    "\n",
    "# Learning rate \n",
    "learning_rate = 0.01\n",
    "\n",
    "# Model configuration\n",
    "model = LSTMAutoencoder(input_dim, hidden_dim, num_layers).to(device)\n",
    "\n",
    "# Configure loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "* Save the model with the Best Loss based on the training data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02509a9af94044c8a610878a4ba05910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss: 0.11583049455245512\n",
      "Model saved\n",
      "\n",
      "train loss: 0.09801602310426863\n",
      "Model saved\n",
      "\n",
      "train loss: 0.09103955404907134\n",
      "Model saved\n",
      "\n",
      "train loss: 0.08712847792261189\n",
      "Model saved\n",
      "\n",
      "train loss: 0.08458687174916926\n",
      "Model saved\n",
      "\n",
      "train loss: 0.08279100409605894\n",
      "Model saved\n",
      "\n",
      "train loss: 0.08144886465213354\n",
      "Model saved\n",
      "\n",
      "train loss: 0.08040474738489911\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0795677098365008\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0788807465470256\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0783061602357805\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07781798646656198\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07739774099236686\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0770318977666934\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07671032088512011\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07642526288923243\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07617069679367676\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0759418656626687\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07573496423397971\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07554691351014725\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07537519451296698\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07521772793324083\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07507277853541841\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07493888688159374\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07481481213798434\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07469949221051944\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07459200906895574\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07449156656935925\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07439746848122253\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0743091057570154\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07422594154843024\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07414750233417494\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07407336755770343\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07400316357796717\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07393655655879114\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07387324669768676\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07381296497844275\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07375546811973463\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07370053640329445\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07364797049855702\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07359758937954919\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07354922869775107\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0735027378857792\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07345798050992981\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07341483156849246\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07337317663469221\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07333291154029083\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07329394052846179\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07325617606113385\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07321953827544089\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07318395363850834\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0731493547940205\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07311568017303352\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07308287279996611\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07305088080741143\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07301965658724037\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07298915570251112\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07295933793290234\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07293016588779423\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07290160493021365\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07287362361692022\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07284619204524259\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07281928348610035\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07279287235423024\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07276715970808328\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07274169698868636\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07271661224630233\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07269195372080343\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07266769624619851\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0726438203164407\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07262030953753132\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07259714903026114\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07257432479000742\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07255182429512783\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07252963552568767\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07250774719267189\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0724861487069261\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07246483008992799\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07244378177319898\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07242299501010963\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07240246128307222\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0723821724935467\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07236212123266969\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07234230024354844\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07232270259389537\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07230332188279871\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07228415188283288\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07226518694610755\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07224642139266552\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07222784993319105\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07220946747120781\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07219126933240551\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0721732506380211\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07215540732295517\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07213773516482408\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07212023002634006\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07210288830657068\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07208570634142085\n",
      "Model saved\n",
      "\n",
      "train loss: 0.07206868072374692\n",
      "Model saved\n",
      "\n",
      "train loss: 0.0720518081592431\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Initialize loss\n",
    "train_loss = []\n",
    "# Initialize total step\n",
    "total_step = len(train_dataloader)\n",
    "# Set number of epochs\n",
    "epoch_in = trange(100, desc='training')\n",
    "# Initialize best Loss value\n",
    "best_Loss= np.inf\n",
    "\n",
    "# Start model training\n",
    "for epoch in epoch_in:\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    preds_ = []\n",
    "    targets_ = []\n",
    "\n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "\n",
    "        inputs = train_data.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Input to the model\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss)}')\n",
    "\n",
    "    \n",
    "    if best_Loss > np.mean(train_loss):\n",
    "        best_Loss = np.mean(train_loss)\n",
    "        torch.save(model, f'./result/Smart_home_LSTM_AE.pt')\n",
    "        print('Model saved')\n",
    "    epoch_in.set_postfix_str(f\"epoch = {epoch}, best_Loss = {best_Loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Setting\n",
    "* Calculate the threshold using validation data:\n",
    "    * 1 Mean + Standard Deviation\n",
    "    * 2 Maximum Value\n",
    "    * 3 99% - Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_ = torch.load(f'./result/Smart_home_LSTM_AE.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2459881743170853\n",
      "4.045138359069824\n",
      "3.0967311209932786\n"
     ]
    }
   ],
   "source": [
    "# Calculate validation data reconstruction loss\n",
    "valid_loss = []\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for batch_idx, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "        inputs_val = valid_data.to(device).float()\n",
    "\n",
    "        outputs_val = model_(inputs_val)\n",
    "        loss = criterion(outputs_val, inputs_val)\n",
    "        \n",
    "        valid_loss.append(loss.item())\n",
    "        \n",
    "# Threshold setting\n",
    "# The threshold should be adjusted according to your own criteria\n",
    "threshold_1 =  statistics.mean(valid_loss) + statistics.stdev(valid_loss)\n",
    "threshold_2 =  max(valid_loss)\n",
    "threshold_3 =  np.percentile(valid_loss, 99) - statistics.stdev(valid_loss) \n",
    "\n",
    "print(threshold_1)\n",
    "print(threshold_2)\n",
    "print(threshold_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "* Proceed with model testing on the test data based on the threshold calculated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the test data\n",
    "test_loss = []\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for batch_idx, test_data in enumerate(test_dataloader):\n",
    "\n",
    "        inputs_test = test_data.to(device).float()\n",
    "\n",
    "        outputs_test = model_(inputs_test)\n",
    "        loss = criterion(outputs_test, inputs_test)\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "        \n",
    "# Create a DataFrame for the test results\n",
    "result = pd.DataFrame(test_loss, columns=['Reconst_Loss'])\n",
    "# Assume that there are no abnormal data\n",
    "result['label'] = 0\n",
    "\n",
    "# Classify normal and abnormal based on each threshold\n",
    "result['pred_1'] = np.where(result['Reconst_Loss']>threshold_1,1,0)\n",
    "result['pred_2'] = np.where(result['Reconst_Loss']>threshold_2,1,0)\n",
    "result['pred_3'] = np.where(result['Reconst_Loss']>threshold_3,1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "* Evaluate performance based on the F1 Score.\n",
    "* After evaluating performance across different thresholds, fix the threshold that shows the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.49      0.65       113\n",
      "\n",
      "   micro avg       1.00      0.49      0.65       113\n",
      "   macro avg       1.00      0.49      0.65       113\n",
      "weighted avg       1.00      0.49      0.65       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Threshold Setting using Mean + Standard Deviation\n",
    "print(classification_report(result['label'], result['pred_1'],labels=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       113\n",
      "\n",
      "   micro avg       1.00      0.99      1.00       113\n",
      "   macro avg       1.00      0.99      1.00       113\n",
      "weighted avg       1.00      0.99      1.00       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Threshold Setting using Maximum Value\n",
    "print(classification_report(result['label'], result['pred_2'],labels=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.84       113\n",
      "\n",
      "   micro avg       1.00      0.73      0.84       113\n",
      "   macro avg       1.00      0.73      0.84       113\n",
      "weighted avg       1.00      0.73      0.84       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Threshold Setting using 99% - Standard Deviation\n",
    "print(classification_report(result['label'], result['pred_3'],labels=[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
